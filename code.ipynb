{"cells":[{"cell_type":"markdown","metadata":{"id":"-8gBjOYeJpAJ"},"source":["# Notes\n","\n","This code can perform the following tasks:\n","\n","\n","*   Tune a CNN to directly reconstruct PET images from Sinograms (find a set of hyperparameters)\n","*   Train a network with a given set of hyperparameters\n","*   Test the network and record MSE and SSIM values for each image tested\n","*   Visualize the data and test results\n","*   Plot training curves, metric histograms, example images\n","\n","The code is organized into sections. The important sections that you can edit are:\n","\n","\n","> **User Parameters** - Edit important user parameters and decide what the code will do\n","\n","> **Configuration Dicts: Supervisory** - Dictionary for supervised learning. Make sure this matches the CNN loaded by the checkpoint file, if you are loading from a checkpoint.\n","\n","In addition to these, you may find that running a single cell is useful when all variables/classes/files have been loaded into memory. This can be quicker than running everything from scratch.\n","\n","The cells nested under **Analysis Functions** each have their own changeable parameters.\n","\n","*Notes:*\n","\n","*1) Raytune in particular is constantly changing. Therefore, if you are running this code after the authors have ceased maintaining it and there are errors, these are likely due to RayTune classes, methods, or functions being changed. Unfortunately, these seem to happen on a regular basis, as the code is relatively new.*\n","\n","*2) This code was originally written to tune/train/test not just sinogram to image supervisory networks (sinogram-->image), but also image to sinogram supervisory networks, GANs, CycleGANs, and Cycle + Supervisory networks. These latter capabilities have not been updated, but much of the code survives for this functionality. In the future, the code may be updated once again have these capabilities.*\n"]},{"cell_type":"markdown","metadata":{"id":"xoNd1Rb3zmng"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"odmouzHe6txY","outputId":"f8240831-cdaa-4b17-afe7-1037f8bd8062","executionInfo":{"status":"ok","timestamp":1759357833798,"user_tz":300,"elapsed":72960,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ray\n","  Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray) (8.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray) (3.19.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray) (4.25.1)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray) (1.1.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray) (25.0)\n","Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray) (5.29.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray) (2.32.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.27.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2025.8.3)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.15.0)\n","Downloading ray-2.49.2-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ray\n","Successfully installed ray-2.49.2\n","Collecting tensorboardX\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (25.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (5.29.5)\n","Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.4\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.12/dist-packages (0.2.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hyperopt) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.17.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.12/dist-packages (from hyperopt) (3.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from hyperopt) (4.67.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from hyperopt) (3.1.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (from hyperopt) (0.10.9.7)\n","Mounted at /content/drive\n"]}],"source":["## Ray Tune ##\n","!pip install ray\n","!pip install -U tensorboardX\n","!pip install -U hyperopt    # Hyperband search algorithmn. Another popular option is 'Optuna'\n","\n","from ray import air, tune, train\n","from ray.air import session\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.tune.schedulers import FIFOScheduler # First in/first out scheduler\n","from ray.tune import ResultGrid, JupyterNotebookReporter, CLIReporter\n","from ray.tune.search.hyperopt import HyperOptSearch    # Search Algorithm (current)\n","#from ray.tune.suggest.ax import AxSearch               # Search Algorithm (couldn't make this work)\n","#from ray.tune.suggest.bayesopt import BayesOptSearch   # Search Algorithm (couldn't make this work)\n","\n","## Pytorch ##\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch.utils.tensorboard import SummaryWriter\n","torch.manual_seed(0)  # For testing purposes\n","\n","## Torchvision ##\n","from torchvision.utils import make_grid\n","from torchvision import transforms\n","\n","## Numpy/MatPlotLib ##\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import Normalize\n","from matplotlib.pyplot import savefig\n","import matplotlib.gridspec as gridspec\n","\n","## Pandas ##\n","import pandas as pd\n","\n","## SciKit #\n","from skimage import metrics\n","from skimage.metrics import structural_similarity\n","from skimage.transform import radon, iradon\n","from skimage.transform import iradon\n","from skimage import morphology\n","from skimage.morphology import opening, erosion\n","#from skimage.restoration import denoise_bilateral, denoise_tv_chambolle, denoise_wavelet\n","\n","## SciPy ##\n","#from scipy.stats import moment as compute_moment\n","\n","## Python ##\n","import os\n","import time\n","import sys\n","#from IPython.display import display, clear_output\n","\n","\n","## Colab ##\n","from google.colab import drive\n","drive.mount('/content/drive') # Mount Google Drive\n","\n","## Determine what Hardware we have ##\n","device = ('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"OSWJFsOU6zsL"},"source":["# User Parameters"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7uyTV83_hQEI","executionInfo":{"status":"error","timestamp":1760307411079,"user_tz":300,"elapsed":150,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"outputId":"8a7fcc7d-9b9b-4c3a-eefc-ce3fbc37cf91","colab":{"base_uri":"https://localhost:8080/","height":218}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'os' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1225171906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mtune_sino_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_sino_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0mtune_image_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_image_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mtrain_sino_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sino_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["#####################\n","### General Setup ###\n","#####################\n","\n","# Basic Options #\n","run_mode='tune'  # Options: 'tune' / 'train' / 'test' / 'visualize' (visualize data set)\n","sino_size=180          # Resize input sinograms to this size (integer). Sinograms are square, which was found to give the best results.\n","sino_channels=3       # Number of channels (sinograms). Options: 1, 3. Unless using scattered coincidences, set to 1.\n","image_size=90         # Image size (Options: 90, 180). Images are square.\n","image_channels=1      # Number of channels (images)\n","train_type='SUP'      # 'SUP' / 'GAN' / 'CYCLESUP' / 'CYCLEGAN' = (Supervisory only/GAN/Cycle consistency+supervisory/CycleGAN)\n","train_SI=True         # If training GAN or SUP, set True to train Gen_SI (Sinogram-->Image), or False to train Gen_IS (Image-->Sinogram)\n","\n","# Global Directories #\n","#data_dir=      '/content/drive/MyDrive/Repository/PET_Data/'                # Where training/testing/tuning data is located\n","data_dir = '/content/drive/MyDrive/Colab/Working/sets/'\n","local_dir=     '/content/drive/MyDrive/Colab/Working/'                              # Directories not explicitly assigned are created in this directory\n","plot_dir=      '/content/drive/MyDrive/Colab/Working/Plots/'                        # Directory to save plots\n","checkpoint_dir='/content/drive/MyDrive/Colab/Working/Checkpoints-temp'              # If not using Ray Tune (not tuning), PyTorch saves and\n","                                                                                    #   loads checkpoint file from here\n","                                                                                    # All checkpoint files (for training, testing, visualizing) save\n","                                                                                    #   the states for a particular network.\n","                                                                                    # Therefore, the hyperparameters for the loaded CNN must match the data in the checkpoint file.\n","                                                                                    # The configuration dictionary, which contains these\n","                                                                                    #   hyperparameter values, is set in the 'Supervisory\" cell, below.\n","############\n","## Tuning ##\n","############\n","# Note: When tuning, ALWAYS select \"restart session and run all\" from Runtime menu in Google Colab, or there may be bugs.\n","tune_scheduler = 'ASHA'     # Use FIFO for simple first in/first out to train to the end, or ASHA for utilizing early stopping poorly performing trials.\n","tune_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-TuneTemp'  # This directory should already exist. The code will not make this for you.\n","tune_csv_file='frame-tunedOnLowSSIM-tunedSSIM-ASHA' # .csv file to save tuning dataframe to\n","tune_exp_name='search-Temp'                         # Experiment directory: Ray tune (and Tensorboard) write to this directory, relative to the local dir this notebook is run from.\n","tune_dataframe_fraction=0.33# At what fraction of the max tuning steps (tune_max_t) do you save values to the tuning dataframe.\n","tune_restore=False          # Restore a run (from the file tune_exp_name in local_dir). Use this if a tuning run terminated early for some reason.\n","tune_max_t = 10             # Maximum number of reports per network. For even training example reporting (reports made at a constant number of training\n","                            # examples), 20 is a good number for ASHA. For FIFO, 10 is a good number.\n","                            # For constant batch size reporting (tune_even_reporting=False), 35 works well.\n","tune_minutes = 30           # How long to run RayTune. 180 minutes is good for 90x90 input. 210 minutes for 180x180.\n","tune_for = 'SSIM'           # Tune for which optimization metric?: 'MSE', 'SSIM', or 'CUSTOM'\n","                            # (user defined, defined later in code).\n","tune_even_reporting=True    # Set to True to ensure we report to Raytune at an even number of training examples,\n","                            # regardless of batch size.\n","tune_iter_per_report=10     # Default value = 10.\n","                            # If tune_even_reporting = True, this is the number of training iterations per Raytune report for a batch size = 512.\n","                            # For a batch size = 256, the iterations/report would be twice this number. For batch size # = 128, it would be four\n","                            # times, etc.\n","                            # If tune_even_reporting = False, this is the number of batches per report (30 works pretty well).\n","tune_augment=True           # Augment data (on the fly) for tuning?\n","num_CPUs=4                  # Number of CPUs to use\n","num_GPUs=1                  # Number of GPUs to use\n","\n","\n","\n","## Select Data Files ##\n","## ----------------- ##\n","#tune_sino_file=  'tune_sino-10k.npy'\n","#tune_image_file= 'tune_image-10k.npy'\n","#tune_sino_file= 'train_sino-highMSE-17500.npy'\n","#tune_image_file='train_image-highMSE-17500.npy'\n","#tune_sino_file= 'train_sino-lowMSE-17500.npy'\n","#tune_image_file='train_image-lowMSE-17500.npy'\n","tune_sino_file= 'train_sino-lowSSIM-17500.npy'\n","tune_image_file='train_image-lowSSIM-17500.npy'\n","\n","\n","\n","##############\n","## Training ##\n","##############\n","train_load_state=False      # Set to True to load pretrained weights. Use if training terminated early.\n","train_save_state=False      # Save network weights to train_checkpoint_file file as it trains\n","train_checkpoint_file='checkpoint-tunedLowSSIM-trainedHighSSIM-100epochs' # Checkpoint file to load or save to\n","#train_checkpoint_file='checkpoint-90x1-tunedLDM_w10s8-b5c-6epochs'\n","#train_checkpoint_file='checkpoint-90x1-tunedLDM_w5s2-6epochs'\n","training_epochs = 100      # Number of training epochs.\n","train_augment=True         # Augment data (on the fly) for training?\n","train_display_step=10      # Number of steps/visualization. Good values: for supervised learning or GAN, set to: 20, For cycle-consistent, set to 10\n","train_sample_division=1    # To evenly sample the training set by a given factor, set this to an integer greater than 1 (ex: to sample every other example, set to 2)\n","train_show_times=False     # Show calculation times during training?\n","\n","## Select Data Files ##\n","## ----------------- ##\n","#train_sino_file= 'train_sino-70k.npy'\n","#train_image_file='train_image-70k.npy'\n","#train_sino_file= 'train_sino-highMSE-17500.npy'\n","#train_image_file='train_image-highMSE-17500.npy'\n","#train_sino_file= 'train_sino-lowMSE-17500.npy'\n","#train_image_file='train_image-lowMSE-17500.npy'\n","train_sino_file= 'train_sino-lowSSIM-17500.npy'\n","train_image_file= 'train_image-lowSSIM-17500.npy'\n","\n","\n","\n","###########\n","# Testing #\n","###########\n","#test_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-Test-Quartile'   # Directory for metric dataframes\n","test_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'  # Directory for metric dataframes\n","test_csv_file = 'combined-tunedLowSSIM-trainedLowSSIM-onTestSet-wMLEM' # csv dataframe file to save testing results to\n","test_checkpoint_file='checkpoint-tunedLowSSIM-trainedLowSSIM-100epochs' # Checkpoint to load model for testing\n","test_display_step=15        # Make this a larger number to save bit of time (displays images/metrics less often)\n","test_batch_size=25          # This doesn't affect the final metrics, just the displayed metrics as testing procedes\n","test_chunk_size=875              # How many examples do you want to test at once? NOTE: This should be a multiple of test_batch_size AND also go into the test set size evenly.\n","testset_size=35000          # Size of the set to test. This must be <= the number of examples in your test set file.\n","test_begin_at=0             # Begin testing at this example number.\n","compute_MLEM=True           # Compute a simple MLEM reconstruction from the sinograms when running testing.\n","                            # This takes a lot longer. If set to false, only FBP is calculated.\n","test_set_type='test'        # Set to 'test' to test on the test set. Set to 'train' to test on the training set.\n","# Defaults\n","test_merge_dataframes=True       # Merge the smaller/chunked dataframes at the end of the test run into one large dataframe?\n","test_show_times=False       # Show calculation times?\n","test_shuffle=False          # Shuffle test set when testing?\n","test_sample_division=1      # To evenly sample the test set by a given factor, set this to an integer greater than 1.\n","\n","## Select Data Files ##\n","## ----------------- ##\n","test_sino_file=  'test_sino-35k.npy'\n","test_image_file= 'test_image-35k.npy'\n","#test_sino_file= 'test_sino-highMSE-8750.npy'\n","#test_image_file= 'test_image-highMSE-8750.npy'\n","#test_sino_file= 'test_sino-lowMSE-8750.npy'\n","#test_image_file= 'test_image-lowMSE-8750.npy'\n","\n","\n","\n","####################\n","## Visualize Data ##\n","####################\n","\n","#visualize_checkpoint_file='checkpoint-90x1-tunedMSE-fc6-6epochs' # Checkpoint file to load/save\n","visualize_checkpoint_file='checkpoint-tunedHigh-trainedHigh-100epochs'\n","visualize_batch_size = 10   # Set value to exactly 120 to see a large grid of images OR =<10 for reconstructions\n","                            #  and ground truth with matched color scales\n","visualize_offset=0          # Image to begin at. Set to 0 to start at beginning.\n","visualize_type='train'      # Set to 'test' or 'train' to visualize the test set or training set, respectively\n","visualize_shuffle=True      # Shuffle data set when visualizing?\n","\n","\n","\n","####################################################################\n","## Assign Values for Various Scenarios: visualize/tune/train/test ##\n","####################################################################\n","\n","tune_sino_path=os.path.join(data_dir, tune_sino_file)\n","tune_image_path=os.path.join(data_dir, tune_image_file)\n","train_sino_path=os.path.join(data_dir, train_sino_file)\n","train_image_path=os.path.join(data_dir, train_image_file)\n","test_sino_path=os.path.join(data_dir, test_sino_file)\n","test_image_path=os.path.join(data_dir, test_image_file)\n","\n","## Run-Type Specific Assignments ##\n","if run_mode=='tune':\n","    sino_path=tune_sino_path\n","    image_path=tune_image_path\n","    augment=tune_augment\n","    shuffle = True\n","    num_epochs=1000         # Tuning is stopped when the iteration = tune_max_t (defined later). We set num_epochs to a large number so tuning doesn't terminate early.\n","    load_state=False\n","    save_state=False\n","    checkpoint_file = ''    # Leave this empty. The checkpoint path is constructed regardless, so this ensures that no error occurs.\n","    offset=0\n","    show_times=False\n","    sample_division=1\n","    tune_dataframe_path = os.path.join(tune_dataframe_dir, tune_csv_file+'.csv')\n","\n","if run_mode=='train':\n","    sino_path=train_sino_path\n","    image_path=train_image_path\n","    augment=train_augment\n","    shuffle=True\n","    num_epochs=training_epochs\n","    load_state=train_load_state\n","    save_state=train_save_state\n","    checkpoint_file = train_checkpoint_file\n","    offset=0\n","    show_times=train_show_times\n","    sample_division=train_sample_division\n","\n","if run_mode=='test':\n","    if test_set_type=='test': # Test on test set\n","        sino_path=test_sino_path\n","        image_path=test_image_path\n","    else: # Test on training set\n","        sino_path=train_sino_path\n","        image_path=train_image_path\n","    augment = False\n","    shuffle = test_shuffle\n","    num_epochs=1\n","    load_state=True # Set to True to load pretrained weights.\n","    save_state=False # Do not save network weights to checkpoint file as we are only testing.\n","    checkpoint_file = test_checkpoint_file\n","    offset=0\n","    show_times=test_show_times\n","    sample_division=test_sample_division\n","\n","if run_mode=='visualize':\n","    if visualize_type=='test':\n","        sino_path=test_sino_path  # test_sino_path\n","        image_path=test_image_path # test_image_path\n","    else:\n","        sino_path=train_sino_path  # test_sino_path\n","        image_path=train_image_path # test_image_path\n","    augment = False\n","    shuffle = visualize_shuffle\n","    num_epochs=1\n","    load_state=True\n","    save_state=False\n","    checkpoint_file = visualize_checkpoint_file\n","    show_times=False\n","    offset=visualize_offset\n","    sample_division=1\n","\n","## Assign dataframe and checkpoint paths ##\n","test_dataframe_path = os.path.join(test_dataframe_dir, test_csv_file+'.csv')\n","checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file) # Requires the assignment of checkpoint_file, so is done after the run_type specific assignments"]},{"cell_type":"markdown","metadata":{"id":"_8x2aams6ekN"},"source":["# Configuration Dicts\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"OXA-yS5rMYIZ"},"source":["## Supervisory\n","\n","In this cell, set the correct hyperparameter dictionary to config_SUP_SI. This is the dictionary of hyperparameters that determines the form of a the network that will be trained, tested, or visualized (when doing supervised learning, Sinogram-->Image). You will usually find these hyperparameters by performing tuning and examining the best performing networks in tensorboard.\n","\n","If training supervisory loss networks only, you don't need to worry about the other dictionaries in this section (GANs, Cycle-Consistent). You also don't need to worry about \"Search Spaces\", as this is simply a dictionary of the search space that Ray Tune uses when tuning. Feel free to look at it though, to see how I set up the search space. The last section (Set Correct Config) is where the configuration dictionary gets assigned. The dictionary is either a searchable space, if tuning, or a set of fixed hyperparameters, if training, testing, or visualizing the data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pf1yniv1MNFf","executionInfo":{"status":"ok","timestamp":1747677226309,"user_tz":300,"elapsed":165,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"a998ce0a-f3cb-4b46-8954-1b159fde8500"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# 1x90x90, Tuned for MSE, highMSE quartile - 66e\\nconfig_SUP_SI = {\\n  \"SI_dropout\": False,\\n  \"SI_exp_kernel\": 4,\\n  \"SI_gen_fill\": 0,\\n  \"SI_gen_final_activ\": nn.Tanh(),\\n  \"SI_gen_hidden_dim\": 13,\\n  \"SI_gen_mult\": 2.427097790975542,\\n  \"SI_gen_neck\": 1,\\n  \"SI_gen_z_dim\": 1943,\\n  \"SI_layer_norm\": \"instance\",\\n  \"SI_normalize\": True,\\n  \"SI_pad_mode\": \"zeros\",\\n  \"SI_scale\": 8100,\\n  \"batch_size\": 399,\\n  \"gen_b1\": 0.5173104983713961,\\n  \"gen_b2\": 0.5269533977675209,\\n  \"gen_lr\": 0.00042406256400739315,\\n  \"sup_criterion\": nn.MSELoss()\\n}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["### Below networks were tuned on whole dataset ###\n","# 1x90x90, Tuned for MSE - fc6 #\n","'''\n","config_SUP_SI={\n","  \"SI_dropout\": False,\n","  \"SI_exp_kernel\": 4,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": None,\n","  \"SI_gen_hidden_dim\": 14,\n","  \"SI_gen_mult\": 2.3737518721494038,\n","  \"SI_gen_neck\": 5,\n","  \"SI_gen_z_dim\": 300,\n","  \"SI_layer_norm\": \"instance\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 266,\n","  \"gen_b1\": 0.5194977285709309,\n","  \"gen_b2\": 0.4955647195661826,\n","  \"gen_lr\": 0.0006569034263698925,\n","  \"sup_criterion\": nn.MSELoss()\n","}\n","'''\n","'''\n","# 1x90x90, Tuned for MAE (mean absolute error) - b08 #\n","config_SUP_SI={\n","  \"SI_dropout\": True,\n","  \"SI_exp_kernel\": 3,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": nn.Tanh(),\n","  \"SI_gen_hidden_dim\": 29,\n","  \"SI_gen_mult\": 3.4493572412953926,\n","  \"SI_gen_neck\": 5,\n","  \"SI_gen_z_dim\": 92,\n","  \"SI_layer_norm\": \"instance\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 184,\n","  \"gen_b1\": 0.41793988944151467,\n","  \"gen_b2\": 0.15133808988276928,\n","  \"gen_lr\": 0.0012653525173041019,\n","  \"sup_criterion\": nn.L1Loss()\n","}\n","'''\n","'''\n","# 1x90x90, Tuned for SSIM - 14d #\n","config_SUP_SI = {\n","  \"SI_dropout\": False,\n","  \"SI_exp_kernel\": 4,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": nn.Tanh(),\n","  \"SI_gen_hidden_dim\": 23,\n","  \"SI_gen_mult\": 1.6605902406330195,\n","  \"SI_gen_neck\": 5,\n","  \"SI_gen_z_dim\": 789,\n","  \"SI_layer_norm\": \"instance\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 71,\n","  \"gen_b1\": 0.2082092731474774,\n","  \"gen_b2\": 0.27147903136187507,\n","  \"gen_lr\": 0.0005481469822215635,\n","  \"sup_criterion\": nn.MSELoss()\n","}\n","'''\n","'''\n","# 1x90x90, Tuned for Local Distributions Metric, 5x5 window, stride 2\n","config_SUP_SI={\n","  \"SI_dropout\": True,\n","  \"SI_exp_kernel\": 3,\n","  \"SI_gen_fill\": 2,\n","  \"SI_gen_final_activ\": nn.Sigmoid(),\n","  \"SI_gen_hidden_dim\": 18,\n","  \"SI_gen_mult\": 2.4691388140182475,\n","  \"SI_gen_neck\": 11,\n","  \"SI_gen_z_dim\": 444,\n","  \"SI_layer_norm\": \"instance\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 33,\n","  \"gen_b1\": 0.8199882799898334,\n","  \"gen_b2\": 0.1207854128656507,\n","  \"gen_lr\": 0.0001095057659925285,\n","  \"sup_criterion\": nn.BCEWithLogitsLoss()\n","}\n","'''\n","'''\n","# 1x90x90, Tuned for Local Distributions Metric, 10x10 window, stride 8 (b5c)\n","config_SUP_SI={\n","  \"SI_dropout\": False,\n","  \"SI_exp_kernel\": 4,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": None,\n","  \"SI_gen_hidden_dim\": 9,\n","  \"SI_gen_mult\": 2.1547197646081444,\n","  \"SI_gen_neck\": 5,\n","  \"SI_gen_z_dim\": 344,\n","  \"SI_layer_norm\": \"batch\",\n","  \"SI_normalize\": False,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 47,\n","  \"gen_b1\": 0.31108788447029295,\n","  \"gen_b2\": 0.3445239707919786,\n","  \"gen_lr\": 0.0007561178182660596,\n","  \"sup_criterion\": nn.L1Loss()\n","}\n","'''\n","\n","\n","### Below networks were tuned on 1/4 of dataset (high MSE or low MSE) ####\n","\n","# 1x90x90, Tuned for SSIM, highSSIM quartile, - c867539\n","config_SUP_SI = {\n","  \"SI_dropout\": False,\n","  \"SI_exp_kernel\": 3,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": nn.Tanh(),\n","  \"SI_gen_hidden_dim\": 14,\n","  \"SI_gen_mult\": 3.1366081867376066,\n","  \"SI_gen_neck\": 5,\n","  \"SI_gen_z_dim\": 1235,\n","  \"SI_layer_norm\": \"instance\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"reflect\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 512,\n","  \"gen_b1\": 0.36092827701745117,\n","  \"gen_b2\": 0.2959809747063715,\n","  \"gen_lr\": 0.0003914885622973457,\n","  \"sup_criterion\": nn.MSELoss()\n","}\n","\n","'''\n","# 1x90x90, Tuned for MSE, lowMSE quartile - d3c\n","config_SUP_SI = {\n","  \"SI_dropout\": False,\n","  \"SI_exp_kernel\": 3,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": nn.Tanh(),\n","  \"SI_gen_hidden_dim\": 10,\n","  \"SI_gen_mult\": 3.5952046080348117,\n","  \"SI_gen_neck\": 5,\n","  \"SI_gen_z_dim\": 1144,\n","  \"SI_layer_norm\": \"batch\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 338,\n","  \"gen_b1\": 0.21119520045946658,\n","  \"gen_b2\": 0.3219437242478679,\n","  \"gen_lr\": 0.0012228287967471555,\n","  \"sup_criterion\": nn.L1Loss()\n","}\n","'''\n","'''\n","# 1x90x90, Tuned for MSE, highMSE quartile - 66e\n","config_SUP_SI = {\n","  \"SI_dropout\": False,\n","  \"SI_exp_kernel\": 4,\n","  \"SI_gen_fill\": 0,\n","  \"SI_gen_final_activ\": nn.Tanh(),\n","  \"SI_gen_hidden_dim\": 13,\n","  \"SI_gen_mult\": 2.427097790975542,\n","  \"SI_gen_neck\": 1,\n","  \"SI_gen_z_dim\": 1943,\n","  \"SI_layer_norm\": \"instance\",\n","  \"SI_normalize\": True,\n","  \"SI_pad_mode\": \"zeros\",\n","  \"SI_scale\": 8100,\n","  \"batch_size\": 399,\n","  \"gen_b1\": 0.5173104983713961,\n","  \"gen_b2\": 0.5269533977675209,\n","  \"gen_lr\": 0.00042406256400739315,\n","  \"sup_criterion\": nn.MSELoss()\n","}\n","'''"]},{"cell_type":"markdown","metadata":{"id":"jpBoGKmTL-eR"},"source":["## GANs + Cycle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4jrm83ALpTH","executionInfo":{"status":"ok","timestamp":1747677226461,"user_tz":300,"elapsed":149,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"ee57c902-8a17-47ac-a30d-e9025287503d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# this config looks decent at step 1100, worse at 1440, better at 1900, etc. (variable). It isn\\'t blocky.\\nconfig_GAN_IS_old = {\\n    \"batch_size\": 82,\\n    \"gen_adv_criterion\": nn.BCEWithLogitsLoss(),\\n    \"gen_lr\": 3.365297856241193e-05,\\n    \"gen_b1\": 0.11790916451301556,\\n    \"gen_b2\": 0.999,\\n\\n    \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\\n    \"IS_normalize\": False, # FALSE\\n    \\'IS_scale\\': 1, # 1\\n    \\'IS_gen_mult\\': 3,\\n    \\'IS_gen_fill\\': 0,\\n    \\'IS_gen_neck\\': 5, # Wide neck\\n    \"IS_gen_z_dim\": 115,\\n    \\'IS_layer_norm\\': \\'instance\\',\\n    \\'IS_pad_mode\\': \\'reflect\\',\\n    \\'IS_dropout\\': False,\\n    \\'IS_exp_kernel\\': 4,\\n    \"IS_gen_final_activ\": nn.Tanh(), # nn.Tanh()\\n    \"IS_disc_patchGAN\": True,\\n    \"IS_gen_hidden_dim\": 16,\\n    \"IS_disc_hidden_dim\": 19,\\n    \"IS_disc_lr\": 0.00020392229473545828,\\n    \"IS_disc_b1\": 0.35984156365558084,\\n    \"IS_disc_b2\": 0.999,\\n    }\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["## Best Configs for GANs ##\n","\n","config_GAN_SI = { # Older, this still outperforms the more recent tuning\n","    'SI_disc_adv_criterion': nn.MSELoss(),\n","    'SI_normalize': True, # True\n","    'SI_scale': 1400, # 1      # Added later\n","    'SI_gen_neck': 1, # 1\n","    'SI_layer_norm': 'batch',\n","    'SI_pad_mode': 'reflect',\n","    'SI_dropout': False,\n","    'SI_exp_kernel': 3,\n","    'SI_gen_fill': 0,\n","    'SI_gen_mult': 1.41,\n","    'SI_gen_z_dim': 115,\n","    'SI_gen_final_activ': nn.Sigmoid(),\n","    'SI_disc_patchGAN': True,\n","    'SI_gen_hidden_dim': 46,\n","    'SI_disc_hidden_dim': 25,\n","    'SI_disc_b1': 0.102081,\n","    'SI_disc_b2': 0.999,\n","    # Gets Overwritten Below\n","    'SI_disc_lr': 0.000167384,\n","    'batch_size': 78,\n","    'gen_adv_criterion': nn.MSELoss(),\n","    'gen_lr': 0.000167384,\n","    'gen_b1': 0.102081,\n","    'gen_b2': 0.999,\n","    }\n","\n","config_GAN_IS = { # new, looks good by step 400, somewhat blocky. May be outperforming config_GAN_SI\n","  \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\n","  \"IS_disc_b1\": 0.3335905891003811,\n","  \"IS_disc_b2\": 0.999,\n","  \"IS_disc_hidden_dim\": 11,\n","  \"IS_disc_patchGAN\": True,\n","  \"IS_dropout\": False,\n","  \"IS_exp_kernel\": 3,\n","  \"IS_gen_fill\": 0,\n","  \"IS_gen_final_activ\": None,\n","  \"IS_gen_hidden_dim\": 15,\n","  \"IS_gen_mult\": 3,\n","  \"IS_gen_neck\": 11,\n","  \"IS_gen_z_dim\": 5,\n","  \"IS_layer_norm\": \"instance\",\n","  \"IS_normalize\": False,\n","  \"IS_pad_mode\": \"reflect\",\n","  \"IS_scale\": 1,\n","  # Gets Overwritten Below\n","  \"IS_disc_lr\": 0.00021705437338035208,\n","  \"batch_size\": 88,\n","  \"gen_adv_criterion\": nn.MSELoss(),\n","  \"gen_b1\": 0.46293297275979556,\n","  \"gen_b2\": 0.999,\n","  \"gen_lr\": 0.00042810775483742824\n","}\n","\n","'''\n","# this config looks decent at step 1100, worse at 1440, better at 1900, etc. (variable). It isn't blocky.\n","config_GAN_IS_old = {\n","    \"batch_size\": 82,\n","    \"gen_adv_criterion\": nn.BCEWithLogitsLoss(),\n","    \"gen_lr\": 3.365297856241193e-05,\n","    \"gen_b1\": 0.11790916451301556,\n","    \"gen_b2\": 0.999,\n","\n","    \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\n","    \"IS_normalize\": False, # FALSE\n","    'IS_scale': 1, # 1\n","    'IS_gen_mult': 3,\n","    'IS_gen_fill': 0,\n","    'IS_gen_neck': 5, # Wide neck\n","    \"IS_gen_z_dim\": 115,\n","    'IS_layer_norm': 'instance',\n","    'IS_pad_mode': 'reflect',\n","    'IS_dropout': False,\n","    'IS_exp_kernel': 4,\n","    \"IS_gen_final_activ\": nn.Tanh(), # nn.Tanh()\n","    \"IS_disc_patchGAN\": True,\n","    \"IS_gen_hidden_dim\": 16,\n","    \"IS_disc_hidden_dim\": 19,\n","    \"IS_disc_lr\": 0.00020392229473545828,\n","    \"IS_disc_b1\": 0.35984156365558084,\n","    \"IS_disc_b2\": 0.999,\n","    }\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kldB7WhyQCDF","executionInfo":{"status":"ok","timestamp":1747677226525,"user_tz":300,"elapsed":60,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"7252f478-117e-46f4-e2f1-a964c5cdf001"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n## Was best for training the CycleGAN all at once ##\\n# Below config is \"SM_1662\", the lowest optim_metric in 9h run, 90x90 symmetrical (not symmetrized parameters) networks.\\n# It was trained on IO_channels==3 but seems to work fine for IO_channels==1. Also, both discriminators use the same architecture,\\n# which is really better suited for the sinogram (Disc_S_90).\\n#\\n# Lessons Learned:\\n# 1) Utilized: different size necks, final activations, channels, patchGAN\\n# 2) NOT Uilized: fill Conv2d layers, different adv_criterion (for disc loss), different normalizations\\n\\nconfig={ # Symmetrize == FALSE (final activations don\\'t match). This was the best over full tune train time (9 hours). Use two Sinogram discriminators.\\n\\'batch_size\\': 107,\\n\\'gen_b1\\': 0.339,\\n\\'gen_b2\\': 0.999,\\n\\'gen_lr\\': 0.000103,\\n\"cycle_criterion\": nn.L1Loss(),\\n\"sup_criterion\": nn.L1Loss(),\\n\"gen_adv_criterion\": nn.KLDivLoss(),\\n\"lambda_adv\": 1,\\n\"lambda_cycle\": 2,\\n\"lambda_sup\": 0,\\n\\n\"IS_disc_adv_criterion\": nn.MSELoss(),\\n\"IS_disc_b1\": 0.19520417398460468,\\n\"IS_disc_b2\": 0.999,\\n\"IS_disc_hidden_dim\": 23,\\n\"IS_disc_lr\": 0.0022230036964765274,  # disc_lr is 10X faster than SI\\n\"IS_disc_patchGAN\": False,            # true for SI (make sense, images can be more true/false in patches)\\n\"IS_gen_fill\": 0,                     # fill=0 for both IS and SI\\n\"IS_gen_final_activ\": nn.Sigmoid(),   # tuned final activations opposite than for GANs\\n\"IS_gen_hidden_dim\": 8,               # IS much less complex than SI (8 vs 16 hidden_dim)\\n\"IS_gen_mult\": 3,                     # mult=3 for both IS and SI\\n\"IS_gen_z_dim\": 5,\\n\"IS_normalize\": True,                 # both are normalized here\\n\"IS_scale\": 1,                        # OMG, this is weird. We are normalizing both, but the SI image scale is 1400x the IS. Could this by why final activation is now Sigmoid?\\n\\n\\'IS_layer_norm\\': \\'batch\\', # Batch\\n\\'IS_pad_mode\\': \\'reflect\\',\\n\\'IS_dropout\\': False,\\n\"IS_gen_neck\": 5,            # 2\\n\\'IS_exp_kernel\\': 4,          # 4\\n\\n\"SI_disc_adv_criterion\": nn.MSELoss(),\\n\"SI_disc_b1\": 0.30423542819878224,\\n\"SI_disc_b2\": 0.999,\\n\"SI_disc_hidden_dim\": 23,\\n\"SI_disc_lr\": 0.00020737432489437965,\\n\"SI_disc_patchGAN\": True,\\n\"SI_gen_fill\": 0,\\n\"SI_gen_final_activ\": nn.Tanh(),\\n\"SI_gen_hidden_dim\": 22,\\n\"SI_gen_mult\": 3,\\n\"SI_gen_z_dim\": 1195,                 # Represents an 8x drop in information into narrowest part of neck\\n\"SI_normalize\": True, # True\\n\"SI_scale\": 1400,\\n\\n\\'SI_layer_norm\\': \\'batch\\',\\n\\'SI_pad_mode\\': \\'reflect\\',\\n\\'SI_dropout\\': False,\\n\"SI_gen_neck\": 1,            # 1\\n\\'SI_exp_kernel\\': 4,          # 4\\n}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["## I've looked at configurations in 'search-CycleGAN' through (and including) '4a92'\n","\n","config_CYCLEGAN={ # Works, yeah! (\"4a92\")\n","    \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\n","    \"IS_disc_b1\": 0.3335905891003811,\n","    \"IS_disc_b2\": 0.999,\n","    \"IS_disc_hidden_dim\": 11,\n","    \"IS_disc_lr\": 0.0006554051278271163,\n","    \"IS_disc_patchGAN\": True,\n","    \"IS_dropout\": False,\n","    \"IS_exp_kernel\": 3,\n","    \"IS_gen_fill\": 0,\n","    \"IS_gen_final_activ\": None,\n","    \"IS_gen_hidden_dim\": 15,\n","    \"IS_gen_mult\": 3,\n","    \"IS_gen_neck\": 11,\n","    \"IS_gen_z_dim\": 5,\n","    \"IS_layer_norm\": \"instance\",\n","    \"IS_normalize\": False,\n","    \"IS_pad_mode\": \"reflect\",\n","    \"IS_scale\": 1,\n","    \"SI_disc_adv_criterion\": nn.MSELoss(),\n","    \"SI_disc_b1\": 0.102081,\n","    \"SI_disc_b2\": 0.999,\n","    \"SI_disc_hidden_dim\": 25,\n","    \"SI_disc_lr\": 0.0005793968896471209,\n","    \"SI_disc_patchGAN\": True,\n","    \"SI_dropout\": False,\n","    \"SI_exp_kernel\": 3,\n","    \"SI_gen_fill\": 0,\n","    \"SI_gen_final_activ\": nn.Sigmoid(),\n","    \"SI_gen_hidden_dim\": 46,\n","    \"SI_gen_mult\": 1.41,\n","    \"SI_gen_neck\": 1,\n","    \"SI_gen_z_dim\": 115,\n","    \"SI_layer_norm\": \"batch\",\n","    \"SI_normalize\": True,\n","    \"SI_pad_mode\": \"reflect\",\n","    \"SI_scale\": 1400,\n","    \"batch_size\": 91,\n","    \"cycle_criterion\": nn.MSELoss(),\n","    \"gen_adv_criterion\": nn.MSELoss(),\n","    \"gen_b1\": 0.1610671788990834,\n","    \"gen_b2\": 0.999,\n","    \"gen_lr\": 0.0023450700434171526,\n","    \"lambda_adv\": 1,\n","    \"lambda_cycle\": 1, #1\n","    \"lambda_sup\": 0, # 0\n","    \"sup_criterion\": nn.L1Loss()\n","    }\n","\n","'''\n","## Was best for training the CycleGAN all at once ##\n","# Below config is \"SM_1662\", the lowest optim_metric in 9h run, 90x90 symmetrical (not symmetrized parameters) networks.\n","# It was trained on IO_channels==3 but seems to work fine for IO_channels==1. Also, both discriminators use the same architecture,\n","# which is really better suited for the sinogram (Disc_S_90).\n","#\n","# Lessons Learned:\n","# 1) Utilized: different size necks, final activations, channels, patchGAN\n","# 2) NOT Uilized: fill Conv2d layers, different adv_criterion (for disc loss), different normalizations\n","\n","config={ # Symmetrize == FALSE (final activations don't match). This was the best over full tune train time (9 hours). Use two Sinogram discriminators.\n","'batch_size': 107,\n","'gen_b1': 0.339,\n","'gen_b2': 0.999,\n","'gen_lr': 0.000103,\n","\"cycle_criterion\": nn.L1Loss(),\n","\"sup_criterion\": nn.L1Loss(),\n","\"gen_adv_criterion\": nn.KLDivLoss(),\n","\"lambda_adv\": 1,\n","\"lambda_cycle\": 2,\n","\"lambda_sup\": 0,\n","\n","\"IS_disc_adv_criterion\": nn.MSELoss(),\n","\"IS_disc_b1\": 0.19520417398460468,\n","\"IS_disc_b2\": 0.999,\n","\"IS_disc_hidden_dim\": 23,\n","\"IS_disc_lr\": 0.0022230036964765274,  # disc_lr is 10X faster than SI\n","\"IS_disc_patchGAN\": False,            # true for SI (make sense, images can be more true/false in patches)\n","\"IS_gen_fill\": 0,                     # fill=0 for both IS and SI\n","\"IS_gen_final_activ\": nn.Sigmoid(),   # tuned final activations opposite than for GANs\n","\"IS_gen_hidden_dim\": 8,               # IS much less complex than SI (8 vs 16 hidden_dim)\n","\"IS_gen_mult\": 3,                     # mult=3 for both IS and SI\n","\"IS_gen_z_dim\": 5,\n","\"IS_normalize\": True,                 # both are normalized here\n","\"IS_scale\": 1,                        # OMG, this is weird. We are normalizing both, but the SI image scale is 1400x the IS. Could this by why final activation is now Sigmoid?\n","\n","'IS_layer_norm': 'batch', # Batch\n","'IS_pad_mode': 'reflect',\n","'IS_dropout': False,\n","\"IS_gen_neck\": 5,            # 2\n","'IS_exp_kernel': 4,          # 4\n","\n","\"SI_disc_adv_criterion\": nn.MSELoss(),\n","\"SI_disc_b1\": 0.30423542819878224,\n","\"SI_disc_b2\": 0.999,\n","\"SI_disc_hidden_dim\": 23,\n","\"SI_disc_lr\": 0.00020737432489437965,\n","\"SI_disc_patchGAN\": True,\n","\"SI_gen_fill\": 0,\n","\"SI_gen_final_activ\": nn.Tanh(),\n","\"SI_gen_hidden_dim\": 22,\n","\"SI_gen_mult\": 3,\n","\"SI_gen_z_dim\": 1195,                 # Represents an 8x drop in information into narrowest part of neck\n","\"SI_normalize\": True, # True\n","\"SI_scale\": 1400,\n","\n","'SI_layer_norm': 'batch',\n","'SI_pad_mode': 'reflect',\n","'SI_dropout': False,\n","\"SI_gen_neck\": 1,            # 1\n","'SI_exp_kernel': 4,          # 4\n","}\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Dt-bnmQuLr12"},"source":["## Search Spaces"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTSHCV3G6mSz"},"outputs":[],"source":["#################################################################################################################################################################\n","## (config_RAY_SI OR config_RAY_IS) gets combined with (config_RAY_SUP or config_RAY_GAN) to form a single hyperparameter space for searching a single network ##\n","#################################################################################################################################################################\n","\n","## Note: For the Coursera CycleGAN:\n","# gen_adv_criterion = disc_adv_criterion = nn.MSELoss()\n","# cycle_criterion = ident_criterion = nn.L1Loss()\n","# for notes on momentum, see: https://distill.pub/2017/momentum/\n","\n","config_RAY_SI = { # Dictionary for Generator: Sinogram-->Image\n","    # Data Loading\n","    'SI_normalize': tune.choice([True, False]),                 # Normalize dataloader outputs and outputs of generator?\n","                                                                # If so, the pixel values in the image all add up to 1.\n","    'SI_scale': 90*90,                                          # If normalizing the pixel images, multiply images by this value.\n","                                                                # The pixel values will then add up to this number.\n","    # Generator Network\n","    'SI_gen_mult': tune.uniform(1.1, 4),                        # Factor by which to multiply channels/block as one moves twowards the center of the network\n","    'SI_gen_fill': tune.choice([0,1,2]),                        # Number of constant-sized Conv2d layers/block\n","    'SI_gen_neck': tune.choice([1,5,11]),                       # Size of network neck: 1 = smallest, 11 = largest\n","    'SI_gen_z_dim': tune.lograndint(64, 4000),                  # If network utilizes smallest neck size (1x1 = a dense layer), this is the number of channels in the neck\n","    'SI_layer_norm': tune.choice(['batch', 'instance','none']), # Layer normalization type ('none' seems to be, in practice, never chosen by tuning)\n","    'SI_pad_mode': tune.choice(['zeros', 'reflect']),           # Padding type\n","    'SI_dropout': tune.choice([True,False]),                    # Implement dropout in network? (without cross-validation, this is likely never chosen)\n","    'SI_exp_kernel': tune.choice([3,4]),                        # Expanding kernel size: 3x3 or 4x4\n","    'SI_gen_final_activ':  tune.choice([nn.Tanh(), nn.Sigmoid(), None]), # Options: tune.choice([nn.Tanh(), nn.Sigmoid(), None]),\n","                                                                # Could add: nn.ReLU6(), nn.Hardsigmoid(), nn.ReLU(), nn.PReLU(), None\n","    'SI_gen_hidden_dim': tune.lograndint(2, 30),                # Generator channel scaling factor. Larger numbers give more total channels.\n","    # Discriminator Network\n","    'SI_disc_hidden_dim': tune.lograndint(10, 30),              # Discriminator channel scaling factor\n","    'SI_disc_patchGAN': tune.choice([True, False]),             # Use PatchGAN or not\n","    # Discriminator Optimizer\n","    'SI_disc_lr': tune.loguniform(1e-4,1e-2),\n","    'SI_disc_b1': tune.loguniform(0.1, 0.999),\n","    'SI_disc_b2': tune.loguniform(0.1, 0.999),\n","    'SI_disc_adv_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss()]), # Possible options: tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.BCEWithLogitsLoss()]),\n","    }\n","\n","config_RAY_IS = { # Dictionary for Generator: Image-->Sinogram\n","    # Data Loading\n","    'IS_normalize': False, # tune.choice([True, False]), # Normalize outputs or not\n","    'IS_scale': 90*90,\n","    # Generator Network\n","    'IS_gen_mult': tune.uniform(1.1, 4),\n","    'IS_gen_fill': tune.choice([0,1,2]),\n","    'IS_gen_neck': tune.choice([1,5,11]),\n","    'IS_gen_z_dim': tune.lograndint(64, 4000),\n","    'IS_layer_norm': tune.choice(['batch', 'instance','none']),\n","    'IS_pad_mode': tune.choice(['zeros', 'reflect']),\n","    'IS_dropout': tune.choice([True,False]),\n","    'IS_exp_kernel': tune.choice([3,4]),\n","    'IS_gen_final_activ': tune.choice([nn.Tanh(), nn.Sigmoid(), None]), # nn.ReLU6(), nn.Hardsigmoid(), nn.ReLU(), nn.PReLU(), None\n","    'IS_gen_hidden_dim': tune.lograndint(2, 30),\n","    # Discriminator Network\n","    'IS_disc_hidden_dim': tune.lograndint(10, 30),\n","    'IS_disc_patchGAN': tune.choice([True, False]),\n","    # Discriminator Optimizer\n","    'IS_disc_lr': tune.loguniform(1e-4,1e-2),\n","    'IS_disc_b1': tune.loguniform(0.1, 0.999),\n","    'IS_disc_b2': tune.loguniform(0.1, 0.999),\n","    'IS_disc_adv_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss()]),\n","    }\n","\n","config_RAY_SUP = { # This dictionary may be merged with either config_RAY_IS or config_RAY_SI to form a single dictionary for supervisory learning\n","    # NEW: New parameters added to config_RAY_SI (related to generator optimizer)\n","    'batch_size': tune.choice([32, 64, 128, 256, 512]), # tune.lograndint(30, 400),\n","    'gen_lr': tune.loguniform(1e-4,1e-2),\n","    'gen_b1': tune.loguniform(0.1, 0.999),\n","    'gen_b2': tune.loguniform(0.1, 0.999),\n","    'sup_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss(), nn.L1Loss(), nn.KLDivLoss(reduction='batchmean')]), # Not SI or IS because used for both\n","    # OVERWRITES: overwrites values from config_RAY_SI or config_RAY_IS. This is done so time isn't wasted looking for unused hyperparameters.\n","    'SI_disc_hidden_dim': 1,\n","    'SI_disc_patchGAN': 1,\n","    'SI_disc_lr': 1,\n","    'SI_disc_b1': 1,\n","    'SI_disc_b2': 1,\n","    'SI_disc_adv_criterion': 1,\n","    'IS_disc_hidden_dim': 1,\n","    'IS_disc_patchGAN': 1,\n","    'IS_disc_lr': 1,\n","    'IS_disc_b1': 1,\n","    'IS_disc_b2': 1,\n","    'IS_disc_adv_criterion': 1,\n","    }\n","\n","config_RAY_GAN = { # This is MERGED with either config_RAY_IS or config_RAY_SI to form a single dictionary for a generative adversarial network.\n","    # NEW\n","    'batch_size': tune.choice([32, 64, 128, 256, 512]),  #tune.lograndint(30, 400),\n","    'gen_lr': tune.loguniform(1e-4,1e-2),\n","    'gen_b1': tune.loguniform(0.1, 0.999),\n","    'gen_b2': 0.999, #tune.loguniform(0.1, 0.999),\n","    'gen_adv_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss()]),\n","    }\n","\n","config_GAN_RAY_cycle = { # Mixed New/Overwrites (when combined with config_SI/config_IS) to form a single dictionary for a cycle-consistent generative adversarial network.\n","    # NEW\n","    'cycle_criterion': tune.choice([nn.MSELoss(), nn.L1Loss()]),\n","    'sup_criterion': tune.choice([nn.MSELoss(), nn.KLDivLoss(reduction='batchmean'), nn.L1Loss(), nn.BCEWithLogitsLoss()]),\n","    'lambda_adv': 1,\n","    'lambda_sup': 0,\n","    'lambda_cycle': 1,\n","    # OVERWRITES\n","    'gen_adv_criterion': nn.MSELoss(), #tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.BCEWithLogitsLoss()]),\n","    'IS_disc_lr': tune.loguniform(1e-4,1e-2),\n","    'SI_disc_lr': tune.loguniform(1e-4,1e-2),\n","    'batch_size': tune.choice([32, 64, 128, 256, 512]),\n","    'gen_lr': tune.loguniform(0.5e-4,1e-2),\n","    'gen_b1': tune.loguniform(0.1, 0.999),\n","    'gen_b2': 0.999, #tune.loguniform(0.1, 0.999),\n","    }\n","\n","config_SUP_RAY_cycle = { # Mixed New/Overwrites (when combined with config_SI/config_IS) to form a single dictionary for a cycle-consistent, partially supervised network.\n","    # NEW\n","    'cycle_criterion': tune.choice([nn.MSELoss(), nn.L1Loss()]),\n","    'lambda_adv': 0,\n","    'lambda_sup': 1,\n","    'lambda_cycle':  tune.uniform(0, 10),\n","    # OVERWRITES\n","    'batch_size': tune.choice([32, 64, 128, 256, 512]),\n","    'gen_lr': tune.loguniform(0.5e-4,1e-2),\n","    'gen_b1': tune.loguniform(0.1, 0.999), # DCGan uses 0.5, https://distill.pub/2017/momentum/\n","    'gen_b2': tune.loguniform(0.1, 0.999),\n","    'sup_criterion': tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.L1Loss(), nn.BCEWithLogitsLoss()]),\n","    # NOT USED\n","    'gen_adv_criterion': nn.MSELoss(), #tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.BCEWithLogitsLoss()]),\n","    'IS_disc_lr': 1e-4, #tune.loguniform(1e-4,1e-2),\n","    'SI_disc_lr': 1e-4, #tune.loguniform(1e-4,1e-2),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"fQ_qmj4fJgK7"},"source":["## Set Correct Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUppJO1XJdof","executionInfo":{"status":"ok","timestamp":1747677226588,"user_tz":300,"elapsed":29,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"295a3303-5847-4434-fe93-f4c3fcc1356b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'SI_normalize': <ray.tune.search.sample.Categorical object at 0x7f18738031d0>, 'SI_scale': 8100, 'SI_gen_mult': <ray.tune.search.sample.Float object at 0x7f18737f8610>, 'SI_gen_fill': <ray.tune.search.sample.Categorical object at 0x7f18738009d0>, 'SI_gen_neck': <ray.tune.search.sample.Categorical object at 0x7f187bfa56d0>, 'SI_gen_z_dim': <ray.tune.search.sample.Integer object at 0x7f1873637bd0>, 'SI_layer_norm': <ray.tune.search.sample.Categorical object at 0x7f1a00253090>, 'SI_pad_mode': <ray.tune.search.sample.Categorical object at 0x7f1873670b50>, 'SI_dropout': <ray.tune.search.sample.Categorical object at 0x7f18736a8a10>, 'SI_exp_kernel': <ray.tune.search.sample.Categorical object at 0x7f18736a8b10>, 'SI_gen_final_activ': <ray.tune.search.sample.Categorical object at 0x7f18736a8dd0>, 'SI_gen_hidden_dim': <ray.tune.search.sample.Integer object at 0x7f187bf9da90>, 'SI_disc_hidden_dim': 1, 'SI_disc_patchGAN': 1, 'SI_disc_lr': 1, 'SI_disc_b1': 1, 'SI_disc_b2': 1, 'SI_disc_adv_criterion': 1, 'batch_size': <ray.tune.search.sample.Categorical object at 0x7f18736aad10>, 'gen_lr': <ray.tune.search.sample.Float object at 0x7f18736aad90>, 'gen_b1': <ray.tune.search.sample.Float object at 0x7f18736aae10>, 'gen_b2': <ray.tune.search.sample.Float object at 0x7f18736aae90>, 'sup_criterion': <ray.tune.search.sample.Categorical object at 0x7f18736ab290>, 'IS_disc_hidden_dim': 1, 'IS_disc_patchGAN': 1, 'IS_disc_lr': 1, 'IS_disc_b1': 1, 'IS_disc_b2': 1, 'IS_disc_adv_criterion': 1}\n"]}],"source":["## Combine Dictionaries ##\n","if run_mode=='train' or 'test' or 'visualize':\n","    if train_type=='SUP':\n","        if train_SI==True:\n","            config = config_SUP_SI\n","        if train_SI==False:\n","            config = config_SUP_IS\n","    if train_type=='GAN':\n","        if train_SI==True:\n","            config = config_GAN_SI\n","        if train_SI==False:\n","            config = config_GAN_IS\n","    if train_type=='CYCLEGAN':\n","        config = config_CYCLEGAN\n","    if train_type=='CYCLESUP':\n","        config = config_CYCLESUP\n","\n","if run_mode=='tune':\n","    if train_type=='SUP':\n","        if train_SI==True:\n","            config = {**config_RAY_SI, **config_RAY_SUP}\n","        if train_SI==False:\n","            config = {**config_RAY_IS, **config_RAY_SUP}\n","    if train_type=='GAN':\n","        if train_SI==True:\n","            config = {**config_RAY_SI, **config_RAY_GAN}\n","        if train_SI==False:\n","            config = {**config_RAY_IS, **config_RAY_GAN}\n","    if train_type=='CYCLESUP':\n","        config = {**config_SUP_SI, **config_SUP_IS, **config_SUP_RAY_cycle}\n","    if train_type=='CYCLEGAN':\n","        config = {**config_GAN_SI, **config_GAN_IS, **config_GAN_RAY_cycle}\n","\n","print(config)"]},{"cell_type":"markdown","metadata":{"id":"wqRdJRNYmpk6"},"source":["# Classes"]},{"cell_type":"markdown","metadata":{"id":"SR6WK0lIERty"},"source":["## Dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEAomee7EWjg"},"outputs":[],"source":["def NpArrayDataLoader(image_array, sino_array, config, image_size = 90, sino_size=90, image_channels=1, sino_channels=1, augment=False, index=0):\n","    '''\n","    Function to load an image and a sinogram. Returns 4 pytorch tensors: the original dataset sinogram and image,\n","    and scaled and (optionally) normalized sinograms and images.\n","\n","    image_array:    image numpy array\n","    sino_array:     sinogram numpy array\n","    config:         configuration dictionary with hyperparameters\n","    image_size:     shape to resize image to (for output)\n","    image_channels: number of channels for output images\n","    sino_size:      shape to resize sinograms to (for output)\n","    sino_channels:  number of channels in output sinograms\n","    augment:        perform data augmentation?\n","    index:          index of the image/sinogram pair to grab\n","    '''\n","    ## Set Normalization Variables ##\n","    if (train_type=='GAN') or (train_type=='SUP'):\n","        if train_SI==True:\n","            SI_normalize=config['SI_normalize']\n","            SI_scale=config['SI_scale']\n","            IS_normalize=False     # If the Sinogram-->Image network (SI) is being trained, don't waste time normalizing sinograms\n","            IS_scale=1             # If the Sinogram-->Image network (SI) is being trained, don't waste time scaling sinograms\n","        else:\n","            IS_normalize=config['IS_normalize']\n","            IS_scale=config['IS_scale']\n","            SI_normalize=False\n","            SI_scale=1\n","    else: # If a cycle-consistent network, normalize & scale everything\n","        IS_normalize=config['IS_normalize']\n","        SI_normalize=config['SI_normalize']\n","        IS_scale=config['IS_scale']\n","        SI_scale=config['SI_scale']\n","\n","    ## Data Augmentation Functions ##\n","    def RandRotate(image_multChannel, sinogram_multChannel):\n","            '''\n","            Function for randomly rotating an image and its sinogram. If the image intersects the edge of the FOV, no rotation is applied.\n","\n","            image_multChannel:    image to rotate. Shape: (C, H, W)\n","            sinogram_multChannel: sinogram to rotate. Shape: (C, H, W)\n","            '''\n","\n","\n","        def IntersectCircularBorder(image):\n","            '''\n","            Function for determining whether an image itersects a circular boundary inscribed within the square FOV.\n","            This function is not currently used.\n","            '''\n","            y_max = image.shape[1]\n","            x_max = image.shape[2]\n","\n","            r_max = y_max/2.0\n","            x_center = (x_max-1)/2.0 # the -1 comes from the fact that the coordinates of a pixel start at 0, not 1\n","            y_center = (y_max-1)/2.0\n","\n","            margin_sum = 0\n","            for y in range(0, y_max):\n","                for x in range(0, x_max):\n","                    if r_max < ((x-x_center)**2 + (y-y_center)**2)**0.5 :\n","                        margin_sum += torch.sum(image[:,y,x]).item()\n","\n","            return_value = True if margin_sum == 0 else False\n","            return return_value\n","\n","        def IntersectSquareBorder(image):\n","            '''\n","            Function for determining whether the image intersects the edge of the square FOV. If it does not, then the image\n","            is fully specified by the sinogram and data augmentation can be performed. If the image does\n","            intersect the edge of the image then some of it may be cropped outside the FOV. In this case,\n","            augmentation via rotation should not be performed as the rotated image may not be fully described by the sinogram.\n","            Looks at all channels in the image.\n","            '''\n","            max_idx = image.shape[1]-1\n","            margin_sum = torch.sum(image[:,0,:]).item() + torch.sum(image[:,max_idx,:]).item() \\\n","                        +torch.sum(image[:,:,0]).item() + torch.sum(image[:,:,max_idx]).item()\n","            return_value = False if margin_sum == 0 else True\n","            return return_value\n","\n","        if IntersectSquareBorder(image_multChannel) == False:\n","            bins = sinogram_multChannel.shape[2]\n","            bins_shifted = np.random.randint(0, bins)\n","            angle = int(bins_shifted * 180/bins)\n","\n","            image_multChannel = transforms.functional.rotate(image_multChannel, angle, fill=0) # Rotate image. Fill in unspecified pixels with zeros.\n","            sinogram_multChannel = torch.roll(sinogram_multChannel, bins_shifted, dims=(2,)) # Cycle (or 'Roll') sinogram by that angle along dimension 2.\n","            sinogram_multChannel[:,:, 0:bins_shifted] = torch.flip(sinogram_multChannel[:,:,0:bins_shifted], dims=(1,)) # flip the cycled portion of the sinogram vertically\n","\n","        return image_multChannel, sinogram_multChannel\n","\n","    def VerticalFlip(image_multChannel, sinogram_multChannel):\n","        image_multChannel = torch.flip(image_multChannel,dims=(1,)) # Flip image vertically\n","        sinogram_multChannel = torch.flip(sinogram_multChannel,dims=(1,2)) # Flip sinogram horizontally and vertically\n","        return image_multChannel, sinogram_multChannel\n","\n","    def HorizontalFlip(image_multChannel, sinogram_multChannel):\n","        image_multChannel = torch.flip(image_multChannel, dims=(2,)) # Flip image horizontally\n","        sinogram_multChannel = torch.flip(sinogram_multChannel, dims=(2,)) # Flip sinogram horizontally\n","        return image_multChannel, sinogram_multChannel\n","\n","    ## Select Data, Convert to Pytorch Tensors ##\n","    image_multChannel = torch.from_numpy(image_array[index,:]) # image_multChannel.shape = (C, X, Y)\n","    sinogram_multChannel = torch.from_numpy(sino_array[index,:]) # sinogram_multChannel.shape = (C, X, Y)\n","\n","    ## Run Data Augmentation on Selected Data. ##\n","    if augment==True:\n","        image_multChannel, sinogram_multChannel = RandRotate(image_multChannel, sinogram_multChannel)           # Always rotates image by a random angle\n","        if np.random.choice([True, False]): # Half of the time, flips the image vertically\n","            image_multChannel, sinogram_multChannel = VerticalFlip(image_multChannel, sinogram_multChannel)\n","        if np.random.choice([True, False]): # Half of the time, flips the image horizontally\n","            image_multChannel, sinogram_multChannel = HorizontalFlip(image_multChannel, sinogram_multChannel)\n","\n","    ## Create A Set of Resized Outputs ##\n","    sinogram_multChannel_resize = transforms.Resize(size = (sino_size, sino_size), antialias=True)(sinogram_multChannel)\n","    image_multChannel_resize    = transforms.Resize(size = (image_size, image_size), antialias=True)(image_multChannel)\n","\n","    ## (Optional) Normalize Resized Outputs Along Channel Dimension ##\n","    if SI_normalize:\n","        a = torch.reshape(image_multChannel_resize, (image_channels,-1))\n","        a = nn.functional.normalize(a, p=1, dim = 1)\n","        image_multChannel_resize = torch.reshape(a, (image_channels, image_size, image_size))\n","    if IS_normalize:\n","        a = torch.reshape(sinogram_multChannel_resize, (sino_channels,-1))                     # Flattens each sinogram. Each channel is normalized.\n","        a = nn.functional.normalize(a, p=1, dim = 1)                      # Normalizes along dimension 1 (values for each of the 3 channels)\n","        sinogram_multChannel_resize = torch.reshape(a, (sino_channels, sino_size, sino_size))  # Reshapes sinograms back into squares.\n","\n","    ## Adjust Output Channels of Resized Outputs ##\n","    if image_channels==1:\n","        image_out = image_multChannel_resize                 # For image_channels = 1, the image is just left alone\n","    else:\n","        image_out = image_multChannel_resize.repeat(image_channels,1,1)   # This chould be altered to account for RGB images, etc.\n","\n","    if sino_channels==1:\n","        sino_out = sinogram_multChannel_resize[0:1,:]        # Selects 1st sinogram channel only. Using 0:1 preserves the channels dimension.\n","    else:\n","        sino_out = sinogram_multChannel_resize               # Keeps full sinogram with all channels\n","\n","    # Returns both original and altered sinograms and images, assigned to CPU or GPU\n","    return sinogram_multChannel.to(device), IS_scale*sino_out.to(device), image_multChannel.to(device), SI_scale*image_out.to(device)\n","\n","class NpArrayDataSet(Dataset):\n","    '''\n","    Class for loading data from .np files, given file directory strings and set of optional transformations.\n","    In the dataset used in our first two conference papers, the data repeat every 17500 steps but with different augmentations.\n","    For the dataset with FORE rebinning, the dataset contains no augmented examples; all augmentation is performed on the fly.\n","    '''\n","    def __init__(self, image_path, sino_path, config, image_size = 90, sino_size=90, image_channels=1, sino_channels=1,\n","                 augment=False, offset=0, num_examples=-1, sample_division=1):\n","        '''\n","        image_path:         path to images in data set\n","        sino_path:          path to sinograms in data set\n","        config:             configuration dictionary with hyperparameters\n","        image_size:         shape to resize image to (for output)\n","        image_channels:     number of channels in images\n","        sino_size:          shape to resize sinograms to (for output)\n","        sino_channels:      number of channels in sinograms (for photopeak sinograms, this is 1)\n","        augment:            Set True to perform on-the-fly augmentation of data set. Set False to not perform augmentation.\n","        offset:             To begin dataset at beginning of the datafile, set offset=0. To begin on the second image, offset = 1, etc.\n","        num_examples:       Max number of examples to load into dataset. Set to -1 to load the maximum number from the numpy array.\n","        sample_division:    set to 1 to use every example, 2 to use every other example, etc. (Ex: if sample_division=2, the dataset will be half the size.)\n","        '''\n","\n","        ## Load Data to Arrays ##\n","        image_array = np.load(image_path, mmap_mode='r')       # We use memmaps to significantly speed up the loading.\n","        sino_array = np.load(sino_path, mmap_mode='r')\n","\n","        ## Set Instance Variables ##\n","        if num_examples==-1:\n","            self.image_array = image_array[offset:,:]\n","            self.sino_array = sino_array[offset:,:]\n","        else:\n","            self.image_array = image_array[offset : offset + num_examples, :]\n","            self.sino_array = sino_array[offset : offset + num_examples, :]\n","\n","        self.config = config\n","        self.image_size = image_size\n","        self.sino_size = sino_size\n","        self.image_channels = image_channels\n","        self.sino_channels = sino_channels\n","        self.augment = augment\n","        self.sample_division = sample_division\n","\n","    def __len__(self):\n","        length = int(len(self.image_array)/sample_division)\n","        return length\n","\n","    def __getitem__(self, idx):\n","\n","        idx = idx*self.sample_division\n","\n","        sino_ground, sino_ground_scaled, image_ground, image_ground_scaled = NpArrayDataLoader(self.image_array, self.sino_array, self.config, self.image_size,\n","                                                                                self.sino_size, self.image_channels, self.sino_channels,\n","                                                                                augment=self.augment, index=idx)\n","\n","        return sino_ground, sino_ground_scaled, image_ground, image_ground_scaled\n","        # Returns both original, as well as altered, sinograms and images"]},{"cell_type":"markdown","metadata":{"id":"XzKaOERZgrrH"},"source":["## Generators"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzHWhXMIgoTV"},"outputs":[],"source":["######################################\n","##### Block Generating Functions #####\n","######################################\n","\n","def contract_block(in_channels, out_channels, kernel_size, stride, padding=0, padding_mode='reflect', fill=0, norm='batch', drop=False):\n","    '''\n","    Function to construct a single \"contracting block.\" Each contracting block consists of one 2D convolutional layer, which decreases\n","    the size (height and width) of the data. There are then up to three 2D convolution layers which do not change the height or width\n","    (e.g. \"constant size layers\").\n","\n","    in_channels:    number of channels at the input of contracting block\n","    out_channels:   number of channels at the output of contracting block\n","    kernel_size:    size of the kernel for the 1st 2D convolutional layer in the contracting block\n","    stride:         stride of the convolution for the 1st 2D convolutional layer in the contracting block\n","    padding:        amount of padding for the the 1st 2D convolutional layer in the contracting block\n","    padding_mode:   padding mode (options: \"zeros\", \"reflect\")\n","    fill:           number of \"constant size\" 2D convolutional layers\n","    norm:           type of layer normalization (\"batch\", \"instance\", or \"none\")\n","    dropout:        include dropout layers in the contracting block? (True or False)\n","    '''\n","\n","    if norm=='batch':    norm = nn.BatchNorm2d(out_channels)\n","    if norm=='instance': norm = nn.InstanceNorm2d(out_channels)\n","    if norm=='none':     norm = nn.Sequential()\n","    dropout = nn.Dropout() if drop==True else nn.Sequential()\n","\n","    # Note: for the contracting block, normalization & dropout follow convolutional layers. For expanding blocks, the order is reversed.\n","    block1 =  nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n","    if fill==0:\n","        block2 = nn.Sequential() # If fill=0, there are no \"constant size\" convolutional layers, and so block2 is empty.\n","    if fill==1:\n","        block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n","    elif fill==2:\n","        block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU(),\n","                                nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n","    elif fill==3:\n","        block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU(),\n","                                nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU(),\n","                                nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n","    return nn.Sequential(block1, block2)\n","\n","def expand_block(in_channels, out_channels, kernel_size=3, stride=2, padding=0, output_padding=0, padding_mode='zeros', fill=0, norm='batch', drop=False, final_layer=False):\n","    '''\n","    Function to construct a single \"expanding block.\" Each expanding block consists of one 2D transposed convolution layer which increases\n","    the size of the incoming data (height and width). There are then up to three 2D convolution layers which do not change the height or\n","    width (e.g. \"constant size layers\").\n","\n","    in_channels:    number of channels at the input of the expanding block\n","    out_channels:   number of channels at the output of the expanding block\n","    kernel_size:    size of the kernel for the 1st 2D transposed convolutional layer in the expanding block\n","    stride:         stride of the convolution for the 1st 2D transposed convolutional layer in the expanding block\n","    padding:        amount of padding for the the 1st 2D transposed convolutional layer in the expanding block\n","    padding_mode:   padding mode (ex: \"zeros\", \"reflect\")\n","    fill:           number of \"constant size\" 2D convolutional layers\n","    norm:           type of layer normalization (\"batch\", \"instance\", or \"none\")\n","    dropout:        include dropout in the expanding block (True or False)\n","    final_layer:    Is this the final layer in the expanding block? (True or False)\n","    '''\n","\n","    if norm=='batch':       norm = nn.BatchNorm2d(out_channels)\n","    if norm=='instance':    norm = nn.InstanceNorm2d(out_channels)\n","    if norm=='none':        norm = nn.Sequential()\n","    dropout = nn.Dropout() if drop==True else nn.Sequential()\n","\n","    # Note: for the expanding block, normalization & dropout precede convolutional layers in blocks 2-3. For expanding blocks, the order is reversed.\n","    block1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, padding_mode=padding_mode)\n","    if fill==0:\n","        block2 = nn.Sequential()\n","    if fill==1:\n","        block2 = nn.Sequential(norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode))\n","    elif fill==2: # For\n","        block2 = nn.Sequential(norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode),\n","                                norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode))\n","    elif fill==3:\n","        block2 = nn.Sequential(norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode),\n","                                norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode),\n","                                norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode))\n","\n","    if final_layer==False: # If not the final layer, I add normalization, dropout and activation.\n","        block3 = nn.Sequential(norm, dropout, nn.ReLU())\n","    else:                  # Otherwise, I leave off the normalization, dropout, and activation. This allows me to do it explicitly\n","                           # at the end of the network using tuned parameters.\n","        block3 = nn.Sequential()\n","    return nn.Sequential(block1, block2, block3)\n","\n","###########################\n","##### Generator Class #####\n","###########################\n","\n","class Generator(nn.Module):\n","    def __init__(self, config, gen_SI=True, input_size=90, input_channels=3, output_channels=3):\n","        '''\n","        A class to generate a 90x90-->90x90 or 180x180-->90x90 encoder-decoder network. The role of each item in the \"config\" dictionary is commented below. In addition, the class constructor takes the following as inputs:\n","\n","        gen_SI:             Equals True if the generator generates images from sinograms. Equals false if the generator generates sinograms from images.\n","                            In a cycle-consistent network, this class generates two networks from the same config dictionary. Hence, the need\n","                            for this parameter.\n","        input_size:         size of the input (90 or 180).\n","        input_channels:     number of generator input channels\n","        output_channels:    number of generator output channels\n","        '''\n","\n","        super(Generator, self).__init__()\n","\n","        ## Set Instance Variables ##\n","        self.output_channels = output_channels\n","\n","        ## If gen_SI == True, we use the \"SI..\" keys from the config dictionary to construct the generator network. ##\n","        if gen_SI:\n","            # The following instance variables are defined since these will be used in the forward() method below. #\n","            self.final_activation = config['SI_gen_final_activ']    # {nn.Tanh(), nn.Sigmoid(), None}\n","                                                                    # Type of activation function employed at the very end of network\n","            self.normalize=config['SI_normalize']                   # {True, False} : Normalization\n","            self.scale=config['SI_scale']                           # Scale factor by which the output is multiplied,\n","                                                                    #    if the output is first normalized\n","\n","            ## The following variables are used in the network constructor, and not the forward() method, so there is no need for instance variables.\n","\n","            neck=config['SI_gen_neck'] #            {1,5,11} :          Width of narrowest part (neck) of the network. The smaller the number, the narrower the neck.\n","            exp_kernel=config['SI_exp_kernel'] #    {3,4} :             Square kernel width (or height) for the expanding part of the network.\n","            z_dim=config['SI_gen_z_dim'] #          (Any real number) : Number of channels in the network neck, if neck=1. If neck=5 or 11, this parameter isn't used.\n","            hidden_dim=config['SI_gen_hidden_dim']# (Any real number) : scales all channels in network by the same linear factor. Larger hidden_dim -->more complex network\n","            fill=config['SI_gen_fill'] #            {0,1,2,3} :         Number of \"constant size\" 2D convolutions in each block\n","            mult=config['SI_gen_mult'] #            (Any real number) : Multiplicative factor by which network channels increase as the layers decrease in height & width\n","            norm=config['SI_layer_norm'] #          {'instance', 'batch', 'none'} : Type of layer normalization\n","            pad=config['SI_pad_mode'] #             {'zeros', 'reflect'} :          Type of padding in each layer/block\n","            drop=config['SI_dropout'] #             {'True', 'False'} :             Whether dropout is used in the network\n","\n","        #If gen_SI == False, we use the \"IS..\" keys from the config dictionary to construct the generator network. ##\n","        else:\n","            self.final_activation = config['IS_gen_final_activ']\n","            self.normalize=config['IS_normalize']\n","            self.scale=config['IS_scale']\n","\n","            neck=config['IS_gen_neck']\n","            exp_kernel=config['IS_exp_kernel']\n","            z_dim=config['IS_gen_z_dim']\n","            hidden_dim=config['IS_gen_hidden_dim']\n","            fill=config['IS_gen_fill']\n","            mult=config['IS_gen_mult']\n","            norm=config['IS_layer_norm']\n","            pad=config['IS_pad_mode']\n","            drop=config['IS_dropout']\n","\n","        ## Abbreviations used for Block Definitions -- used to make code less awkward ##\n","        in_chan = input_channels\n","        out_chan = output_channels\n","\n","        dim_0 = int(hidden_dim*mult**0) # Number of output channels of 1st block/input channels of 2nd block\n","        dim_1 = int(hidden_dim*mult**1) # Number of output channels of 2nd block/input channels of 3rd block\n","        dim_2 = int(hidden_dim*mult**2) # Follows pattern above\n","        dim_3 = int(hidden_dim*mult**3)\n","        dim_4 = int(hidden_dim*mult**4)\n","        dim_5 = int(hidden_dim*mult**5)\n","\n","        ### Block Definitions ###\n","\n","        ## Build the Contracting Path ##\n","        # The formula for the output size of a transposed convolution (nn.Conv2d) in Pytorch is as follows:\n","        # Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1 = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n","\n","        if input_size==180:\n","            self.contract = nn.Sequential(\n","                # nn.Conv2d: Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1 = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n","                # Sinogram Shape: (3,90,90)\n","                contract_block(in_chan, dim_0, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [180+2-3]/2 + 1 = 90\n","                contract_block(dim_0,   dim_1, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [90+2-3]/2 + 1 = 45.5\n","                contract_block(dim_1,   dim_2, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [45+2-3]/2 + 1 = 23\n","                contract_block(dim_2,   dim_2, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [23+2-4]/2 + 1 = 11.5\n","            )\n","        elif input_size==90:\n","            self.contract = nn.Sequential(\n","                contract_block(in_chan, dim_0, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [90+2-3]/2 + 1 = 45.5  : a 90x90 input gives a 45x45 output\n","                contract_block(dim_0,   dim_1, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [45+2-3]/2 + 1 = 23    : a 45x45 input gives a 23x23 output\n","                contract_block(dim_1,   dim_2, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [23+2-4]/2 + 1 = 11.5  : a 23x23 input gives a 11x11 output\n","            )\n","\n","        ## Build the Neck. There are 3 options ##\n","        # neck=1 gives the narrowest (1x1) neck #\n","        if neck==1:\n","            self.neck = nn.Sequential(\n","                contract_block(dim_2, dim_3, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [11+2-4]/2 + 1 = 5.5\n","                contract_block(dim_3, dim_4, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm           ), # H = [5+2*1-3]/2 + 1 = 3\n","                contract_block(dim_4, z_dim, 3, stride=1, padding=0,                   fill=0,    norm='batch'        ), # H = 1   ||norm is set to 'batch' because 'instance' won't work on 1x1 layer\n","                expand_block(  z_dim, dim_4, 3, stride=2, padding=0,                   fill=fill, norm=norm           ), # H = [1-1]*2+5 = 3\n","                expand_block(  dim_4, dim_3, 4, stride=2, padding=2, output_padding=1, fill=fill, norm=norm           ), # H = [3-1]*2+4-2*2+1 = 5\n","            )\n","\n","        # neck=5 gives the middle width (5x5) neck #\n","        if neck==5:\n","            self.neck = nn.Sequential(\n","                contract_block(dim_2, dim_3, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [11+2-4]/2 + 1 = 5.5\n","                contract_block(dim_3, dim_3, 5, stride=1, padding=2, padding_mode=pad,            norm=norm           ), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block)\n","                contract_block(dim_3, dim_3, 5, stride=1, padding=2, padding_mode=pad,            norm=norm           ), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block)\n","                contract_block(dim_3, dim_3, 5, stride=1, padding=2, padding_mode=pad,            norm=norm           ), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block)\n","                #contract_block(dim_3, dim_3, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block) # Add this next tuning!\n","            )\n","\n","        # neck=11 gives the thickest (11x11) neck #\n","        if neck==11:\n","            self.neck = nn.Sequential(\n","                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n","                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n","                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n","                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n","                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n","            )\n","\n","        ## Build the Expanding Blocks ##\n","        # The formula for the output size of a transposed convolution (nn.ConvTranspose2d:) in Pytorch is as follows:\n","        # Hf = (Hi-1)*stride -2*padding +dilation*(kernel-1) +output_padding+1\n","        #    = (Hi-1)*stride +kernel -2*padding +output_padding (for dialation=1)\n","\n","        # For neck=1 or 5, the output from previous layers is 5x5. Therefore, these can use the same expanding blocks #\n","        if (neck==1 or neck==5):\n","            if exp_kernel==3:\n","            # Expanding block for neck=1 or 5, expanding kernel size = 3)\n","                self.expand = nn.Sequential(\n","                    expand_block(dim_3, dim_2,                      kernel_size=3, stride=2, padding=0, output_padding=0, fill=fill, norm=norm), # H = (5-1)*2  +3         = 11\n","                    expand_block(dim_2, dim_1,                      kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm), # H = (11-1)*2 +3 -2*1 +1 = 22\n","                    expand_block(dim_1, dim_0,                      kernel_size=3, stride=2, padding=0, output_padding=0, fill=fill, norm=norm), # H = (22-1)*2 +3         = 45\n","                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm), # H = (45-1)*2 +3 -2*1 +1 = 90\n","                )\n","\n","            elif exp_kernel==4:\n","            # Expanding block for neck=1 or 5, expanding kernel size = 4\n","                self.expand = nn.Sequential(\n","                    expand_block(dim_3, dim_2,                      kernel_size=4, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (5-1)*2  +4 -2*1 +1 = 11\n","                    expand_block(dim_2, dim_1,                      kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (11-1)*2 +4 -2*1    = 22\n","                    expand_block(dim_1, dim_0,                      kernel_size=4, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (21-1)*2 +4 -2*1 +1 = 45\n","                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (45-1)*2 +4 -2*1    = 90\n","                )\n","\n","        # For neck=11, the output is 11x11. This neck requires its own expanding blocks #\n","        if neck==11:\n","            if exp_kernel==3:\n","            # Expanding block for neck=11, expanding kernel size = 3\n","                self.expand = nn.Sequential(\n","                    expand_block(dim_2, dim_1,                      kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (11-1)*2 +3 -2*1 +1 = 22\n","                    expand_block(dim_1, dim_0,                      kernel_size=3, stride=2, padding=0, output_padding=0, fill=fill, norm=norm),  # H = (22-1)*2 +3         = 45\n","                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (45-1)*2 +3 -2*1 +1 = 90\n","                )\n","\n","            if exp_kernel==4:\n","            # Expanding block for neck=11, expanding kernel size = 4\n","                self.expand = nn.Sequential(\n","                    expand_block(dim_2, dim_1,                      kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (11-1)*2 +4 -2*1    = 22\n","                    expand_block(dim_1, dim_0,                      kernel_size=4, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (21-1)*2 +4 -2*1 +1 = 45\n","                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (45-1)*2 +4 -2*1    = 90\n","                )\n","\n","    def forward(self, input):\n","        # This method gets run when the network is called to produce an output from an input #\n","\n","        batch_size = len(input)  # Get batch size\n","\n","        a = self.contract(input) # Run input through contracting blocks\n","        a = self.neck(a)         # Run output from contracting blocks through the neck\n","        a = self.expand(a)       # Run outoput from the neck through the expanding blocks\n","\n","        if self.final_activation:   # Optional final activations\n","            a = self.final_activation(a)\n","        if self.normalize:          # Optionally normalize\n","            a = torch.reshape(a,(batch_size, self.output_channels, 90**2)) # Flattens each image\n","            a = nn.functional.normalize(a, p=1, dim = 2)\n","            a = torch.reshape(a,(batch_size, self.output_channels , 90, 90)) # Reshapes images back into square matrices\n","            a = self.scale*a        # If normalizing, multiply the outputs by a scale factor\n","\n","        return a                    # Return the output"]},{"cell_type":"markdown","metadata":{"id":"vcAixWoDBypD"},"source":["## Discriminators"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCx0M-G0B04j"},"outputs":[],"source":["#################################\n","#### SINOGRAMS DISCRIMINATOR ####\n","#################################\n","\n","class Disc_S_90(nn.Module):\n","    '''\n","    Through experimentation it has been found that sinogram discriminators work best with a fat network neck.\n","    This class takes as input a 90x90.\n","    '''\n","    def __init__(self, config, disc_I=True, input_channels=3):\n","        super(Disc_S_90, self).__init__()\n","\n","        hidden_dim=config['IS_disc_hidden_dim']\n","        patchGAN=config['IS_disc_patchGAN']\n","\n","        ## Sequence 1 ##\n","        self.seq1 = nn.Sequential(\n","            # Sinogram Shape: (in_channels,90,90)\n","            # nn.Conv2d: Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1\n","            #               = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n","\n","            # Feature Map Block\n","            nn.Conv2d(in_channels=sino_channels, out_channels=hidden_dim, kernel_size=7, padding=3, padding_mode='reflect'),\n","\n","            # Contracting Block without normalization:\n","            # H1 = (90-4)/2+1 = 44\n","            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim*2, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n","                nn.LeakyReLU(negative_slope=0.2),\n","\n","            # Contracting Blocks:\n","            # H1 = (44-4)/2+1 = 21\n","            nn.Conv2d(in_channels=hidden_dim*2, out_channels=hidden_dim*3, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n","                nn.InstanceNorm2d(hidden_dim*3), nn.LeakyReLU(negative_slope=0.2),\n","            # H1 = (21-4)/2+1 = 9.5 = 9\n","            nn.Conv2d(in_channels=hidden_dim*3, out_channels=hidden_dim*4, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n","                nn.InstanceNorm2d(hidden_dim*4), nn.LeakyReLU(negative_slope=0.2),\n","            # H1 = (9-4)/2+1 = 3.5 = 3\n","            nn.Conv2d(in_channels=hidden_dim*4, out_channels=hidden_dim*5, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n","                nn.InstanceNorm2d(hidden_dim*5), nn.LeakyReLU(negative_slope=0.2),\n","        )\n","\n","        ## PatchGAN ##\n","        if patchGAN==True:\n","            # H = [3+2*1-3]/1+1 = 3 (3x3x3 matrix)\n","            self.seq2 = nn.Sequential(\n","                nn.Conv2d(hidden_dim*5, hidden_dim*5, kernel_size=3, padding=1, padding_mode='reflect'),\n","                    nn.BatchNorm2d(hidden_dim*5), nn.LeakyReLU(negative_slope=0.2),\n","            )\n","        else:\n","            self.seq2 = nn.Sequential(\n","                # H0 = (3-3)/1+1 = 1 (1x1x3 matrix)\n","                nn.Conv2d(in_channels=hidden_dim*5, out_channels=hidden_dim*5, kernel_size=3),\n","                    nn.BatchNorm2d(hidden_dim*5), nn.LeakyReLU(negative_slope=0.2),\n","            )\n","        ## 1x1 Convolution ##\n","        self.seq3 = nn.Conv2d(hidden_dim * 5, sino_channels, kernel_size=1)\n","\n","    def forward(self, image):\n","        a = self.seq1(image)\n","        b = self.seq2(a) # a tensor\n","        c = self.seq3(b)\n","        #return disc_pred.view(len(disc_pred), -1) # returns a flattened tensor\n","        return c.squeeze()\n","\n","##############################\n","#### IMAGES DISCRIMINATOR ####\n","##############################\n","\n","class Disc_I_90(nn.Module):\n","    def __init__(self, config, disc_I=True, input_channels=3):\n","        super(Disc_I_90, self).__init__()\n","\n","        hidden_dim=config['SI_disc_hidden_dim']\n","        patchGAN=config['SI_disc_patchGAN']\n","\n","        ## Sequence 1 ##\n","        self.seq1 = nn.Sequential(\n","            # Image Shape: (1,90,90)\n","            # nn.Conv2d: Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1\n","            #               = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n","\n","            # H = [90-4]/2+1 = 44\n","            nn.Conv2d(in_channels=input_channels, out_channels=hidden_dim, kernel_size=4, stride=2),\n","                nn.BatchNorm2d(hidden_dim), nn.LeakyReLU(negative_slope=0.2),\n","            # H = [44-4]/2+1 = 21\n","            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim*2, kernel_size=4, stride=2),\n","                nn.BatchNorm2d(hidden_dim*2), nn.LeakyReLU(negative_slope=0.2),\n","            # H = [21-4]/2+1 = 9.5 = 9\n","            nn.Conv2d(in_channels=hidden_dim*2, out_channels=hidden_dim*4, kernel_size=4, stride=2),\n","                nn.BatchNorm2d(hidden_dim*4), nn.LeakyReLU(negative_slope=0.2),\n","            # H = [9+2-4]/2+1 = 4.5 = 4\n","            nn.Conv2d(in_channels=hidden_dim*4, out_channels=hidden_dim*4, kernel_size=4, stride=2, padding=1),\n","                nn.BatchNorm2d(hidden_dim*4), nn.LeakyReLU(negative_slope=0.2),\n","        )\n","\n","        ## Sequence 2 ##\n","        if patchGAN==True:\n","            # H = [4+2-3]/1+1 = 4\n","            self.seq2=nn.Conv2d(hidden_dim*4, 1, kernel_size=3, padding=1, padding_mode='reflect')\n","        else:\n","            # H = [4-4]/2+1 = 1\n","            self.seq2=nn.Conv2d(hidden_dim*4, 1, kernel_size=4, stride=2)\n","\n","    def forward(self, image):\n","\n","        a = self.seq1(image)\n","        disc_pred = self.seq2(a) # a tensor\n","        #return disc_pred.view(len(disc_pred), -1) # returns a flattened tensor\n","        return disc_pred.squeeze()"]},{"cell_type":"markdown","metadata":{"id":"yDdfXS_b90J7"},"source":["# Functions"]},{"cell_type":"markdown","metadata":{"id":"fc9w1e_ACjmb"},"source":["## Cropping & Weight Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IbC9Sgg9RcT"},"outputs":[],"source":["def crop_single_image_by_size(image, crop_size=-1):\n","    '''\n","    Function to crop a single image to a square shape, with even margins around the edges.\n","\n","    image:       Input image tensor of shape [height, width]\n","    crop_size:   Edge size of (square) image to keep. The edges are discarded.\n","    '''\n","    x_size = image.shape[1]\n","\n","    margin_low = int((x_size-crop_size)/2.0)  # (90-71)/2 = 19/2 = 9.5 -->9\n","    margin_high = x_size-crop_size-margin_low # 90-71-9 = 10\n","\n","    pix_min = 0 + margin_low\n","    pix_max = x_size - margin_high\n","\n","    image = image[pix_min : pix_max , pix_min : pix_max]\n","\n","    return image\n","\n","def crop_single_image_by_factor(image, crop_factor=1):\n","    '''\n","    Function to crop a single image for a factor, with even margins around the edges.\n","\n","    image:       Input image tensor of shape [height, width]\n","    crop_factor: Fraction of image to keep. The image is trimmed so the edges are discarded.\n","    '''\n","    x_size = image.shape[1]\n","    y_size = image.shape[0]\n","\n","    x_margin = int(x_size*(1-crop_factor)/2)\n","    y_margin = int(y_size*(1-crop_factor)/2)\n","\n","    x_min = 0 + x_margin\n","    x_max = x_size - x_margin\n","    y_min = 0 + y_margin\n","    y_max = y_size - y_margin\n","\n","    return image_tensor[y_min:y_max , x_min:x_max]\n","\n","\n","def crop_image_tensor_with_corner(batch, crop_size, corner=(0,0)):\n","    '''\n","    Function which returns a smaller, cropped version of a tensor (multiple images)\n","\n","    batch:       a batch of images with dimensions: (num_images, channel, y_dimension, x_dimension)\n","    corner:      upper-left corner of window\n","    crop_size:   size of cropping window (int)\n","    '''\n","\n","    y_min = corner[0]\n","    y_max = corner[0]+crop_size\n","    x_min = corner[1]\n","    x_max = corner[1]+crop_size\n","\n","    return batch[:, :, y_min:y_max , x_min:x_max ]\n","\n","\n","def crop_image_tensor_by_factor(image_tensor, crop_factor=1):\n","    '''\n","    Function to crop an image tensor, with even margins around the edges.\n","\n","    image_tensor:   Input image tensor of shape [image number, channel, height, width]\n","    crop_factor:    Fraction of image to keep. The images are trimmed so the edges are discarded.\n","    '''\n","    x_size = image_tensor.shape[3]\n","    y_size = image_tensor.shape[2]\n","\n","    x_margin = int(x_size*(1-crop_factor)/2)\n","    y_margin = int(y_size*(1-crop_factor)/2)\n","\n","    x_min = 0 + x_margin\n","    x_max = x_size - x_margin\n","    y_min = 0 + y_margin\n","    y_max = y_size - y_margin\n","\n","    return image_tensor[:,:, y_min:y_max , x_min:x_max ]\n","\n","def weights_init(m): # 'm' represents layers in the generator or discriminator.\n","\n","    #Function for initializing network weights to normal distribution, with mean 0 and s.d. 0.02\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.normal_(m.weight, 0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)"]},{"cell_type":"markdown","metadata":{"id":"SjQTwQeHF6BO"},"source":["## Reconstructions & Projection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WD7RY-uF13-","executionInfo":{"status":"ok","timestamp":1747677226773,"user_tz":300,"elapsed":7,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"a39a004e-7e72-4f8c-b82a-7fb58f7b3edc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n### Functions that are no longer used ###\\n\\ndef FBP2(sinogram_tensor, config, image_size = 90, circle=circle):\\n    #This is an alternative filtered back-projection implementation. Not currently used.\\n\\n    normalize = config[\"SI_normalize\"]\\n    scale = config[\\'SI_scale\\']\\n\\n    photopeak_array = sinogram_tensor[:,0,:,:].detach().squeeze().cpu().numpy()\\n    # Note: there\\'s no need to clamp the sinogram as it contains no negative values.\\n\\n    first=True\\n    for sino in photopeak_array[0:,]:\\n        image = iradon(sino,\\n                    circle=circle,\\n                    preserve_range=True,\\n                    filter_name=\\'cosine\\' # Options: \\'ramp\\', \\'shepp-logan\\', \\'cosine\\', \\'hamming\\', \\'hann\\'\\n                    )\\n\\n        ## For Individual Images: create resized/dimensioned Torch tensors ##\\n        image = np.expand_dims(image, axis = 0) # Creates an extra dimension at beginning for images in the batch.\\n        image = torch.from_numpy(image)             # I convert the array to a tensor so I can perform the the resizing and clamping below\\n        image = torch.clamp(image, min=0)           # Clamping\\n        image = (transforms.Resize(size = (image_size, image_size))(image)).numpy() # I convert back to Numpy so I can use the append function later.\\n\\n        ## Normalize each individual image ##\\n        if normalize==True:\\n            image = image/np.sum(image)\\n\\n        if first==True:\\n            image_array = image\\n            first=False\\n        else:\\n            image_array = np.append(image_array, image, axis=0)\\n\\n    image_array = np.expand_dims(image_array, axis=1) # Creates channels dimension\\n\\n    return scale*torch.from_numpy(image_array).to(device)\\n\\n\\ndef shape_smooth(r,R=10000): # NOTE: this function is currently not used\\n\\n    #Returns a smoothly varying function. At r=0, returns 1. As r increases, returned value decreases.\\n    #The function if a portion of a circle.\\n\\n    return (R**2-r**2)**0.5+1-R\\n\\n\\ndef weights_init(m): # \\'m\\' represents layers in the generator or discriminator.\\n\\n    #Function for initializing network weights to normal distribution, with mean 0 and s.d. 0.02\\n\\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\\n        torch.nn.init.uniform_(m.weight, 0, 0.0001)\\n    if isinstance(m, nn.BatchNorm2d):\\n        torch.nn.init.uniform_(m.weight, 0, 0.0001)\\n        torch.nn.init.constant_(m.bias, 0.02)\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["def iradon_MLEM(sino_ground, azi_angles=None, max_iter=15, circle=True, crop_factor=2**0.5/2):\n","    '''\n","    Function to reconstruct a single PET image from a single sinogram using ML-EM.\n","\n","    sino_ground:    sinogram (photopeak). This is a numpy array with minimum values of 0. Shape: (H,W)\n","    azi_angles:     list of azimuthal angles for sinogram. If set to None, angles are assumed to span [0,180)\n","    max_iter:       Maximum number of iterations for ML-EM algorithm.\n","    circle:         circle=True: The projection data spans the width (or height) of the activity distribution, and the reconstructed image is circular.\n","                    circle=False: The projection data (sinograms) spans the corner-to-corner line of the activity distribution, and the reconstructed image is square.\n","    crop_size:      Size to crop the image to after performing ML-EM. For ML-EM performed on a 90x90 sinogram, the\n","                    output image will be 90x90. However, it is necessary to crop this to 64x64 to get the same FOV\n","                    as the dataset. This means the image must cropped by a factor of sqrt(2)/2.\n","    '''\n","    if azi_angles==None:\n","        num_angles = sino_ground.shape[1] # Width\n","        azi_angles=np.linspace(0, 180, num_angles, endpoint=False)\n","\n","    ## Create Sensitivity Image ##\n","    sino_ones = np.ones(sino_ground.shape)\n","    sens_image = iradon(sino_ones, azi_angles, circle=circle, filter_name=None)\n","\n","    if circle==False:\n","        def modify_sens(image, const_factor=0.9, slope=0.03):\n","            '''\n","            Modifies an image so that the area in the central FOV remains constant, but values at edges are attenuated.\n","            image               image to modify\n","            constant_factor     fraction of the image to leave alone\n","            slope               increase this to attenuate images at the edges more\n","            '''\n","            def shape_piecewise(r, const_value, slope):\n","                if r <= const_value:\n","                    return 1\n","                else:\n","                    return 1+slope*(r-const_value)\n","\n","            y_max = image.shape[0]\n","            x_max = image.shape[1]\n","\n","            const_dist = const_factor*x_max/2 # radius over which image remains constant\n","\n","            x_center = (x_max-1)/2.0 # the -1 comes from the fact that the coordinates of a pixel start at 0, not 1\n","            y_center = (y_max-1)/2.0\n","\n","            for y in range(0, y_max):\n","                for x in range(0, x_max):\n","                    r = ((x-x_center)**2 + (y-y_center)**2)**0.5\n","\n","                    total_factor = shape_piecewise(r, const_dist, slope) # creates a circular shaped piece-wise\n","                    #total_factor = shape_piecewise(abs(x-x_center), const_dist, slope) * shape_piecewise(abs(y-y_center), const_dist, slope) # square-shaped piecewise\n","                    #total_factor = shape_piecewise(abs(y-y_center), const_dist, slope) # vertical only\n","\n","                    image[y,x] = image[y,x]*total_factor\n","\n","            return image\n","        sens_image = modify_sens(sens_image)\n","\n","    ## Create blank reconstruction ##\n","    image_recon  = np.ones(sens_image.shape)\n","\n","    for iter in range(max_iter):\n","\n","        if circle==True:\n","            sens_image = sens_image + 0.001 # Guarantees the denominator is >0\n","\n","        sino_recon = radon(image_recon, azi_angles, circle=circle) #\n","        sino_recon[sino_recon==0]=1000 # Set a limit on the denominator (next line)\n","        sino_ratio = sino_ground / (sino_recon) #\n","        image_ratio = iradon(sino_ratio, azi_angles, circle=circle, filter_name=None) / sens_image\n","        image_ratio[image_ratio>1.5]=1.5 # Sets limit on backprojected ratio, on how fast image can grow. Threshold and set value should equal each other (good value=1.5)\n","        image_recon = image_recon * image_ratio\n","        image_recon[image_recon<0]=0 # Sets floor on image pixels. No need to adjust.\n","\n","        #footprint = morphology.disk(1)\n","        #image_recon = opening(image_recon, footprint)\n","\n","    image_cropped = crop_single_image_by_factor(image_recon, crop_factor=crop_factor)\n","    #image_cropped = crop_single_image_by_size(image_recon, crop_size=crop_size)\n","\n","    return image_cropped\n","\n","def reconstruct(sinogram_tensor, config, image_size=90, recon_type='FBP', circle=True):\n","    '''\n","    Function for calculating a reconstructed PET image tensor, given a sinogram_tensor. One image is reconstructed for\n","    each sinogram in the sinogram_tensor.\n","\n","    sinogram_tensor:    Tensor of sinograms of size (number of images)x(channels)x(height)x(width).\n","                        Only the first channel (photopeak) is used for recontruction here.\n","    config:             configuration dictionary\n","    image_size:         size of output (images are resized to this shape)\n","    recon_type:         Can be set to 'MLEM' for maximum-likelihood expectation maximization, or 'FBP' for\n","                        filtered back-projection.\n","    circle              circle=True: The projection data spans the width (or height) of the activity distribution, and the reconstructed image is circular.\n","                        circle=False: The projection data (sinograms) spans the corner-to-corner line of the activity distribution, and the reconstructed image is square.\n","\n","    Function returns a tensor of reconstructed images. Returned images are resized, and optionall normalized and scaled (according to the keys in the configuration dictionary)\n","    '''\n","    normalize = config[\"SI_normalize\"]\n","    scale = config['SI_scale']\n","\n","    photopeak_array = torch.clamp(sinogram_tensor[:,0,:,:], min=0).detach().cpu().numpy()  # Here, we collapse the channel dimension.\n","    # Note: there really should be no need to clamp the sinogram, as it should contain no negative values, but might as well.\n","\n","    ## Reconstruct Individual Sinograms ##\n","    first=True\n","    for sino in photopeak_array[0:,]:\n","        if recon_type == 'FBP':\n","            image = iradon(sino.squeeze(), # Sinogram is now 2D\n","                        circle=False, # For an unknown reason, circle=False gives better reconstructions here. Maybe due to errors introduced in interpolation.\n","                        preserve_range=True,\n","                        filter_name='cosine' # Options: 'ramp', 'shepp-logan', 'cosine', 'hamming', 'hann'\n","                        )\n","        else:\n","            image = iradon_MLEM(sino, circle=circle)\n","\n","        ## Morphologic Opening - removes outlier pixels than can cause problems with image normalization\n","        #footprint = morphology.disk(1)\n","        #image = opening(image, footprint)\n","\n","        ## Concatenate Images ##\n","        image = np.expand_dims(image, axis=0) # Add a dimension to the beginning of the reconstructed image\n","        if first==True:\n","            image_array = image\n","            first=False\n","        else:\n","            image_array = np.append(image_array, image, axis=0)\n","\n","    ## For All Images: create resized/dimensioned Torch tensor ##\n","    image_array = np.expand_dims(image_array, axis=1)        # Creates channels dimension\n","    a = torch.from_numpy(image_array)                        # Converts to Torch tensor\n","    a = torch.clamp(a, min=0)                                # You HAVE to clamp before normalizing or the negative values throw it off.\n","    a = transforms.Resize(size = (image_size, image_size), antialias=True)(a) # Resize tensor\n","\n","    ## Normalize Entire Tensor ##\n","    if normalize:\n","        batch_size = len(a)\n","        a = torch.reshape(a,(batch_size, 1, image_size**2)) # Flattens each image\n","        a = nn.functional.normalize(a, p=1, dim = 2)\n","        a = torch.reshape(a,(batch_size, 1 , image_size, image_size)) # Reshapes images back into square matrices\n","        a = scale*a\n","\n","    return a.to(device)\n","\n","def project(image_tensor, circle=False, theta=-1):\n","    '''\n","    Perform the forward radon transform to calculate projections from images. Returns an array of sinograms.\n","\n","    image_tensor:   tensor of PET images\n","    theta:          numpy array of projection angles. Default is [0,180)\n","    '''\n","    image_collapsed = torch.clamp(image_tensor[:,0,:,:], min=0).detach().squeeze().cpu().numpy()\n","\n","    if theta==-1:\n","        theta = np.arange(0,180)\n","\n","    first=True\n","    for image in image_collapsed[0:,]:\n","        sino = radon(image,\n","                    circle=circle,\n","                    preserve_range=True,\n","                    theta=theta,\n","                    )\n","        sino = np.moveaxis(np.atleast_3d(sino), 2, 0) # Adds a blank axis and moves it to the beginning\n","        if first==True:\n","            sino_array=sino\n","            first=False\n","        else:\n","            sino_array = np.append(sino_array, sino, axis=0)\n","\n","    return torch.from_numpy(sino_array)\n","\n","'''\n","### Functions that are no longer used ###\n","\n","def FBP2(sinogram_tensor, config, image_size = 90, circle=circle):\n","    #This is an alternative filtered back-projection implementation. Not currently used.\n","\n","    normalize = config[\"SI_normalize\"]\n","    scale = config['SI_scale']\n","\n","    photopeak_array = sinogram_tensor[:,0,:,:].detach().squeeze().cpu().numpy()\n","    # Note: there's no need to clamp the sinogram as it contains no negative values.\n","\n","    first=True\n","    for sino in photopeak_array[0:,]:\n","        image = iradon(sino,\n","                    circle=circle,\n","                    preserve_range=True,\n","                    filter_name='cosine' # Options: 'ramp', 'shepp-logan', 'cosine', 'hamming', 'hann'\n","                    )\n","\n","        ## For Individual Images: create resized/dimensioned Torch tensors ##\n","        image = np.expand_dims(image, axis = 0) # Creates an extra dimension at beginning for images in the batch.\n","        image = torch.from_numpy(image)             # I convert the array to a tensor so I can perform the the resizing and clamping below\n","        image = torch.clamp(image, min=0)           # Clamping\n","        image = (transforms.Resize(size = (image_size, image_size))(image)).numpy() # I convert back to Numpy so I can use the append function later.\n","\n","        ## Normalize each individual image ##\n","        if normalize==True:\n","            image = image/np.sum(image)\n","\n","        if first==True:\n","            image_array = image\n","            first=False\n","        else:\n","            image_array = np.append(image_array, image, axis=0)\n","\n","    image_array = np.expand_dims(image_array, axis=1) # Creates channels dimension\n","\n","    return scale*torch.from_numpy(image_array).to(device)\n","\n","\n","def shape_smooth(r,R=10000): # NOTE: this function is currently not used\n","\n","    #Returns a smoothly varying function. At r=0, returns 1. As r increases, returned value decreases.\n","    #The function if a portion of a circle.\n","\n","    return (R**2-r**2)**0.5+1-R\n","\n","\n","def weights_init(m): # 'm' represents layers in the generator or discriminator.\n","\n","    #Function for initializing network weights to normal distribution, with mean 0 and s.d. 0.02\n","\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.uniform_(m.weight, 0, 0.0001)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.uniform_(m.weight, 0, 0.0001)\n","        torch.nn.init.constant_(m.bias, 0.02)\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Doag6oulGKVo"},"source":["## Display Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bV8lRESBGS52"},"outputs":[],"source":["def show_single_unmatched_tensor(image_tensor, grid=False, cmap='inferno', fig_size=1):\n","    '''\n","    Function for visualizing images. The images are displayed, each with their own colormap scaling, so quantitative comparisons are not possible.\n","    Send only the images you want plotted to this function. Works with both single channel and multi-channel images.\n","    If using the single-channel grid option, it plots 120 images in a 15x8 grid.\n","\n","    image_tensor:   image tensor of shape [num, chan, height, width]\n","    grid:           If True, displays images in a 15x8 grid (120 images in total). If false, images are displayed in a horizontal line.\n","    cmap:           Matplotlib color map\n","    fig_size:       figure size\n","    '''\n","    print(f'Shape: {image_tensor.shape} // Min: {torch.min(image_tensor)} // Max: {torch.max(image_tensor)} \\\n","    //Mean Sum (per image): {torch.sum(image_tensor).item()/(image_tensor.shape[0]*image_tensor.shape[1])} // Sum (a single image): {torch.sum(image_tensor[0,0,:])}')\n","\n","    #image_tensor = image_tensor.detach().squeeze(.cpu()\n","    image_tensor = image_tensor.detach().cpu()\n","    image_tensor = torch.clamp(image_tensor, min=0)\n","\n","    num = image_tensor.size(dim=0)\n","    chan = image_tensor.size(dim=1)\n","\n","    ## Plot 3-Channel Images ##\n","    #image_np = image_grid.mean(dim=0).squeeze().numpy() # This also works!\n","\n","    ## Plot Multi-Channel Images ##\n","    if chan!=1:\n","        #123\n","        print(f'Mean (Ch 0): {torch.mean(image_tensor[:,0,:,:])} // Mean (Ch 1): {torch.mean(image_tensor[:,1,:,:])} // Mean (Ch 2): {torch.mean(image_tensor[:,2,:,:])}')\n","\n","        # Plot Grid #\n","        if grid:\n","            fig, ax = plt.subplots(num, chan, figsize=(fig_size*num, fig_size*chan), constrained_layout=True)\n","            for N in range(0, num): # Iterate over image number\n","                for C in range(0, chan): # Iterate over channels\n","                    img = image_tensor[N,C,:,:]\n","                    ax[N,C].axis('off')\n","                    ax[N,C].imshow(img.squeeze(), cmap=cmap)\n","\n","        # Plot in-Line #\n","        else:\n","            fig, ax = plt.subplots(1, num*(chan+1), figsize=(fig_size, fig_size*num*(chan+1)), constrained_layout=True)\n","            i=0\n","            for N in range(0, num): # Iterate over image number\n","                for C in range(0, chan): # Iterate over channels\n","                    img = image_tensor[N,C,:,:]\n","                    ax[i].axis('off')\n","                    ax[i].imshow(img.squeeze(), cmap=cmap)\n","                    i+=1\n","                blank = torch.ones_like(img)\n","                ax[i].axis('off')\n","                ax[i].imshow(blank.squeeze())\n","                i+=1\n","\n","    ## Plot 1 Channel Images ##\n","    else:\n","        # Plot Grid #\n","        # Note: This plots 120 images at a time!\n","        if grid:\n","            cols, rows = 15, 8\n","        # Plot in-Line #\n","        else:\n","            rows = 1\n","            cols = image_tensor.shape[0]\n","\n","        figure=plt.figure(figsize=(cols*fig_size,rows*fig_size))\n","\n","        for i in range(0, cols*rows):\n","            img = image_tensor[i]             # Shape: torch.Size([3, 1, 180, 180]) /\n","            figure.add_subplot(rows,cols,i+1) # MatplotLib indeces start at 1\n","            plt.axis(\"off\")\n","            plt.imshow(img.squeeze(), cmap=cmap)\n","\n","    plt.show()\n","\n","\n","def show_multiple_matched_tensors(*image_tensors, cmap='inferno', fig_size=0.8):\n","    '''\n","    Function for visualizing images from multiple tensors. Each image is \"matched\" with images from the other tensors,\n","    and each matched set of images (one from each tensor) is plotted with the same colormap in a column.\n","    Send only the images you want plotted to this function. Works with both single channel and multi-channel images.\n","\n","    image_tensors:  list of tensors, each of which may contain multiple images.\n","    '''\n","    for tensor in image_tensors:\n","        # Begin by printing statistics for each tensor\n","        print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n","        // Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n","\n","    combined_tensor = torch.cat(image_tensors, dim=0).detach().cpu()\n","    combined_tensor = torch.clamp(combined_tensor, min=0)\n","\n","    num_rows = len(image_tensors)           # The number of rows equals the number of tensors (images to match)\n","    num_cols = len(image_tensors[0])        # The length of the zeroth element (of the list) is the number of images in a tensor.\n","    num_chan = image_tensors[0].size(dim=1) # Equivalent to: image_tensors[0].shape(1)\n","\n","    ## Plot 1 Channel Images ##\n","    if num_chan==1:\n","        fig, ax = plt.subplots(num_rows, num_cols, squeeze=False, figsize=(fig_size*num_cols, fig_size*num_rows), constrained_layout=True)\n","        #fig, ax = plt.subplots(num_rows, num_cols, constrained_layout=True)\n","\n","        i=0 # i = column number\n","        for col in range(0, num_cols): # Iterate over column number. All images in a column will have the same colormap.\n","            img_list=[]\n","            min_list=[]\n","            max_list=[]\n","\n","            # Construct image list and normalization object for matched images in a column (iterating over rows) #\n","            for row in range(0,num_rows):                               # We iterate over rows in orcer\n","                img = combined_tensor[row*num_cols+col, 0 ,:,:]         # Grab the correct image (zeroth channel for 1-D images)\n","                img_list.append(img)                                    # We construct a new image list for each row\n","                min_list.append(torch.min(img).item())                  # Create list of image minimums\n","                max_list.append(torch.max(img).item())                  # Create list of image maximums\n","            norm = Normalize(vmin=min(min_list), vmax=max(max_list))    # We construct a normalization object with min/max = min/max pixel value for all images in list\n","\n","            # Plot normalized images in a single column (iterating over rows) #\n","            for row in range(0,num_rows):\n","                ax[row, i].axis('off')\n","                ax[row, i].imshow(img_list[row].squeeze(), cmap=cmap, norm=norm) # Squeeze gets rid of extra channel dimension\n","            i+=1\n","\n","    ## Plot Multi-Channel Images ##\n","    else:\n","        print(f'Mean (Ch 0): {torch.mean(combined_tensor[:,0,:,:])} // Mean (Ch 1): {torch.mean(combined_tensor[:,1,:,:])} // Mean (Ch 2): {torch.mean(combined_tensor[:,2,:,:])}')\n","\n","        #if num_cols>3:  # Restricts to 3-channels. You could get rid of this without an issue.\n","        #    num_cols=3\n","\n","        # Construct figure and axes. Note: 'num_chan+1' arises from the divider blank image btw. each multi-channel image\n","        fig, ax = plt.subplots(num_rows, num_cols*(num_chan+1), squeeze=False, figsize=(fig_size*num_cols*(num_chan+1), fig_size*num_rows), constrained_layout=True)\n","\n","        i=0\n","        for col in range(0, num_cols):      # Iterate over column number\n","            for chan in range(0, num_chan): # Iterate over channels\n","                img_list=[]\n","                min_list=[]\n","                max_list=[]\n","\n","                # Iterates over rows (one row per tensor) to construct an image list and normalization object a single column. All matched images have the same channel. #\n","                for row in range(0,num_rows):\n","                    img = combined_tensor[row*num_cols+col, chan ,:,:] # Constructs an image list where each row has the same channel #\n","                    img_list.append(img)\n","                    min_list.append(torch.min(img).item())\n","                    max_list.append(torch.max(img).item())\n","                norm = Normalize(vmin=min(min_list), vmax=max(max_list))\n","\n","                # Iterates over rows to plot matched images in a single column. These share the same channel. #\n","                for row in range(0,num_rows):\n","                    ax[row, i].axis('off')\n","                    ax[row, i].imshow(img_list[row].squeeze(), cmap=cmap, norm=norm) # Squeeze gets rid of extra channel dimension\n","                i+=1\n","\n","            # After all channels have been iterated, the complete multi-channel image has been plotted. Now we plot a divider before the next image #\n","            for row in range(0,num_rows):\n","                blank = torch.ones_like(img)\n","                ax[row, i].axis('off')\n","                ax[row, i].imshow(blank.squeeze())\n","            i+=1\n","\n","    plt.show()\n","\n","def show_single_commonmap_tensor(image_tensor, nrow=15, figsize=(27,18), cmap='inferno'):\n","    '''\n","    Function for visualizing images from one tensor, all of which will be plotted with the same scaled colormap. Only works with single-channel image tensors.\n","\n","    image_tensor:  image tensor. nrow should go into this evenly.\n","    nrow:          number of rows for the image grid\n","    figsize:       figure size\n","    cmap:          color map\n","    '''\n","    tensor = torch.clamp(image_tensor, min=0).detach().cpu()\n","    image_grid = make_grid(tensor, nrow=nrow)  # from torchvision.utils import make_grid\n","\n","    #print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n","    #// Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n","\n","    fig, ax = plt.subplots(1,1, figsize=figsize)\n","    ax.axis('off')\n","\n","    image_grid = image_grid[0,:].squeeze()\n","    #plt.imshow(image_grid, cmap=cmap)\n","    im = ax.imshow(image_grid, cmap=cmap)\n","    #fig.colorbar(im, ax=ax)\n","    plt.show()\n","\n","def show_multiple_commonmap_tensors(*image_tensors, cmap='inferno'):\n","    '''\n","    Function for visualizing images from multiple tensors, all of which will be plotted with the same scaled colormap. Only works with single-channel image tensors.\n","\n","    *image_tensors: list of image tensors, all of which should contain the same number of images. Only send the number of images you want to plot to this function.\n","    '''\n","    # Print tensor statistics #\n","    for tensor in image_tensors:\n","        print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n","        // Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n","\n","    num_rows = len(image_tensors)\n","    num_columns = len(image_tensors[0])\n","    # Combine tensors into one & clamp #\n","    combined_tensor = torch.cat(image_tensors, dim=0).detach().cpu()\n","    combined_tensor = torch.clamp(combined_tensor, min=0)\n","    # Make a grid of the tensors #\n","    image_grid = make_grid(combined_tensor, nrow=num_columns) # Note: nrow is the number of images displayed in each row (i.e., the number of columns)\n","\n","    # Determine figure size #\n","    print('num_rows:', num_rows)\n","    fig, ax = plt.subplots(1,1, figsize=(30,1*num_rows))\n","    #fig, ax = plt.subplots(1,1, figsize=(30,7))\n","\n","    ax.axis('off')\n","\n","    image_grid = image_grid[0,:].squeeze()\n","    im = ax.imshow(image_grid, cmap=cmap)\n","    fig.colorbar(im, ax=ax)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ajsf7Iok0X2I"},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NzzJJ4-70XE5"},"outputs":[],"source":["##################################################\n","## Functions for Calculating Metrics Dataframes ##\n","##################################################\n","\n","## Calculate Arbitrary Metric ##\n","def calculate_metric(batch_A, batch_B, img_metric_function, return_dataframe=False, label='default', crop_factor=1):\n","    '''\n","    Function which calculates metric values for two batches of images.\n","    Returns either the average metric value for the batch or a dataframe of individual image metric values.\n","\n","    batch_A:                tensor of images to compare [num, chan, height, width]\n","    batch_B:                tensor of images to compare [num, chan, height, width]\n","    img_metric_function:    a function which calculates a metric (MSE, SSIM, etc.) from two INDIVIDUAL images\n","    return_dataframe:       If False, then the average is returned.\n","                            Otherwise both the average, and a dataframe containing the metric values of the images in the batches, are returned.\n","    label:                  what to call dataframe, if it is created\n","    crop_factor:            factor by which to crop both batches of images. 1 = whole image is retained.\n","    '''\n","\n","    if crop_factor != 1:\n","        A = crop_image_tensor_by_factor(batch_A, crop_factor=crop_factor)\n","        B = crop_image_tensor_by_factor(batch_B, crop_factor=crop_factor)\n","\n","    length = len(batch_A)\n","    metric_avg = 0\n","    metric_list = []\n","\n","    for i in range(length):\n","        image_A = batch_A[i:i+1,:,:,:] # Using i:i+1 instead of just i preserves the dimensionality of the array\n","        image_B = batch_B[i:i+1,:,:,:]\n","\n","        metric_value = img_metric_function(image_A, image_B)\n","        metric_avg += metric_value/length\n","        if return_dataframe==True:\n","            metric_list.append(metric_value)\n","\n","    metric_frame = pd.DataFrame({label : metric_list})\n","\n","    if return_dataframe==False:\n","        return metric_avg\n","    else:\n","        return metric_frame, metric_avg\n","\n","\n","def update_tune_dataframe(tune_dataframe, model, config, mean_CNN_MSE, mean_CNN_SSIM, mean_CNN_CUSTOM):\n","    '''\n","    Function to update the tune_dataframe for each trial run that makes it partway through the tuning process.\n","\n","    tune_dataframe      a dataframe that stores model and IQA metric information for a particular trial\n","    model               model being trained (in tuning)\n","    config              configuration dictionary\n","    mean_CNN_MSE        mean MSE for the CNN\n","    mean_CNN_SSIM       mean SSIM for the CNN\n","    mean_CNN_CUSTOM     mean custom metric for the CNN\n","\n","    '''\n","    # Extract values from config dictionary\n","    SI_dropout =        config['SI_dropout']\n","    SI_exp_kernel =     config['SI_exp_kernel']\n","    SI_gen_fill =       config['SI_gen_fill']\n","    SI_gen_hidden_dim = config['SI_gen_hidden_dim']\n","    SI_gen_neck =       config['SI_gen_neck']\n","    SI_layer_norm =     config['SI_layer_norm']\n","    SI_normalize =      config['SI_normalize']\n","    SI_pad_mode =       config['SI_pad_mode']\n","    batch_size =        config['batch_size']\n","    gen_lr =            config['gen_lr']\n","\n","    # Calculate number of trainable weights in CNN\n","    num_params = sum(map(torch.numel, model.parameters()))\n","\n","    # Concatenate Dataframe\n","    add_frame = pd.DataFrame({'SI_dropout': SI_dropout, 'SI_exp_kernel': SI_exp_kernel, 'SI_gen_fill': SI_gen_fill, 'SI_gen_hidden_dim': SI_gen_hidden_dim,\n","                            'SI_gen_neck': SI_gen_neck, 'SI_layer_norm': SI_layer_norm, 'SI_normalize': SI_normalize, 'SI_pad_mode': SI_pad_mode, 'batch_size': batch_size,\n","                            'gen_lr': gen_lr, 'num_params': num_params, 'mean_CNN_MSE': mean_CNN_MSE, 'mean_CNN_SSIM': mean_CNN_SSIM, 'mean_CNN_CUSTOM': mean_CNN_CUSTOM}, index=[0])\n","\n","    tune_dataframe = pd.concat([tune_dataframe, add_frame], axis=0)\n","\n","    # Save Dataframe to File\n","    tune_dataframe.to_csv(tune_dataframe_path, index=False)\n","\n","    return tune_dataframe\n","\n","\n","def reconstruct_images_and_update_test_dataframe(sino_tensor, image_size, CNN_output, ground_image, test_dataframe, config):\n","    '''\n","    Function which: A) performs reconstructions (FBP and possibly ML-EM)\n","                    B) constructs a dataframe of metric values (MSE & SSIM) for these reconstructions, and also for the CNN output, with respect to the ground truth image.\n","                    C) concatenates this with the test dataframe passed to this function\n","                    D) returns the concatenated dataframe, mean metric values, and reconstructions\n","\n","    sino_tensor:    sinogram tensor of shape [num, chan, height, width]\n","    image_size:     image_size\n","    CNN_output:     CNN reconstructions\n","    ground_image:   ground truth images\n","    test_dataframe: dataframe to append metric values to\n","    config:         general config dictionary\n","    '''\n","\n","    # Construct Outputs #\n","    FBP_output = reconstruct(sino_tensor, config, image_size=image_size, recon_type='FBP')\n","    if compute_MLEM==True:\n","        MLEM_output = reconstruct(sino_tensor, config, image_size=image_size, recon_type='MLEM')\n","    else: # If not looking at ML-EM, don't waste time computing the MLEM images, which can take awhile.\n","        MLEM_output = FBP_output\n","\n","    # Dataframes: build dataframes for every reconstruction technique/metric combination #\n","    batch_CNN_MSE,  mean_CNN_MSE   = calculate_metric(ground_image, CNN_output, MSE,  return_dataframe=True, label='MSE (Network)')\n","    batch_CNN_SSIM,  mean_CNN_SSIM = calculate_metric(ground_image, CNN_output, SSIM, return_dataframe=True, label='SSIM (Network)')\n","    batch_FBP_MSE,  mean_FBP_MSE   = calculate_metric(ground_image, FBP_output, MSE,  return_dataframe=True, label='MSE (FBP)')\n","    batch_FBP_SSIM,  mean_FBP_SSIM = calculate_metric(ground_image, FBP_output, SSIM, return_dataframe=True, label='SSIM (FBP)')\n","    batch_MLEM_MSE, mean_MLEM_MSE  = calculate_metric(ground_image, MLEM_output, MSE, return_dataframe=True, label='MSE (ML-EM)')\n","    batch_MLEM_SSIM, mean_MLEM_SSIM= calculate_metric(ground_image, MLEM_output, SSIM,return_dataframe=True, label='SSIM (ML-EM)')\n","\n","    # Concatenate batch dataframes and larger running test dataframe\n","    add_frame = pd.concat([batch_CNN_MSE, batch_FBP_MSE, batch_MLEM_MSE, batch_CNN_SSIM, batch_FBP_SSIM, batch_MLEM_SSIM], axis=1)\n","    test_dataframe = pd.concat([test_dataframe, add_frame], axis=0)\n","\n","    # Return a whole lot of stuff\n","    return test_dataframe, mean_CNN_MSE, mean_CNN_SSIM, mean_FBP_MSE, mean_FBP_SSIM, mean_MLEM_MSE, mean_MLEM_SSIM, FBP_output, MLEM_output\n","\n","######################\n","## Metric Functions ##\n","######################\n","\n","## Metrics which take only single images as inputs ##\n","## ----------------------------------------------- ##\n","def SSIM(image_A, image_B, win_size=-1):\n","    '''\n","    Function to return the SSIM for two 2D images.\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    win_size:       window size to use when computing the SSIM. This must be an odd number. If =-1, the full size of the image is used (or full size-1 so it's odd).\n","    '''\n","\n","    if win_size == -1:   # The default shape of the window size is the same size as the image.\n","        x = image_A.shape[2]\n","        win_size = (x if x % 2 == 1 else x-1) # Guarantees the window size is odd.\n","\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    max_value = max([np.amax(image_A_npy, axis=(0,1)), np.amax(image_B_npy, axis=(0,1))])   # Find maximum among the images\n","    min_value = min([np.amin(image_A_npy, axis=(0,1)), np.amin(image_B_npy, axis=(0,1))])   # Find minimum among the images\n","    data_range = max_value-min_value\n","\n","    SSIM_image = structural_similarity(image_A_npy, image_B_npy, data_range=data_range, gaussian_weights=False, use_sample_covariance=False, win_size=win_size)\n","\n","    return SSIM_image\n","\n","## Metrics which take either batches or images as inputs ##\n","## ----------------------------------------------------- ##\n","def MSE(image_A, image_B):\n","    '''\n","    Function to return the mean square error for two 2D images (or two batches of images).\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    return torch.mean((image_A-image_B)**2).item()\n","\n","def NMSE(image_A, image_B):\n","    '''\n","    Function to return the normalized mean square error for two 2D images (or two batches of images).\n","\n","    image_A:        pytorch tensor for a single image (reference image)\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    return (torch.mean((image_A-image_B)**2)/torch.mean(image_A**2)).item()\n","\n","def MAE(image_A, image_B):\n","    '''\n","    Function to return the mean absolute error for two 2D images (or two batches of images).\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    return torch.mean(torch.abs(image_A-image_B)).item()\n","\n","def calculate_moments(batch_A, batch_B, window_size = 10, stride=10, dataframe=False):\n","    '''\n","    Function to return the three statistical moment scores for two image tensors.\n","    '''\n","    ## Nested Functions ##\n","\n","    def compare_moments(win_A, win_B, moment):\n","        def compute_moment(win, moment, axis=1):\n","            mean_value = np.mean(win, axis=axis)\n","            if moment == 1:\n","                return mean_value\n","            else:\n","                mean_array = np.array([mean_value] * win.shape[1]).T  # The square brackets in win.shape[1] mean the value is repeated spatially\n","                moment = np.mean((win - mean_array)**moment, axis=1)\n","                return moment\n","\n","        batch_size = win_A.shape[0]\n","\n","\n","        reshape_A = (torch.reshape(win_A, (batch_size, -1))).detach().cpu().numpy()\n","        reshape_B = (torch.reshape(win_B, (batch_size, -1))).detach().cpu().numpy()\n","\n","        moment_A = compute_moment(reshape_A, moment=moment)\n","        moment_B = compute_moment(reshape_B, moment=moment)\n","        moment_score = np.mean(np.absolute(moment_A-moment_B)/(np.absolute(moment_A)+0.1))\n","\n","        '''\n","        print('===============================')\n","        print('MOMENT: ', moment)\n","        print('moment_A shape: ', moment_A.shape)\n","        print('moment_A mean: ', np.mean(moment_A))\n","        print('moment_B shape: ', moment_B.shape)\n","        print('moment_B mean: ', np.mean(moment_B))\n","        print('moment_score, |moment_A-moment_B|/(moment_A+0.1) : ', moment_score)\n","        '''\n","        return moment_score\n","\n","    ## Code ##\n","    image_size = batch_A.shape[2]\n","\n","    num_windows = int((image_size)/stride) # Maximum number of windows occurs when: stride = window_size.\n","    while (num_windows-1)*stride + window_size > image_size: # Solve for the number of windows (crops)\n","        num_windows += -1\n","\n","    moment_1_running_score = 0\n","    moment_2_running_score = 0\n","    moment_3_running_score = 0\n","\n","    for i in range(0, num_windows):\n","        for j in range(0, num_windows):\n","            corner = (i*stride, j*stride)\n","\n","            win_A = crop_image_tensor_with_corner(batch_A, window_size, corner)\n","            win_B = crop_image_tensor_with_corner(batch_B, window_size, corner)\n","\n","            moment_1_score = compare_moments(win_A, win_B, moment=1)\n","            moment_2_score = compare_moments(win_A, win_B, moment=2)\n","            moment_3_score = compare_moments(win_A, win_B, moment=3)\n","\n","            moment_1_running_score += moment_1_score\n","            moment_2_running_score += moment_2_score\n","            moment_3_running_score += moment_3_score\n","\n","    return moment_1_running_score, moment_2_running_score, moment_3_running_score\n","\n","def LDM(batch_A, batch_B):\n","    '''\n","    Calculate the local distributions metric (LDM) for two batches of images\n","    '''\n","\n","    score_1, score_2, score_3 = calculate_moments(batch_A, batch_B, window_size=5, stride=5)\n","\n","    score_1 = score_1*1\n","    score_2 = score_2*1\n","    score_3 = score_3*1\n","\n","    '''\n","    print('Scores')\n","    print('====================')\n","    print(score_1)\n","    print(score_2)\n","    print(score_3)\n","    '''\n","\n","    return score_1+score_2+score_3\n","\n","def custom_metric(batch_A, batch_B):\n","    return 0\n","    #return MSE(batch_A, batch_B)\n","\n","\n","\n","###############################################\n","## Average or a Batch Metrics: Good for GANs ##\n","###############################################\n","\n","# Range #\n","def range_metric(real, fake):\n","    '''\n","    Computes a simple metric which penalizes \"fake\" images in a batch for having a range different than the \"real\" images in a batch.\n","    Only a single metric number is returned.\n","    '''\n","    range_real = torch.max(real).item()-torch.min(real).item()\n","    range_fake = torch.max(fake).item()-torch.min(fake).item()\n","\n","    return abs(range_real-range_fake)/(range_real+.1)\n","\n","# Average #\n","def avg_metric(real, fake):\n","    '''\n","    Computes a simple metric which penalizes \"fake\" images in a batch for having an average value different than the \"real\" images in a batch.\n","    Only a single metric number is returned.\n","    '''\n","    avg_metric = abs((torch.mean(real).item()-torch.mean(fake).item())/(torch.mean(real)+.1).item())\n","    return avg_metric\n","\n","# Pixel Variation #\n","def pixel_dist_metric(real, fake):\n","    '''\n","    Computes a metric which penalizes \"fake\" images for having a pixel distance different than the \"real\" images.\n","\n","    real: real image tensor\n","    fake: fake image tensor\n","    '''\n","    def pixel_dist(image_tensor):\n","        '''\n","        Function for computing the pixel distance (standard deviation from mean) for a batch of images.\n","        For simplicity, it only looks at the 0th channel.\n","        '''\n","        array = image_tensor[:,0,:,:].detach().cpu().numpy().squeeze()\n","        sd = np.std(array, axis=0)\n","        avg=np.mean(sd)\n","        return(avg)\n","\n","    pix_dist_fake = pixel_dist(fake)\n","    pix_dist_real = pixel_dist(real)\n","\n","    return abs((pix_dist_real-pix_dist_fake)/(pix_dist_real+.1)) # The +0.1 in the denominators guarantees we don't divide by zero\n","\n","###################\n","## Old Functions ##\n","###################\n","\n","\n","def LDM_OLD(real, fake, crop_size = 10, stride=10):\n","    '''\n","    Function to return the local distributions metric for two images.\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_size = real.shape[2]\n","\n","    i_max = int((image_size)/stride) # Maximum number of windows occurs when the stride equals the crop_size\n","    while (i_max-1)*stride + crop_size > image_size: # If stride < crop_size, we need fewer need to solve for the number of crops\n","        i_max += -1\n","\n","    def crop_image_tensor_with_corner(A, corner=(0,0), crop_size=1):\n","        '''\n","        Function which returns a small, cropped version of an image.\n","\n","        A           a batch of images with dimensions: (num_images, channel, height, width)\n","        corner      upper-left corner of window\n","        crop_size   size of croppiong window\n","        '''\n","        x_min = corner[1]\n","        x_max = corner[1]+crop_size\n","        y_min = corner[0]\n","        y_max = corner[0]+crop_size\n","        return A[:,:, y_min:y_max , x_min:x_max ]\n","\n","    running_dist_score = 0\n","    running_avg_score = 0\n","\n","    for i in range(0, i_max):\n","        for j in range(0, j_max):\n","            corner = (i*crop_size, j*crop_size)\n","            win_real = crop_image_tensor_with_corner(real, corner, crop_size)\n","            win_fake = crop_image_tensor_with_corner(fake, corner, crop_size)\n","\n","            #range_score = range_metric(win_real, win_fake)\n","            avg_score = avg_metric(win_real, win_fake)\n","            pixel_dist_score = pixel_dist_metric(win_real, win_fake)\n","\n","            running_dist_score += pixel_dist_score\n","            running_avg_score += avg_score\n","\n","    combined_score = running_dist_score + running_avg_score\n","\n","    return combined_score\n"]},{"cell_type":"markdown","metadata":{"id":"iN7zClTzVvMp"},"source":["## Loss Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCFRLWqJVuY2","executionInfo":{"status":"ok","timestamp":1747677226893,"user_tz":300,"elapsed":7,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"30c693a7-5d54-4f7b-f04a-844c8bf2ca65"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef get_gen_adv_loss(fake_X, disc_X, adv_criterion):\\n    print('Calc generative adversarial loss')\\n    disc_fake_pred = disc_X(fake_X)\\n    adversarial_loss = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) # Called only from get_gen_loss\\n    return adversarial_loss\\n\\ndef get_sup_loss(fake_X, real_X, sup_criterion):\\n    print('Calc supervisory loss')\\n    sup_loss = sup_criterion(fake_X, real_X)\\n    return sup_loss\\n\\ndef get_cycle_loss(fake_I, gen_IS, low_rez_S, cycle_criterion):\\n    print('Calc cycle loss')\\n    cycle_S = gen_IS(fake_I)\\n    cycle_loss = cycle_criterion(cycle_S, low_rez_S)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["def get_supervisory_loss(fake_X, real_X, sup_criterion):\n","    '''\n","    Function to calculate the supervisory loss.\n","\n","    fake_X:         fake image tensor (Terminology from GANs. For supervisory networks, it's arbitrary whether fake_X or real_X are ground truths or reconstructions)\n","    real_X:         real image tensor\n","    sup_criterion   loss function. Will be a Pytorch object.\n","    '''\n","    #print('Calc supervisory loss')\n","    sup_loss = sup_criterion(fake_X, real_X)\n","    return sup_loss\n","\n","def get_disc_loss(fake_X, real_X, disc_X, adv_criterion):\n","    '''\n","    Function to calculate the discriminator loss. Used to train the discriminator.\n","    '''\n","    disc_fake_pred = disc_X(fake_X.detach()) # Detach generator from fake batch\n","    disc_fake_loss = adv_criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred)) # Good fakes shoudl yield predictions = 0.\n","    disc_real_pred = disc_X(real_X)\n","    disc_real_loss = adv_criterion(disc_real_pred, torch.ones_like(disc_real_pred)) # Good fakes shoudl yield predictions = 1.\n","    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n","    return disc_loss\n","\n","def get_gen_adversarial_loss(real_X, gen_XY, disc_Y, adv_criterion):\n","    '''\n","    Function to calculate the adversarial loss (for gen_XY) and fake_Y (from real_X).\n","    '''\n","    fake_Y = gen_XY(real_X)\n","    disc_fake_pred = disc_Y(fake_Y)\n","    adversarial_loss = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) # generator is penalized for discriminmator getting it right\n","    return adversarial_loss, fake_Y\n","\n","def get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion):\n","    '''\n","    Function to calculate the cycle-consistency loss (for gen_YX).\n","    '''\n","    cycle_X = gen_YX(fake_Y)\n","    cycle_loss = cycle_criterion(cycle_X, real_X)\n","    return cycle_loss, cycle_X\n","\n","def get_gen_loss(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, config):\n","    '''\n","    Function to calculate the total generator loss. Used to train the generators.\n","    '''\n","    supervisory_criterion = config['sup_criterion']\n","    cycle_criterion = config['cycle_criterion']\n","    gen_adversarial_criterion = config['gen_adv_criterion']\n","    lambda_adv = config['lambda_adv']\n","    lambda_sup = config['lambda_sup']\n","    lambda_cycle = config['lambda_cycle']\n","\n","    # Adversarial Loss\n","    if lambda_adv != 0: # To save resources, we only run this code if lambda_adv != 0\n","        adv_loss_AB, fake_B = get_gen_adversarial_loss(real_A, gen_AB, disc_B, gen_adversarial_criterion)\n","        adv_loss_BA, fake_A = get_gen_adversarial_loss(real_B, gen_BA, disc_A, gen_adversarial_criterion)\n","        adv_loss = adv_loss_AB+adv_loss_BA\n","    else: # Even if we don't compute adversarial losses, we still need fake_A and fake_B for later code\n","        fake_A = gen_BA(real_B)\n","        fake_B = gen_AB(real_A)\n","\n","    # Supervisory Loss\n","    if lambda_sup != 0: # To save resources, we only run this code if lambda_sup != 0\n","        sup_loss_AB = get_supervisory_loss(fake_B, real_B, supervisory_criterion)\n","        sup_loss_BA = get_supervisory_loss(fake_A, real_A, supervisory_criterion)\n","        sup_loss = sup_loss_AB+sup_loss_BA\n","\n","    # Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)\n","    cycle_loss_AB, cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)\n","    cycle_loss_BA, cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)\n","    cycle_loss = cycle_loss_AB+cycle_loss_BA\n","\n","    # Total Generator Loss\n","    if lambda_sup == 0:\n","        gen_loss = lambda_adv*adv_loss+lambda_cycle*cycle_loss\n","        return gen_loss, adv_loss.item(), 0, cycle_loss.item(), cycle_A, cycle_B\n","    elif lambda_adv == 0:\n","        gen_loss = lambda_sup*sup_loss+lambda_cycle*cycle_loss\n","        return gen_loss, 0, sup_loss.item(), cycle_loss.item(), cycle_A, cycle_B\n","    else:\n","        gen_loss = lambda_adv*adv_loss+lambda_sup*sup_loss+lambda_cycle*cycle_loss\n","        return gen_loss, adv_loss.item(), sup_loss.item(), cycle_loss.item(), cycle_A, cycle_B\n","\n","### Functons for Assymmetric/Separate (Older) ###\n","'''\n","def get_gen_adv_loss(fake_X, disc_X, adv_criterion):\n","    print('Calc generative adversarial loss')\n","    disc_fake_pred = disc_X(fake_X)\n","    adversarial_loss = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) # Called only from get_gen_loss\n","    return adversarial_loss\n","\n","def get_sup_loss(fake_X, real_X, sup_criterion):\n","    print('Calc supervisory loss')\n","    sup_loss = sup_criterion(fake_X, real_X)\n","    return sup_loss\n","\n","def get_cycle_loss(fake_I, gen_IS, low_rez_S, cycle_criterion):\n","    print('Calc cycle loss')\n","    cycle_S = gen_IS(fake_I)\n","    cycle_loss = cycle_criterion(cycle_S, low_rez_S)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"FIxCEyDO3Xdy"},"source":["## Timing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0avff6f3WFF"},"outputs":[],"source":["def display_times(label_string, init_time, show_times):\n","    '''\n","    Function to display the time it takes to perform individual steps in the code. This can be helpful when trying to streamline things. Also returns the current time, which can be used to reset the timer in the code that calls the function.\n","\n","    init_time:      initiation time when the process started\n","    label_string:   string to label the displayed time\n","    show_times:     show times or not\n","    '''\n","    current_time = time.time()\n","\n","    if show_times == True:\n","        print(f'{label_string} (ms): {(current_time-init_time)*1000}')\n","\n","    return current_time"]},{"cell_type":"markdown","metadata":{"id":"CxBvKZ2-80VH"},"source":["# Tune/Train/Test Functions"]},{"cell_type":"markdown","metadata":{"id":"3iLmrMc7b20J"},"source":["## SUP Loss Only"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4kG5oJcby7G"},"outputs":[],"source":["def train_Supervisory_Sym(config, offset=0, num_examples=-1, sample_division=1):\n","    '''\n","    Function to train or test a network with supervisory loss only. Also used for visualizing data in the dataset.\n","    '''\n","    print('Dataset offset:', offset)\n","    print('Dataset num_examples:', num_examples)\n","    print('Dataset sample_division:', sample_division)\n","\n","    ############################\n","    ### Initialize Variables ###\n","    ############################\n","\n","    ## Grab some values and assign to local variables ##\n","    sup_criterion=config['sup_criterion']\n","    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n","\n","    ## If Tuning ##\n","    if run_mode=='tune':\n","        batch_size=config['batch_size']   #config['batch_size']=tune.choice([32, 64, 128, 256, 512, 1024])\n","        batch_mult = 512/batch_size if tune_even_reporting == True else 1\n","        display_step = tune_iter_per_report*batch_mult # Larger batch size --> fewer training iterations per report to RayTune\n","\n","        if tune_restore==False:\n","            tune_dataframe = pd.DataFrame({'SI_dropout': [], 'SI_exp_kernel': [], 'SI_gen_fill': [], 'SI_gen_hidden_dim': [], 'SI_gen_neck': [], 'SI_layer_norm': [], 'SI_normalize': [],'SI_pad_mode': [], 'batch_size': [], 'gen_lr': [], 'num_params': [], 'mean_CNN_MSE': [], 'mean_CNN_SSIM': [], 'mean_CNN_CUSTOM': []})\n","            tune_dataframe.to_csv(tune_dataframe_path, index=False)\n","        else:\n","            tune_dataframe = pd.read_csv(tune_dataframe_path)\n","\n","    ## If Training ##\n","    elif run_mode=='train':\n","        batch_size=config['batch_size']\n","        display_step = train_display_step\n","\n","    ## If Testing ##\n","    elif run_mode=='test':\n","        batch_size = config['batch_size'] = test_batch_size  # If we don't override the batch size in the config dictionary, the same batch size will be used as was used to train the network. Therefore, we override it.\n","        display_step = test_display_step\n","        test_dataframe = pd.DataFrame({'MSE (Network)' : [],  'MSE (FBP)': [],  'MSE (ML-EM)': [],'SSIM (Network)' : [], 'SSIM (FBP)': [], 'SSIM (ML-EM)': []})\n","\n","    ## If Visualizeing ##\n","    elif run_mode=='visualize':\n","        batch_size = config['batch_size'] = visualize_batch_size # If we don't override the batch size in the config dictionary, the same batch size will be used as was used to train the network. Therefore, we override it.\n","        display_step = 1\n","\n","    ## Define running variables ##\n","    mean_gen_loss = 0; mean_CNN_SSIM = 0 ; mean_CNN_MSE = 0 ; mean_CNN_CUSTOM = 0; report_num = 1  # First report to RayTune is report_num=1.\n","\n","    ###########################\n","    ### Instantiate Classes ###\n","    ###########################\n","\n","    # Generator #\n","    if train_SI==True:\n","        gen =  Generator(config=config, gen_SI=True,  input_size=sino_size, input_channels=sino_channels,  output_channels=image_channels).to(device)\n","    else:\n","        gen =  Generator(config=config, gen_SI=False, input_size=image_size, input_channels=image_channels, output_channels=sino_channels ).to(device)\n","\n","    # Optimizer #\n","    gen_opt = torch.optim.Adam(gen.parameters(), lr=config['gen_lr'], betas=(config['gen_b1'], config['gen_b2']))\n","\n","    # Dataloader #\n","    dataloader = DataLoader(\n","        NpArrayDataSet(image_path=image_path, sino_path=sino_path, config=config, image_size=image_size, image_channels=image_channels,\n","                       sino_size=sino_size, sino_channels=sino_channels, augment=augment, offset=offset, num_examples=num_examples, sample_division=sample_division),\n","        batch_size=batch_size,\n","        shuffle=shuffle\n","    )\n","\n","\n","    ##############################\n","    ### Set Initial Conditions ###\n","    ##############################\n","\n","    ## If loading checkpoint (training, testing or visualizing). For tuning, load_state=False (always). ##\n","    if load_state==True:\n","        checkpoint = torch.load(checkpoint_path) # checkpoint is a dictionary of dictionaries\n","        gen.load_state_dict(checkpoint['gen_state_dict'])\n","        gen_opt.load_state_dict(checkpoint['gen_opt_state_dict'])\n","\n","        # If testing or visualizing, start from the beginning #\n","        if run_mode=='test' or run_mode=='visualize':\n","            gen.eval()  # Evaluation mode-->don't run backprojection\n","            start_epoch=0; end_epoch=1; batch_step = 0\n","\n","        # If training, pick up where we left off #\n","        elif run_mode=='train':\n","            start_epoch = checkpoint['epoch'] # Note: if interrupted, this epoch may be trained more than once\n","            end_epoch = training_epochs\n","            batch_step = checkpoint['batch_step'] # Note: because training is done with shuffling (unless you alter it), stopping partway through a training epoch will result in the network seeing some training examples more than once, and some not at all.\n","\n","    ## If starting from scratch ##\n","    else:\n","        gen = gen.apply(weights_init)\n","        start_epoch=0 ; batch_step = 0\n","        end_epoch=num_epochs  # =1000 for tuning (Ray Tune terminates before you hit this), =training_epochs for training, =1 for testing or visualizing\n","\n","\n","    ## Initialize timestamps to keep track of calculation times ##\n","    time_init_full = time.time()   # This is reset at the display time so that the full step time is displayed (see below).\n","    time_init_loader = time.time()  # This is reset at the display time, but also reset at the end of the inner \"for loop\", so that only displays the data loading time.\n","\n","    ########################\n","    ### Loop over Epochs ###\n","    ########################\n","\n","    ### Loop over Epochs ###\n","    for epoch in range(start_epoch, end_epoch):\n","\n","        #########################\n","        ### Loop Over Batches ###\n","        #########################\n","        for sino_ground, sino_ground_scaled, image_ground, image_ground_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n","\n","            # Show times #\n","            current_time = display_times('loader time', time_init_loader, show_times) # current_time is a dummy variable that isn't used in this loop\n","            time_init_full = display_times('FULL STEP TIME', time_init_full, show_times) # This step resets time_init_full after displaying the time so this displays the full time to fun the loop over a batch.\n","\n","            # Assign inputs and targets #\n","            if train_SI==True:\n","                target=image_ground_scaled\n","                input=sino_ground_scaled\n","            else:\n","                target=sino_ground_scaled\n","                input=image_ground_scaled\n","\n","            #######################\n","            ## Calculate Outputs ##\n","            #######################\n","\n","            ## If Tuning or Training, train one step ##\n","            if run_mode=='tune' or run_mode=='train':\n","                time_init_train = time.time() # Initialize timestamp for training duration\n","\n","                gen_opt.zero_grad()\n","                CNN_output = gen(input)\n","\n","                if run_mode=='train' and torch.sum(CNN_output[1,0,:]) < 0: # Let's you know if the network starts outputing predominantly negative values.\n","                    print('PIXEL VALUES SUM TO A NEGATIVE NUMBER. IF THIS CONTINUES FOR AWHILE, YOU MAY NEED TO RESTART')\n","\n","                # Update gradients\n","                gen_loss = sup_criterion(CNN_output, target)\n","                gen_loss.backward()\n","                gen_opt.step()\n","                # Keep track of the average generator loss\n","                mean_gen_loss += gen_loss.item() / display_step\n","\n","                current_time = display_times('training time', time_init_train, show_times)\n","\n","            ## If Testing or Vizualizing, calculate output only ##\n","            else:\n","                CNN_output=gen(input).detach()\n","\n","            # Increment batch_step\n","            batch_step += 1\n","\n","            ####################################\n","            ### Run-Type Specific Operations ###\n","            ####################################\n","            time_init_metrics=time.time()\n","\n","\n","            ## If Tuning or Training ##\n","            # We only calculate the mean value of the metrics, but not dataframes or reconstructions. Mean values are used to calculate the optimization metrics #\n","            if (run_mode == 'tune') or (run_mode=='train'):\n","\n","                mean_CNN_SSIM += calculate_metric(target, CNN_output, SSIM)/ display_step # The SSIM function can only take single images as inputs, not batches, so we use a wrapper function and pass batches to it.\n","                mean_CNN_MSE +=  calculate_metric(target, CNN_output, MSE) / display_step # The MSE function can take either single images or batches. We use the wrapper for consistency.\n","\n","                time_init_custom=time.time()\n","                # Custom metrics can take a long time to calculate, so we don't use a wrapper (which would loop through individual images in calculations.)\n","                mean_CNN_CUSTOM += custom_metric(target, CNN_output) / display_step\n","                current_time = display_times('Custom metric time', time_init_custom, show_times)\n","\n","            ## If Testing ##\n","            # We reconstruct images and we calculate metric dataframes #\n","            if run_mode == 'test':\n","                test_dataframe, mean_CNN_MSE, mean_CNN_SSIM, mean_FBP_MSE, mean_FBP_SSIM, mean_MLEM_MSE, mean_MLEM_SSIM, FBP_output, MLEM_output =  reconstruct_images_and_update_test_dataframe(\n","                    input, image_size, CNN_output, image_ground_scaled, test_dataframe, config)\n","\n","            ## If Visualizing ##\n","            if run_mode=='visualize':\n","                # We calculate reconstructions but not metric values. #\n","                FBP_output =  reconstruct(input, config, image_size=image_size, recon_type='FBP')\n","                MLEM_output = reconstruct(input, config, image_size=image_size, recon_type='MLEM')\n","\n","\n","            # Show metric calculation time #\n","            current_time = display_times('metrics time', time_init_metrics, show_times)\n","\n","            ######################################\n","            ### VISUALIZATION / REPORTING CODE ###\n","            ######################################\n","\n","            if batch_step % display_step == 0 # and (batch_step > 0 or run_mode != 'tune'):\n","\n","                time_init_visualization=time.time()\n","\n","                example_num = batch_step*batch_size\n","\n","                ## If Tuning ##\n","                if run_mode=='tune':\n","\n","                    session.report({'MSE':mean_CNN_MSE, 'SSIM':mean_CNN_SSIM, 'CUSTOM':mean_CNN_CUSTOM, 'example_number': example_num, 'batch_step':batch_step, 'epoch':epoch}) # Report to RayTune multiple times per trial\n","\n","                    if int(tune_dataframe_fraction*tune_max_t) == report_num: # We only update tune_dataframe once per trial\n","                        tune_dataframe = update_tune_dataframe(tune_dataframe, gen, config, mean_CNN_MSE, mean_CNN_SSIM, mean_CNN_CUSTOM)\n","\n","                    report_num +=1\n","\n","                ## If Training ##\n","                if run_mode == 'train':\n","                    # Display Batch Metrics #\n","                    print('================Training===================')\n","                    print(f'CURRENT PROGRESS: epoch: {epoch} / batch_step: {batch_step} / image #: {example_num}')\n","                    print(f'mean_gen_loss:', mean_gen_loss)\n","                    print(f'mean_CNN_MSE :', mean_CNN_MSE)\n","                    print(f'mean_CNN_SSIM:', mean_CNN_SSIM)\n","                    print(f'mean-CNN_CUSTOM', mean_CNN_CUSTOM)\n","                    print('===========================================')\n","                    print('Last Batch MSE: ', calculate_metric(target, CNN_output, MSE))\n","                    print('Last Batch SSIM: ', calculate_metric(target, CNN_output, SSIM))\n","\n","                    # Display Inputs & Reconstructions#\n","                    print('Input:')\n","                    show_single_unmatched_tensor(input[0:9])\n","                    print('Target/Output:')\n","                    show_multiple_matched_tensors(target[0:9], CNN_output[0:9])\n","\n","                ## If Testing ##\n","                if run_mode == 'test':\n","                    # Display Batch Metrics #\n","                    print('==================Testing==================')\n","                    print(f'mean_CNN_MSE/mean_MLEM_MSE/mean_FBP_MSE : {mean_CNN_MSE}/{mean_MLEM_MSE}/{mean_FBP_MSE}')\n","                    print(f'mean_CNN_SSIM/mean_MLEM_SSIM/mean_FBP_SSIM: {mean_CNN_SSIM}/{mean_MLEM_SSIM}/{mean_FBP_SSIM}')\n","                    print('===========================================')\n","\n","                    # Display Inputs & Reconstructions #\n","                    print('Input')\n","                    show_single_unmatched_tensor(input[0:9])\n","                    print('Target/Output/MLEM/FBP:')\n","                    show_multiple_matched_tensors(target[0:9], CNN_output[0:9], MLEM_output[0:9], FBP_output[0:9])\n","\n","                ## If Visualizing ##\n","                if run_mode == 'visualize':\n","                    if visualize_batch_size==120:\n","                        print(f'visualize_offset: {visualize_offset}, Image Number (batch_step*120): {batch_step*120}')\n","                        show_single_unmatched_tensor(target, grid=True, cmap='inferno', fig_size=1)\n","                    else:\n","                        print('Input:')\n","                        show_single_unmatched_tensor(input[0:visualize_batch_size])\n","                        print('Target/ML-EM/FBP/Output:')\n","                        show_multiple_matched_tensors(target[0:visualize_batch_size], MLEM_output[0:visualize_batch_size], FBP_output[0:visualize_batch_size], CNN_output[0:visualize_batch_size])\n","\n","\n","                # Save State -- This does not occur with every batch used in training so save resources #\n","                if save_state:\n","                    print('Saving model!')\n","                    torch.save({\n","                        'epoch': epoch,\n","                        'batch_step': batch_step,\n","                        'gen_state_dict': gen.state_dict(),\n","                        'gen_opt_state_dict': gen_opt.state_dict(),\n","                        }, checkpoint_path)\n","\n","                # Zero running stats -- occurs once per visualization step #\n","                mean_gen_loss = 0 ; mean_CNN_SSIM = 0 ; mean_CNN_MSE = 0 ; mean_CNN_CUSTOM=0\n","\n","                # Show visualization time #\n","                current_time = display_times('visualization time', time_init_visualization, show_times)\n","\n","\n","            # Time step to display loader time\n","            time_init_loader = time.time()\n","\n","\n","    ############################################\n","    ### Complete end of Train Function Tasks ###\n","    ############################################\n","\n","    # Save Network State (Training) #\n","    if save_state:\n","        print('Saving model!')\n","        path = os.path.join(checkpoint_dir, checkpoint_file)\n","        torch.save({\n","            'epoch': epoch+1, # If we are saving after an epoch is completed, and we pick up training later, we have to start at the next epoch.\n","            'batch_step': batch_step,\n","            'gen_state_dict': gen.state_dict(),  # dictionary of dictionaries!\n","            'gen_opt_state_dict': gen_opt.state_dict(),\n","            }, path)\n","\n","    # If testing, return dataframe #\n","    if run_mode=='test':\n","        return test_dataframe"]},{"cell_type":"markdown","metadata":{"id":"0LRDr105TDxk"},"source":["## GAN / CYCLE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cs3heiSl-JyD"},"outputs":[],"source":["'''\n","Note: It makes no sense to \"test\" a GAN or use SSIM since there is nothing to compare it to. Therefore, this functionality is left out here.\n","Also, now that you've defined assigned the checkpoint_dir and test_dataframe_dir in the \"User Parameters cell\", you can get rid of the path constructions below.\n","\n","'''\n","def train_test_GAN(config, checkpoint_dir=None, load_state=False, save_state=False):\n","    '''\n","    Note: Arguments are set to False/None to ensure that when RayTune calles train(), states are not saved/loaded\n","    Note: you may want to use 'model.train()' to put model back into training mode if you put it into eval mode at some point...\n","    '''\n","    print('Training GAN only!!')\n","\n","    ## Grab from Config ##\n","\n","    batch_size=config['batch_size']\n","    gen_adv_criterion=config['gen_adv_criterion']\n","    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n","\n","    ## Tensorboard ##\n","    writer=SummaryWriter(tensorboard_dir)\n","\n","    # Generators/Discriminators #\n","\n","    ## These are the original networks, and work great with 71x71 images ##\n","    #disc = Disc_I_Orig(config=config).to(device)\n","    #gen =  Gen_SI_Orig(config=config).to(device)\n","\n","    ## These are the modified networks, for 90x90, and also work great ##\n","    #disc = Disc_I_Orig_90(config=config).to(device)\n","    #gen = Gen_SI_Orig_90(config=config).to(device)\n","\n","    if train_SI==True:\n","        ## Now let's try a flex generator and Gen_SI_Orig_90 discriminator ##\n","        disc_adv_criterion=config['SI_disc_adv_criterion']\n","        disc = Disc_I_90(config=config, input_channels=image_channels).to(device)\n","        gen =  Gen_90(config=config, gen_SI=True, input_channels=sino_channels, output_channels=image_channels).to(device)\n","        gen_opt = torch.optim.Adam(gen.parameters(), lr=config['gen_lr'], betas=(config['gen_b1'], config['gen_b2'])) #betas are optional inputs\n","        disc_opt = torch.optim.Adam(disc.parameters(), lr=config['SI_disc_lr'], betas=(config['SI_disc_b1'], config['SI_disc_b2']))\n","    else:\n","        disc_adv_criterion=config['IS_disc_adv_criterion']\n","        disc = Disc_S_90(config=config, input_channels=sino_channels).to(device)\n","        gen =  Gen_90(config=config, gen_SI=False, input_channels=image_channels, output_channels=sino_channels).to(device)\n","        gen_opt = torch.optim.Adam(gen.parameters(), lr=config['gen_lr'], betas=(config['gen_b1'], config['gen_b2'])) #betas are optional inputs\n","        disc_opt = torch.optim.Adam(disc.parameters(), lr=config['IS_disc_lr'], betas=(config['IS_disc_b1'], config['IS_disc_b2']))\n","\n","    ## Load Data ##\n","    dataloader = DataLoader(\n","        NpArrayDataSet(image_path=image_path, sino_path=sino_path, config=config, resize_size=resize_size, image_channels=image_channels, sino_channels=sino_channels, offset=True),\n","        batch_size=batch_size,\n","        shuffle=shuffle\n","    )\n","\n","    ## Load Checkpoint ##\n","    if checkpoint_dir and load_state:\n","        # Load dictionary\n","        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n","        checkpoint = torch.load(checkpoint_path)\n","        # Load values from dictionary\n","        start_epoch = checkpoint['epoch'] #If interrupted, this epoch may be trained more than once\n","        end_epoch = start_epoch + num_epochs\n","        batch_step = checkpoint['batch_step']\n","        gen.load_state_dict(checkpoint['gen_state_dict'])\n","        gen_opt.load_state_dict(checkpoint['gen_opt_state_dict'])\n","        disc.load_state_dict(checkpoint['disc_state_dict'])\n","        disc_opt.load_state_dict(checkpoint['disc_opt_state_dict'])\n","    else:\n","        print('Starting from scratch')\n","        start_epoch=0\n","        end_epoch=num_epochs\n","        batch_step = 0\n","        gen = gen.apply(weights_init)\n","        disc = disc.apply(weights_init) # Both gen & disc inherit nn.Module functionality (.apply())\n","\n","    ## Loop Over Epochs ##\n","    for epoch in range(start_epoch, end_epoch):\n","        pix_dist_real_array = np.array([]) # Reset every epoch\n","        mean_gen_loss = 0  # Reset every display step, but I define it here so it's available later\n","        mean_disc_loss = 0 # Reset every display step\n","        mean_pix_metric = 0  # Reset every display step\n","        time_init_full = time.time()\n","\n","        ## Loop Over Batches ##\n","        for sino, sino_ground_scaled, image, image_ground_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n","\n","            print(f'FULL step (time): {(time.time()-time_init_full)*1000}')\n","            time_init_full = time.time()\n","\n","            if train_SI==True:\n","                real=image_ground_scaled\n","                noise=sino_ground_scaled\n","            else:\n","                real=sino_ground_scaled\n","                noise=image_ground_scaled\n","\n","            #print(f'Real Type: {real.dtype}, Real Shape:  {real.shape}')\n","            #print(f'Noise Type: {noise.dtype}, Noise Shape:  {noise.shape}')\n","            #cur_batch_size = len(real)\n","\n","            ## UPDATE DISCRIMINATOR ##\n","            disc_opt.zero_grad()                    # Zero gradients before every batch #\n","            disc_real_pred = disc(real)             # Predictions on Real Images #\n","\n","            with torch.no_grad(): # We won't be optmizing generator here, so disabling gradients saves on resources\n","                fake = gen(noise)\n","            disc_fake_pred = disc(fake.detach())\n","\n","            a = torch.ones_like(disc_real_pred)\n","\n","            disc_real_loss = disc_adv_criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n","            disc_fake_loss = disc_adv_criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n","\n","            disc_loss = (disc_fake_loss + disc_real_loss) / 2\n","            disc_loss.backward(retain_graph=True) # retain_graph=True is set so that we can perform gradient calculations using \"backward\" twice:\n","                                                  # you need to compute gradients of discriminator in order to obtain gradients of generator, later.\n","                                                  # Otherwise, for performance reasons, you can't do this.\n","            disc_opt.step()\n","\n","            # Keep track of the average discriminator loss\n","            mean_disc_loss += disc_loss.item() / display_step\n","\n","            ## UPDATE GENERATOR ##\n","            gen_opt.zero_grad()\n","            # Generator adversarial loss\n","            disc_fake_pred = disc(gen(noise))\n","            gen_loss = gen_adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n","            # Update gradients\n","            gen_loss.backward()\n","            gen_opt.step()\n","            # Keep track of the average generator loss\n","            mean_gen_loss += gen_loss.item() / display_step #gen_loss.item() reduces tensor to scalar. It updates loss per display step\n","\n","            ## PIXEL DISTANCE METRIC ##\n","            pix_dist_fake = pixel_dist(fake)\n","            pix_dist_real = pixel_dist(real)\n","            pix_dist_real_array = np.append(pix_dist_real_array, pix_dist_real)\n","            pix_dist_real_avg = np.mean(pix_dist_real_array)\n","            #pix_dist_real_avg = 0.00029 # determined experimentally\n","            pix_metric = abs((pix_dist_real_avg-pix_dist_fake)/pix_dist_real_avg)\n","            mean_pix_metric += pix_metric / display_step\n","\n","            ## visualization CODE ##\n","            if batch_step % display_step == 0 and batch_step > 0: # runs if batch_step is a multiple of the display step\n","\n","                # Calculate Individual Loss Terms #\n","                loss_balance=abs(mean_gen_loss-mean_disc_loss)\n","                r_metric= range_metric(real, fake)\n","                a_metric= avg_metric(real, fake)\n","\n","                # Metric Loss #\n","                optim_metric=0.5*loss_balance+mean_pix_metric+a_metric #+r_metric\n","\n","                ## REPORT AND SAVE STATE ##\n","                # Report #\n","                if run_mode=='tune':\n","                    tune.report(batch_step=batch_step, epoch=epoch,\n","                                mean_gen_loss=mean_gen_loss, mean_disc_loss=mean_disc_loss, loss_balance=loss_balance,\n","                                range_metric=r_metric, avg_metric = a_metric, mean_pix_metric=mean_pix_metric, optim_metric=optim_metric\n","                                )\n","                else:\n","                    # Display Stats #\n","                    print(f'===========================================\\nEPOCH: {epoch}, STEP: {batch_step}')\n","\n","                    print(f'Real Image Batch Min: {torch.min(real)} // Max: {torch.max(real)} // Mean: {torch.mean(real)} // Sum: {torch.sum(real).item()}')\n","                    print(f'Fake Image Batch Min: {torch.min(fake)} // Max: {torch.max(fake)} // Mean: {torch.mean(fake)} // Sum: {torch.sum(fake).item()}')\n","                    print(f'mean_gen_loss: {mean_gen_loss} // mean_disc_loss: {mean_disc_loss}')\n","                    print(f'loss_balance: {loss_balance}')\n","                    print(f'mean_pixel_metric: {mean_pix_metric}')\n","                    print(f'range_metric: {r_metric}')\n","                    print(f'avg_metric: {a_metric}')\n","                    print(f'optim_metric: {optim_metric}')\n","\n","                    # visualize Images #\n","                    print('Reals: ')\n","                    show_single_unmatched_tensor(real)\n","                    print('Fakes: ')\n","                    show_single_unmatched_tensor(fake)\n","\n","                    writer.add_scalar('generator loss', mean_gen_loss, batch_step)\n","                    writer.add_scalar('discriminator loss', mean_disc_loss, batch_step)\n","                    writer.add_scalar('loss balance', loss_balance, batch_step)\n","                    writer.add_scalar('pixel distance loss', mean_pix_metric, batch_step)\n","                    #writer.add_image(\"real\", make_grid(real_image_tensor[:25], nrow=5, normalize=True)) # [:num_images]=[0:num_images]\n","                    #writer.add_image(\"fake\", make_grid(fake_image_tensor[:25], nrow=5, normalize=True))\n","                    writer.flush()\n","\n","                # Save State #\n","                if checkpoint_dir and save_state:\n","                    path = os.path.join(checkpoint_dir, checkpoint_file)\n","                    torch.save({\n","                        'epoch': epoch,\n","                        'batch_step': batch_step,\n","                        'gen_state_dict': gen.state_dict(),\n","                        'gen_opt_state_dict': gen_opt.state_dict(),\n","                        'disc_state_dict': disc.state_dict(),\n","                        'disc_opt_state_dict': disc_opt.state_dict(),\n","                        }, path)\n","\n","                # Zero Stats #\n","                mean_disc_loss = 0\n","                mean_gen_loss = 0\n","                mean_pix_metric = 0\n","\n","    ## And the end of the epoch loop, we do a final save of the model ##\n","    if checkpoint_dir and save_state:\n","        path = os.path.join(checkpoint_dir, checkpoint_file)\n","        torch.save({\n","            'epoch': epoch,\n","            'batch_step': batch_step,\n","            'gen_state_dict': gen.state_dict(),\n","            'gen_opt_state_dict': gen_opt.state_dict(),\n","            'disc_state_dict': disc.state_dict(),\n","            'disc_opt_state_dict': disc_opt.state_dict(),\n","            }, path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dOrxzbuEV5N"},"outputs":[],"source":["## Note: This function still needs to be updated for SSIM and testing with the test set. See 'START HERE' comment below.\n","\n","def train_test_CYCLE(config, checkpoint_dir=None, load_state=False, save_state=False):\n","    '''\n","    Note: Arguments are set to None/False to ensure that when RayTune calles train(), states are not saved/loaded. This uses up way too much hard drive space.\n","    Note: you may want to use 'model.train()' to put model back into training mode if you put it into eval mode at some point...\n","    '''\n","\n","    ## Grab Stuff from Config Dict. ##\n","    batch_size = config['batch_size']\n","    gen_b1 = config['gen_b1']\n","    gen_b2 = config['gen_b2']\n","    gen_lr = config['gen_lr']\n","    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n","\n","    ## Tensorboard ##\n","    writer=SummaryWriter(tensorboard_dir)\n","\n","    ## Initialize Generators/Discriminator/Summary Writer ##\n","    disc_I = Disc_S_90(config=config, input_channels=image_channels).to(device)\n","    disc_S = Disc_S_90(config=config, input_channels=sino_channels).to(device)\n","    gen_SI = Gen_90(config=config, gen_SI=True, input_channels=sino_channels, output_channels=image_channels).to(device)\n","    gen_IS = Gen_90(config=config, gen_SI=False, input_channels=image_channels, output_channels=sino_channels).to(device)\n","\n","    gen_both_opt = torch.optim.Adam(list(gen_SI.parameters()) + list(gen_IS.parameters()), lr=gen_lr, betas=(gen_b1, gen_b2)) # Common optimizer\n","    disc_I_opt = torch.optim.Adam(disc_I.parameters(), lr=config['SI_disc_lr'], betas=(config['SI_disc_b1'], config['SI_disc_b2']))\n","    disc_S_opt = torch.optim.Adam(disc_S.parameters(), lr=config['IS_disc_lr'], betas=(config['IS_disc_b1'], config['IS_disc_b2']))\n","\n","    ## Load Data ##\n","    dataloader = DataLoader(\n","        NpArrayDataSet(image_path=image_path, sino_path=sino_path, config=config, resize_size=resize_size, image_channels=image_channels, sino_channels=sino_channels),\n","        batch_size=batch_size,\n","        shuffle=shuffle\n","    )\n","\n","    ## Load Checkpoint ##\n","    if checkpoint_dir and load_state:\n","        # Load dictionary\n","        checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoint_file))\n","        # Load values from dictionary\n","        start_epoch = checkpoint['epoch'] #If interrupted, this epoch may be trained more than once\n","        end_epoch = start_epoch + num_epochs\n","        batch_step = checkpoint['batch_step']\n","        gen_SI.load_state_dict(checkpoint['gen_SI_state_dict'])\n","        gen_IS.load_state_dict(checkpoint['gen_IS_state_dict'])\n","        gen_both_opt.load_state_dict(checkpoint['gen_both_opt_state_dict'])\n","        disc_I.load_state_dict(checkpoint['disc_I_state_dict'])\n","        disc_S.load_state_dict(checkpoint['disc_S_state_dict'])\n","        disc_I_opt.load_state_dict(checkpoint['disc_I_opt_state_dict'])\n","        disc_S_opt.load_state_dict(checkpoint['disc_S_opt_state_dict'])\n","        if run_mode=='test':\n","            gen_SI.eval()\n","            gen_IS.eval()\n","\n","    ## START HERE WITH UPDATING THIS FUNCION FOR SSIM AND TEST SET FUNCTIONALITY\n","\n","    else:\n","        print('Starting from scratch')\n","        start_epoch=0\n","        end_epoch=num_epochs\n","        batch_step = 0\n","        gen_SI = gen_SI.apply(weights_init)\n","        gen_IS = gen_IS.apply(weights_init)\n","        disc_I = disc_I.apply(weights_init)\n","        disc_S = disc_S.apply(weights_init)\n","\n","    ## Loop Over Epochs ##\n","    for epoch in range(start_epoch, end_epoch):\n","\n","        # Following variables reset every display step. The line below only establishes these variables, it does not reset them.\n","        mean_disc_loss, mean_adv_loss, mean_sup_loss, mean_cycle_loss, mean_pix_metric, mean_range_metric, mean_avg_metric = 0,0,0,0,0,0,0\n","\n","        ## Loop Over Batches ##\n","\n","        time_init_full = time.time()\n","        #time_init_loader = time.time()\n","\n","        for sino, sino_ground_scaled, image, image_ground_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n","\n","            #print(f'iter dataloader (time): {(time.time()-time_init_loader)*1000}')\n","            #print(f'FULL step (time): {(time.time()-time_init_full)*1000}')\n","            time_init_full = time.time()\n","\n","            real_S = sino_ground_scaled\n","            real_I = image_ground_scaled\n","\n","            ## Update Networks ##\n","\n","            # Update Discriminators #\n","            # Image Discriminator #\n","            disc_I_opt.zero_grad() # Zero out the gradient before backpropagation\n","            with torch.no_grad(): # We won't be optmizing the generator here, so disabling gradients saves on resources\n","                fake_I = gen_SI(real_S)\n","\n","            disc_I_loss = get_disc_loss(fake_I, real_I, disc_I, config['SI_disc_adv_criterion'])\n","            disc_I_loss.backward(retain_graph=True) # Update gradients\n","            disc_I_opt.step() # Update optimizer\n","\n","            # Sinogram Discriminator #\n","            disc_S_opt.zero_grad() # Zero out the gradient before backpropagation\n","            with torch.no_grad(): # We won't be optmizing the generator here, so disabling gradients saves on resources\n","                fake_S = gen_IS(real_I)\n","            disc_S_loss = get_disc_loss(fake_S, real_S, disc_S, config['IS_disc_adv_criterion'])\n","            disc_S_loss.backward(retain_graph=True) # Update gradients\n","            disc_S_opt.step() # Update optimizer\n","\n","            # Generators #\n","            gen_both_opt.zero_grad()\n","            gen_loss, adv_loss, sup_loss, cycle_loss, cycle_I, cycle_S = get_gen_loss(real_I, real_S, gen_IS, gen_SI, disc_I, disc_S, config)\n","            gen_loss.backward() # Update gradients\n","            gen_both_opt.step() # Update optimizer\n","\n","            #print(f'update generator (time)): {(time.time()-time_init_gen)*1000}')\n","\n","            ## Metrics ##\n","            # Pixel Distance #\n","            pix_metric_I = pixel_metric(real_I, fake_I)\n","            pix_metric_S = pixel_metric(real_S, fake_S)\n","            p_metric = pix_metric_I + pix_metric_S\n","\n","            # Range Metric #\n","            range_metric_I = range_metric(real_I, fake_I)\n","            range_metric_S = range_metric(real_S, fake_S)\n","            r_metric = range_metric_I+range_metric_S\n","\n","            # Average Metric #\n","            avg_metric_I = avg_metric(real_I, fake_I)\n","            avg_metric_S = avg_metric(real_S, fake_S)\n","            a_metric = avg_metric_I + avg_metric_S\n","\n","            ## Running Statistics ##\n","            # Mean loss terms #\n","            mean_disc_loss    += (abs(disc_I_loss.item()) + abs(disc_S_loss.item())) / display_step\n","            mean_adv_loss     += abs(adv_loss) / display_step\n","            mean_sup_loss     += abs(sup_loss) / display_step\n","            mean_cycle_loss   += abs(cycle_loss) / display_step\n","            mean_pix_metric   += p_metric / display_step\n","            mean_range_metric += r_metric / display_step\n","            mean_avg_metric   += a_metric / display_step\n","\n","            ## visualization CODE ##\n","            if batch_step % display_step == 1 and batch_step > 0: # runs if batch_step is a multiple of the display step\n","\n","                # Optim_Metric #\n","                MS_Error = MSE(real_I, fake_I)\n","                loss_balance=abs(mean_adv_loss-mean_disc_loss)\n","                #optim_metric = 0.5*loss_balance+mean_cycle_loss+mean_pix_metric #+mean_avg_metric #+mean_range_metric\n","                optim_metric = MS_Error\n","\n","                # Prune #\n","                #gen_SI = prune_gen(gen_SI)\n","                #gen_IS = prune_gen(gen_IS)\n","\n","                ## Report  to Ray Tune ##\n","                if run_mode=='tune':\n","                    tune.report(batch_step=batch_step, epoch=epoch,\n","                                mean_adv_loss=mean_adv_loss, mean_disc_loss=mean_disc_loss, loss_balance=loss_balance,\n","                                mean_sup_loss=mean_sup_loss,\n","                                mean_cycle_loss=mean_cycle_loss,\n","                                mean_pix_metric=mean_pix_metric,\n","                                mean_avg_metric=mean_avg_metric,\n","                                optim_metric=optim_metric\n","                                )\n","                ## Display Stats & Images ##\n","                else:\n","                    print(f'================================================================================\\nEPOCH: {epoch}, STEP: {batch_step}, Batch Size: {batch_size}')\n","\n","                    lambda_adv, lambda_sup, lambda_cycle = config['lambda_adv'], config['lambda_sup'], config['lambda_cycle']\n","\n","                    print(f'MSE (Images):  {MS_Error}')\n","                    print(f'lambda * Mean Adversarial Loss: {lambda_adv*mean_adv_loss}')\n","                    print(f'lambda * Mean Supervisory Loss: {lambda_sup*mean_sup_loss}')\n","                    print(f'lambda * Mean Cycle Loss      : {lambda_cycle*mean_cycle_loss}')\n","                    print(f'mean_disc_loss: {mean_disc_loss} // mean_adv_loss: {mean_adv_loss} // loss_balance (M) {loss_balance}')\n","                    print(f'mean_pix_metric (M): {mean_pix_metric}')\n","                    print(f'range_metric (M): {mean_range_metric}')\n","                    print(f'avg_metric: {mean_avg_metric}')\n","                    print(f'optim_metric: {optim_metric}')\n","\n","                    ## visualize Images ##\n","                    # Images #\n","                    print('Ground Truth Images:')\n","                    show_single_unmatched_tensor(real_I)\n","                    print('Generated PET Images:')\n","                    show_single_unmatched_tensor(fake_I)\n","                    print('Cycle PET Images:')\n","                    show_single_unmatched_tensor(cycle_I)\n","\n","                    # Sinograms #\n","                    print('Grount Truth Sinograms:')\n","                    show_single_unmatched_tensor(real_S) # low_rez_S = real\n","                    print('Generated Sinograms:')\n","                    show_single_unmatched_tensor(fake_S)\n","                    print('Cycle Sinograms:')\n","                    show_single_unmatched_tensor(cycle_S)\n","\n","                    # Less interesting #\n","                    '''\n","                    print('Resized Model Images:')\n","                    show_single_unmatched_tensor(resized_I[0:9])\n","                    print('FBP, Full-Rez Sinograms, resized (90x90):')\n","                    show_single_unmatched_tensor(FBP_I[0:9])\n","\n","                    print('Hi-Rez Sinograms:')\n","                    show_single_unmatched_tensor(high_rez_S)\n","                    print('Sinogram of Ground Truth Images:')\n","                    show_single_unmatched_tensor(project(ground_I))\n","                    print('Sinogram of Generated PET:')\n","                    show_single_unmatched_tensor(project(fake_I))\n","                    print('FBP, Low-Rez Sinograms:')\n","                    show_single_unmatched_tensor(reconstruct(low_rez_S))\n","                    '''\n","\n","                    writer.add_scalar('mean adversarial loss', mean_adv_loss, batch_step)\n","                    writer.add_scalar('discriminator loss', mean_disc_loss, batch_step)\n","                    writer.add_scalar('loss balance', loss_balance, batch_step)\n","                    writer.add_scalar('pixel distance loss', mean_pix_metric, batch_step)\n","                    writer.add_scalar('cycle loss', mean_cycle_loss)\n","                    writer.add_scalar('supervisory loss (ground)', mean_sup_loss)\n","                    writer.flush()\n","\n","                # Save State #\n","                if checkpoint_dir and save_state:\n","                    path = os.path.join(checkpoint_dir, checkpoint_file)\n","                    torch.save({\n","                        'epoch': epoch,\n","                        'batch_step': batch_step,\n","                        'gen_SI_state_dict': gen_SI.state_dict(),\n","                        'gen_IS_state_dict': gen_IS.state_dict(),\n","                        'gen_both_opt_state_dict': gen_both_opt.state_dict(),\n","                        'disc_I_state_dict': disc_I.state_dict(),\n","                        'disc_S_state_dict': disc_S.state_dict(),\n","                        'disc_I_opt_state_dict': disc_I_opt.state_dict(),\n","                        'disc_S_opt_state_dict': disc_S_opt.state_dict(),\n","                        }, path)\n","\n","                # Zero Stats #\n","                mean_adv_loss = 0  # Should balance with mean_disc_loss (below)\n","                mean_disc_loss = 0 # Should balance with mean_adv_loss (above)\n","                mean_sup_loss_model = 0\n","                mean_sup_loss_ground = 0 #\n","                mean_cycle_loss = 0 # Better performing models will minimize this\n","                mean_pix_metric = 0 # Reasonable to minimize this for tuning purposes\n","                mean_range_metric=0\n","                mean_avg_metric=0\n","\n","            batch_step += 1 #updates with every batch\n","\n","\n","            time_init_loader=time.time()\n","#call model.eval() before test set\n"]},{"cell_type":"markdown","metadata":{"id":"jw52guvr8KVm"},"source":["## Tune Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRUCef12dLWG"},"outputs":[],"source":["def Tune(tune_max_t=40, trainable='SUP', grace_period=1):\n","    '''\n","    This function is called to tune the \"trainable\" function, given:\n","\n","    tune_max_t:     maximum number of time units (in this case, number of reports) per trial.\n","    grace_period:   minimum number of raytune reports to run before aborting a trial due to poor performance\n","    '''\n","\n","    ## What am I tuning for? ##\n","    if tune_for=='MSE':     # Values for these metric labels are passed to RayTune in the training function: session.report(.)\n","        optim_metric='MSE'\n","        min_max='min' # minimise MSE\n","    elif tune_for=='SSIM':\n","        optim_metric='SSIM'\n","        min_max='max' # maximize SSIM\n","    elif tune_for=='CUSTOM':\n","        optim_metric='CUSTOM'\n","        min_max='min'\n","\n","    print('===================')\n","    print('tune_max_t:', tune_max_t)\n","    print('optim_metric:',optim_metric)\n","    print('min_max:', min_max)\n","    print('grace_period:', grace_period)\n","    print('tune_minutes', tune_minutes)  # Set in \"User Parameters\".\n","    print('===================')\n","\n","    ## Reporters ##\n","    reporter1 = CLIReporter( # This reporter currently isn't used.\n","        metric_columns=[optim_metric,'batch_step'])\n","\n","    reporter = JupyterNotebookReporter(\n","        overwrite=True,                                           # Overwrite subsequent reporter tables in output (so there is no scrolling)\n","        metric_columns=[optim_metric,'batch_step','example_num'], # Values for both 'batch_step' and 'example_num' are passed to RayTune\n","        metric=[optim_metric],                                    # Which metric is used to determine best trial?\n","        #mode=['min'],\n","        sort_by_metric=True,                                      # Order reporter table by metric\n","    )\n","\n","    ## Trial Scheduler and Run Config ##\n","    if tune_scheduler == 'ASHA':\n","        scheduler = ASHAScheduler(\n","            time_attr='training_iteration', # \"Time\" is measured in training iterations. 'training_iteration' is a RayTune keyword (not passed in session.report(...)).\n","            max_t=tune_max_t, # (default=40). Maximum time units per trial (units = time_attr). Note: Ray Tune will by default run a maximum of 100 display steps (reports) per trial\n","            metric=optim_metric, # This is the label in a dictionary passed to RayTune (in session.report(...))\n","            mode=min_max,\n","            grace_period=grace_period, # Train for a minumum number of time_attr. Set in Tune() arguments.\n","            #reduction_factor=2\n","            )\n","        run_config=air.RunConfig(       # How to perform the run\n","            name=tune_exp_name,         # Ray checkpoints saved to this file, relative to local_dir. Set in \"User Parameters\"\n","            storage_path=local_dir,     # Local directory. Set in \"User Parameters\"\n","            progress_reporter=reporter, # Specified above\n","            failure_config=air.FailureConfig(fail_fast=False), # default = False. Keeps running if there is an error.\n","            checkpoint_config=air.CheckpointConfig(\n","                num_to_keep=10,         # Maximum number of checkpoints that are kept per run (for each trial)\n","                checkpoint_score_attribute=optim_metric,  # Determines which checkpoints are kept on disk.\n","                checkpoint_score_order=min_max\n","                )\n","            )\n","    else:\n","        scheduler = FIFOScheduler()     # First in/first out scheduler\n","        run_config=train.RunConfig(\n","            stop={'training_iteration': tune_max_t}, # When using the FIFO scheduler, we must explicitly specify the stopping criterian.\n","            name=tune_exp_name,         # Ray checkpoints saved to this file, relative to local_dir\n","            storage_path=local_dir,     # Local directory\n","            progress_reporter=reporter,\n","            failure_config=air.FailureConfig(fail_fast=False), # default = False\n","            checkpoint_config=air.CheckpointConfig(\n","                num_to_keep=10,         # Maximum number of checkpoints that are kept per run.\n","                checkpoint_score_attribute=optim_metric,  # Determines which checkpoints are kept on disk.\n","                checkpoint_score_order=min_max)\n","        )\n","        '''\n","        run_config=train.RunConfig(       # How to perform the run\n","            name=tune_exp_name,              # Ray checkpoints saved to this file, relative to local_dir\n","            storage_path=local_dir,     # Local directory\n","            progress_reporter=reporter,\n","            failure_config=air.FailureConfig(fail_fast=False), # default = False\n","            checkpoint_config=air.CheckpointConfig(\n","                num_to_keep=10,         # Maximum number of checkpoints that are kept per run.\n","                checkpoint_score_attribute=optim_metric,  # Determines which checkpoints are kept on disk.\n","                checkpoint_score_order=min_max,\n","                stop={\"time_total_s\": 5})\n","            #    stop={\"training_iteration\": tune_max_t}) # The FIFO scheduler does not have a stopping criterian, so this stops the trial.\n","            )\n","        '''\n","\n","    ## HyperOpt Search Algorithm ##\n","    search_alg = HyperOptSearch(metric=optim_metric, mode=min_max)  # It's also possible to pass the search space directly to the search algorithm here.\n","                                                                    # But then the search space needs to be defined in terms of the specific search algorithm methods, rather than letting RayTune translate.\n","\n","    ## Which trainable do you want to use? ##\n","    if trainable=='SUP':\n","        trainable_with_resources = tune.with_resources(train_Supervisory_Sym, {\"CPU\":num_CPUs,\"GPU\":num_GPUs}) # train_Supervisory_Sym is a function of the config dictionary, but we don't state that explicitly.\n","    elif trainable=='GAN':\n","        trainable_with_resources = tune.with_resources(train_test_GAN, {\"CPU\":num_CPUs,\"GPU\":num_GPUs})\n","    elif trainable=='CYCLE':\n","        trainable_with_resources = tune.with_resources(train_test_CYCLE, {\"CPU\":num_CPUs,\"GPU\":num_GPUs})\n","\n","    ## If starting from scratch ##\n","    if tune_restore==False:\n","\n","        # Initialize a blank tuner object\n","        tuner = tune.Tuner(\n","                trainable_with_resources,       # The objective function w/ resources\n","                param_space=config,             # Let RayTune know what parameter space (dictionary) to search over.\n","                tune_config=tune.TuneConfig(    # How to perform the search\n","                    num_samples=-1,\n","                    time_budget_s=tune_minutes*60, # time_budget is in seconds\n","                    scheduler=scheduler,\n","                    search_alg=search_alg,\n","                    ),\n","                run_config=run_config\n","                )\n","\n","    ## If loading from a checkpoint ##\n","    else:\n","        # Load the tuner\n","        tuner = tune.Tuner.restore(\n","            path=os.path.join(local_dir, tune_exp_name), # Path where previous run is checkpointed\n","            trainable=trainable_with_resources,\n","            resume_unfinished = False\n","            )\n","\n","    result_grid: ResultGrid = tuner.fit()\n"]},{"cell_type":"markdown","metadata":{"id":"9jMA9no37mJa"},"source":["## Test CNN (by chunks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jImHfP4V7lle"},"outputs":[],"source":["def test_by_chunks(test_begin_at=0, test_chunk_size=5000, testset_size = 35000, sample_division=1, part_name='batch_dataframe_part_',\n","         test_merge_dataframes=False, test_csv_file='combined_dataframe'):\n","    '''\n","    Splits up testing the CNN (on a test set) into smaller chunks so that computer time-outs don't result in lost work.\n","\n","    test_begin_at:      Where to begin the testing. You set this to >0 if the test terminates early and you need to pick up partway through the test set.\n","    test_chunk_size:    How many examples to test in each chunk\n","    testset_size:       Number of examples that you wish to test. This can be less than the number of examples in the dataset file but not more.\n","    sample_division:    To test every example, set to 1. To test every other example, set to 2, and so forth.\n","    part_name:          Roots of dataframe parts files (containing testing results) that will be saved. These will have a number appended to them when saved.\n","    test_merge_dataframes:  Set to True to merge the smaller parts dataframes into a larger dataframe once the smaller parts have finished calculating.\n","                            Otherwise, you can use the MergeTests function below at a later time.\n","    '''\n","\n","    label_num=int(test_begin_at/test_chunk_size) # Which numbered dataframe parts file you start at.\n","\n","    for index in range(test_begin_at, testset_size, test_chunk_size):\n","\n","        save_filename = part_name+str(label_num)+'.csv'\n","\n","        print('###############################################')\n","        print(f'################# Working on:', save_filename)\n","        print(f'################# Starting at example: ', index)\n","        print('###############################################')\n","\n","        # Since run_mode=='test', the training function returns a test dataframe. #\n","        chunk_dataframe = train_Supervisory_Sym(config, offset=index, num_examples=test_chunk_size, sample_division=sample_division)\n","        chunk_dataframe_path = os.path.join(test_dataframe_dir, save_filename)\n","        chunk_dataframe.to_csv(chunk_dataframe_path, index=False)\n","        label_num += 1\n","\n","    if test_merge_dataframes==True:\n","        max_index = int(testset_size/test_chunk_size)-1\n","        merge_test_chunks(max_index, part_name=part_name, test_csv_file=test_csv_file)\n","\n","\n","def merge_test_chunks(max_index, part_name='batch_dataframe_part_', test_csv_file='combined_dataframe'):\n","    '''\n","    Function for merging smaller dataframes (which contain metrics for individual images) into a single larger dataframe.\n","\n","    max_index:      number of largest index\n","    part_name:      root of part filenames (not including the numbers appended to the end)\n","    test_csv_file:  filename for the combined dataframe\n","    '''\n","\n","    ## Build list of filenames ##\n","    names = []\n","    for i in range(0, max_index+1):\n","        save_filename = part_name+str(i)+'.csv'\n","        names.append(save_filename)\n","\n","    ## Concatenate parts dataframes ##\n","    first = True\n","    for name in names:\n","        add_path = os.path.join(test_dataframe_dir, name)\n","        print('Concatenating: ', add_path)\n","        add_frame = pd.read_csv(add_path)\n","\n","        if first==True:\n","            test_dataframe = add_frame\n","            first=False\n","        else:\n","            test_dataframe = pd.concat([test_dataframe, add_frame], axis=0)\n","\n","    ## Save Result ##\n","    test_dataframe.to_csv(test_dataframe_path, index=False)\n","    test_dataframe.describe()\n","\n","#merge_test_chunks(34)"]},{"cell_type":"markdown","metadata":{"id":"6TmoNMP8UO8j"},"source":["# Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2Qi-EkPUQaD","executionInfo":{"status":"error","timestamp":1747677761067,"user_tz":300,"elapsed":81734,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"520cbbd1-5b02-4585-e8e2-357c6412cac9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-05-19 18:01:21,980\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"]},{"output_type":"stream","name":"stdout","text":["+----------------------------------------------------------+\n","| Configuration for experiment     search-Temp             |\n","+----------------------------------------------------------+\n","| Search algorithm                 SearchGenerator         |\n","| Scheduler                        AsyncHyperBandScheduler |\n","| Number of trials                 9223372036854775807     |\n","+----------------------------------------------------------+\n","\n","View detailed results here: /content/drive/MyDrive/Colab/Working/search-Temp\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-05-19_17-53-49_975554_2531/artifacts/2025-05-19_18-01-21/search-Temp/driver_artifacts`\n","\n","Trial status: 1 PENDING\n","Current time: 2025-05-19 18:01:22. Total running time: 0s\n","Logical resource usage: 4.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                       status     SI_normalize       SI_gen_mult     SI_gen_fill     SI_gen_neck     SI_gen_z_dim   SI_layer_norm     SI_pad_mode     SI_dropout       SI_exp_kernel   SI_gen_final_activ       SI_gen_hidden_dim     batch_size       gen_lr     gen_b1     gen_b2   sup_criterion   |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_Supervisory_Sym_c00a965c   PENDING    False                  3.01369               1              11              787   batch             reflect         True                         3                                            4            512   0.00317833   0.238311   0.265424   L1Loss()        |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n","Trial train_Supervisory_Sym_c00a965c started with configuration:\n","+----------------------------------------------------------+\n","| Trial train_Supervisory_Sym_c00a965c config              |\n","+----------------------------------------------------------+\n","| IS_disc_adv_criterion                                  1 |\n","| IS_disc_b1                                             1 |\n","| IS_disc_b2                                             1 |\n","| IS_disc_hidden_dim                                     1 |\n","| IS_disc_lr                                             1 |\n","| IS_disc_patchGAN                                       1 |\n","| SI_disc_adv_criterion                                  1 |\n","| SI_disc_b1                                             1 |\n","| SI_disc_b2                                             1 |\n","| SI_disc_hidden_dim                                     1 |\n","| SI_disc_lr                                             1 |\n","| SI_disc_patchGAN                                       1 |\n","| SI_dropout                                          True |\n","| SI_exp_kernel                                          3 |\n","| SI_gen_fill                                            1 |\n","| SI_gen_final_activ                                       |\n","| SI_gen_hidden_dim                                      4 |\n","| SI_gen_mult                                      3.01369 |\n","| SI_gen_neck                                           11 |\n","| SI_gen_z_dim                                         787 |\n","| SI_layer_norm                                      batch |\n","| SI_normalize                                       False |\n","| SI_pad_mode                                      reflect |\n","| SI_scale                                            8100 |\n","| batch_size                                           512 |\n","| gen_b1                                           0.23831 |\n","| gen_b2                                           0.26542 |\n","| gen_lr                                           0.00318 |\n","| sup_criterion                                   L1Loss() |\n","+----------------------------------------------------------+\n","\u001b[36m(train_Supervisory_Sym pid=6418)\u001b[0m Dataset offset: 0\n","\u001b[36m(train_Supervisory_Sym pid=6418)\u001b[0m Dataset num_examples: -1\n","\u001b[36m(train_Supervisory_Sym pid=6418)\u001b[0m Dataset sample_division: 1\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[36m(train_Supervisory_Sym pid=6418)\u001b[0m <ipython-input-8-8839941d6671>:93: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-05-19 18:01:52. Total running time: 30s\n","Logical resource usage: 4.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                       status     SI_normalize       SI_gen_mult     SI_gen_fill     SI_gen_neck     SI_gen_z_dim   SI_layer_norm     SI_pad_mode     SI_dropout       SI_exp_kernel   SI_gen_final_activ       SI_gen_hidden_dim     batch_size        gen_lr     gen_b1     gen_b2   sup_criterion   |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_Supervisory_Sym_c00a965c   RUNNING    False                  3.01369               1              11              787   batch             reflect         True                         3                                            4            512   0.00317833    0.238311   0.265424   L1Loss()        |\n","| train_Supervisory_Sym_9820be41   PENDING    True                   1.83327               1               1             1774   instance          reflect         True                         3                                            3             64   0.000110998   0.571989   0.467622   MSELoss()       |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-05-19 18:02:22. Total running time: 1min 0s\n","Logical resource usage: 4.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                       status     SI_normalize       SI_gen_mult     SI_gen_fill     SI_gen_neck     SI_gen_z_dim   SI_layer_norm     SI_pad_mode     SI_dropout       SI_exp_kernel   SI_gen_final_activ       SI_gen_hidden_dim     batch_size        gen_lr     gen_b1     gen_b2   sup_criterion       iter     total time (s)          MSE         SSIM     CUSTOM     example_number |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_Supervisory_Sym_c00a965c   RUNNING    False                  3.01369               1              11              787   batch             reflect         True                         3                                            4            512   0.00317833    0.238311   0.265424   L1Loss()               3            53.0304   4.7027e+08   0.00211798          0              15872 |\n","| train_Supervisory_Sym_9820be41   PENDING    True                   1.83327               1               1             1774   instance          reflect         True                         3                                            3             64   0.000110998   0.571989   0.467622   MSELoss()                                                                                           |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"output_type":"stream","name":"stderr","text":["2025-05-19 18:02:35,659\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n","2025-05-19 18:02:35,694\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/content/drive/MyDrive/Colab/Working/search-Temp' in 0.0333s.\n"]},{"output_type":"stream","name":"stdout","text":["Trial status: 1 RUNNING | 1 PENDING\n","Current time: 2025-05-19 18:02:35. Total running time: 1min 13s\n","Logical resource usage: 4.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n","+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                       status     SI_normalize       SI_gen_mult     SI_gen_fill     SI_gen_neck     SI_gen_z_dim   SI_layer_norm     SI_pad_mode     SI_dropout       SI_exp_kernel   SI_gen_final_activ       SI_gen_hidden_dim     batch_size        gen_lr     gen_b1     gen_b2   sup_criterion       iter     total time (s)           MSE         SSIM     CUSTOM     example_number |\n","+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| train_Supervisory_Sym_c00a965c   RUNNING    False                  3.01369               1              11              787   batch             reflect         True                         3                                            4            512   0.00317833    0.238311   0.265424   L1Loss()               4            62.2966   4.71189e+08   0.00228672          0              20992 |\n","| train_Supervisory_Sym_9820be41   PENDING    True                   1.83327               1               1             1774   instance          reflect         True                         3                                            3             64   0.000110998   0.571989   0.467622   MSELoss()                                                                                            |\n","+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"output_type":"stream","name":"stderr","text":["2025-05-19 18:02:38,896\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n","Resume experiment with: Tuner.restore(path=\"/content/drive/MyDrive/Colab/Working/search-Temp\", trainable=...)\n","2025-05-19 18:02:38,905\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n","- train_Supervisory_Sym_9820be41: FileNotFoundError('Could not fetch metrics for train_Supervisory_Sym_9820be41: both result.json and progress.csv were not found at /content/drive/MyDrive/Colab/Working/search-Temp/train_Supervisory_Sym_9820be41_2_IS_disc_adv_criterion=1,IS_disc_b1=1,IS_disc_b2=1,IS_disc_hidden_dim=1,IS_disc_lr=1,IS_disc_patch_2025-05-19_18-01-26')\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"error","ename":"SyntaxError","evalue":"'break' outside loop (<ipython-input-23-60432af037eb>, line 25)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-60432af037eb>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"]}],"source":["if run_mode=='tune':\n","    if train_type==\"SUP\":\n","        print('Tuning w/ Supervisory Only!')\n","        time.sleep(3)\n","        Tune(tune_max_t=tune_max_t, trainable='SUP', grace_period=1) # for 90-90, tune_max_t=35 | 180-71, tune_max_t=25p | for LDM, tune_max_T=25\n","    if train_type=='GAN':\n","        print('Tuning a GAN!')\n","        time.sleep(3)\n","        Tune(tune_max_t=tune_max_t, trainable='GAN', grace_period=1)\n","    if train_type=='CYCLESUP' or train_type=='CYCLEGAN':\n","        print('Tuning a Cycle!')\n","        time.sleep(3)\n","        Tune(tune_max_t=tune_max_t, trainable='CYCLE', grace_period=1)\n","elif (run_mode=='train') or (run_mode=='visualize'):\n","    if train_type==\"SUP\":\n","        train_Supervisory_Sym(config=config, offset=offset, num_examples=-1, sample_division=sample_division)\n","    if train_type=='GAN':\n","        train_test_GAN(config=config)\n","    if train_type=='CYCLESUP' or train_type=='CYCLEGAN':\n","        train_test_CYCLE(config=config)\n","elif run_mode=='test':\n","    test_by_chunks(test_begin_at=test_begin_at, test_chunk_size=test_chunk_size, testset_size=testset_size, sample_division=sample_division, part_name='batch_dataframe_part_', test_merge_dataframes=test_merge_dataframes, test_csv_file=test_csv_file)\n","break"]},{"cell_type":"markdown","metadata":{"id":"tunHHb885PKf"},"source":["# Analysis Functions"]},{"cell_type":"markdown","metadata":{"id":"6jkVa95flUdo"},"source":["## Plot: Tuning Curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiM_F1KL1cLd"},"outputs":[],"source":["def PlotFrame(experiment_path, ax, x_ticks, x_label, y_ticks, y_label, xlim=None, ylim=None, logy=False, max_plot_num=-1):\n","    '''\n","    This function plots the dataframes for each tuning (experiment).\n","\n","    experiment_path:    path to the experiment file\n","    ax:                 Matplotlib axis object to plot the dataframes\n","    x_ticks:            x-axis label\n","    x_label:            x-axis title\n","    y_ticks:            y-axis label\n","    y_label:            y-axis title\n","    xlim:               lower limit for the x-axis. Set to None to set no limit.\n","    ylim:               lower limit for the y-axis. Set to None to set no limit.\n","    logy:               use a logarithmic scale for the y-axis?\n","    max_plot_num        maximum number of dataframes to plot. Set to -1 to plot all dataframes.\n","    '''\n","    restored_tuner = tune.Tuner.restore(experiment_path,\n","                                        trainable = tune.with_resources(train_Supervisory_Sym, {\"CPU\":4,\"GPU\":1}))\n","    result_grid = restored_tuner.get_results()\n","\n","    for i, result in enumerate(result_grid):\n","        #print(i)\n","        #label = f\"lr={result.config['lr']:.3f}, momentum={result.config['momentum']}\"\n","        try: # Keeps plotting even if there is an error with one of the plots\n","            result.metrics_dataframe.plot(x=x_ticks, y=y_ticks, ax=ax, label='test', legend=False, xlim=xlim, ylim=ylim,\n","                                          logy=logy, fontsize=ticksize)\n","        except:\n","            print('Error Plotting')\n","        if i==max_plot_num:\n","            break\n","    ax.set_ylabel(y_label, fontsize=fontsize) # 'fontsize' is a variable set outside of the function (see below)\n","    ax.set_xlabel(x_label, fontsize=fontsize)\n","\n","    return result_grid\n","\n","\n","#####################\n","## Plot Appearance ##\n","#####################\n","\n","## Paths ##\n","#tune_exp_name='search-Full-tunedMSE-SPIE'\n","#tune_exp_name='search-Full-tunedLDM_w5s2_meanWeighted'\n","tune_exp_name='search-Full-tunedLDM_w5s5_evenWeighted'\n","#tune_exp_name='search-Full-tunedMSE-AHSA_scheduler'\n","#tune_exp_name='search-Quartile-lowSSIM-tunedSSIM-D'\n","\n","plot_save_name='figure-tuning'    # Save tuning plot to this filename (do not include extension)\n","plot_dir= '/content/drive/MyDrive/Colab/Working/Plots/'\n","\n","## Defaults ##\n","local_dir='/content/drive/MyDrive/Colab/Working/'\n","experiment_path = f\"{local_dir}{tune_exp_name}\"\n","\n","## Figure ##\n","'''\n","titlesize=14\n","fontsize=10\n","ticksize=8 # font for ticks. Set to None for default\n","dpi=800\n","fig_size=(10,2) # Figure Size\n","fig, axs = plt.subplots(nrows=1, ncols=2, figsize=fig_size, dpi=dpi)\n","ax1 = axs[0] ; ax2 = axs[1]\n","'''\n","titlesize=13\n","fontsize=12\n","ticksize=10\n","dpi=800\n","figsize=(10,8)\n","\n","fig = plt.figure(figsize=figsize, dpi=dpi)\n","gs = gridspec.GridSpec(ncols=100, nrows=100)\n","\n","# Top Row Axes #\n","ax1 = fig.add_subplot(gs[0:25,   0:100])\n","ax2 = fig.add_subplot(gs[38:62,   0:100])\n","ax3 = fig.add_subplot(gs[75:100,  0:100])\n","\n","###########\n","## Plots ##\n","###########\n","\n","#result_grid = PlotFrame(experiment_path, axs[0], 'example_number', 'Example Number', 'MSE', 'MSE', ylim=ylim_MSE, logy=True)\n","result_grid = PlotFrame(experiment_path, ax1, 'batch_step', 'Batch Step', 'MSE', 'MSE', ylim=(4,20), logy=True)\n","ax1.set_title('(A) MSE Learning Curves', fontsize=titlesize)\n","\n","result_grid = PlotFrame(experiment_path, ax2, 'batch_step', 'Batch Step', 'SSIM', 'SSIM', ylim=(0,0.8), logy=False)\n","ax2.set_title('(B) SSIM Learning Curves', fontsize=titlesize)\n","\n","result_grid = PlotFrame(experiment_path, ax3, 'batch_step', 'Batch Step', 'CUSTOM', 'Local Distributions Metric', ylim=(300,500))\n","ax3.set_title('(A) LDM Learning Curves', fontsize=titlesize)\n","\n","\n","#save_path = plot_dir+plot_save_name+'.svg'\n","#savefig(save_path, bbox_inches='tight')\n","\n","##########################\n","## Pick out Best Result ##\n","##########################\n","\n","logdir = result_grid.get_best_result(\"SSIM\", mode=\"max\")\n","print('##################')\n","print('## Best Result! ##')\n","print('##################')\n","print(logdir)"]},{"cell_type":"markdown","metadata":{"id":"_-RDucbz-NOi"},"source":["## Plot: Tuning Stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hW_RJkIi-XaT"},"outputs":[],"source":["tune_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-Tune-Full'\n","tune_csv_file = 'frame-tunedMSE-ASHA'\n","\n","tune_dataframe_path = os.path.join(tune_dataframe_dir, tune_csv_file+'.csv')\n","tune_dataframe = pd.read_csv(tune_dataframe_path)\n","\n","## Describe Dataframes ##\n","\n","#plt.scatter(tune_dataframe['num_params'], tune_dataframe['mean_CNN_MSE'])\n","#plt.scatter(tune_dataframe['num_params'][1:], tune_dataframe['mean_CNN_MSE'][1:])\n","\n","tune_dataframe.plot.scatter('num_params', 'mean_CNN_MSE', ylim=(0,5))\n","tune_dataframe.plot.scatter('gen_lr', 'mean_CNN_MSE', ylim=(0,5))\n","tune_dataframe.plot.scatter('batch_size', 'mean_CNN_MSE', ylim=(0,5))\n","\n","'''\n","plt.scatter(tune_dataframe['num_params'], tune_dataframe['mean_CNN_MSE'], ylim=(0,1))\n","plt.xlabel('Number of Parameters')\n","plt.ylabel('MSE')\n","plt.show()\n","'''\n","\n","tune_dataframe.describe()"]},{"cell_type":"markdown","metadata":{"id":"nFWg1RoN4NVJ"},"source":["## Load: Test Dataframes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mMOMILr4NAr"},"outputs":[],"source":["# tunedMSE #\n","test_dataframe_dir1= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n","#test_csv_file1 = 'combined-tunedFullMSE-trainedFull-onTrainingSet-noMLEM'   # Use this dataframe to determine thresholds for sorting training set by metrics\n","#test_csv_file1 = 'combined-tunedFullMSE-trainedFull-onTestSet-wMLEM'       # Use this dataframe to determine thresholds for sorting test set by metrics\n","#test_csv_file1 = 'combined-tunedFullSSIM-trainedFull-onTestSet-wMLEM'\n","test_csv_file1 = 'combined-tunedHighMSE-trainedHighMSE-onTestSet-wMLEM'\n","\n","#test_dataframe_dir2= '/content/drive/MyDrive/Colab/Working/Dataframes-Test-Quartile-MSE'\n","#test_dataframe_dir2= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n","#test_csv_file2 = 'combined-tunedFullSSIM-trainedFull-onTestSet-wMLEM'\n","#test_csv_file2 = 'combined-tunedHighMSE-trainedHighMSE-onTestSet-wMLEM'\n","test_csv_file2 = 'combined-tunedLowSSIM-trainedLowSSIM-onTestSet-wMLEM'\n","\n","# Read Dataframes from File #\n","dataframe_path1 = os.path.join(test_dataframe_dir1, test_csv_file1+'.csv')\n","dataframe1 = pd.read_csv(dataframe_path1)\n","dataframe_path2 = os.path.join(test_dataframe_dir2, test_csv_file2+'.csv')\n","dataframe2 = pd.read_csv(dataframe_path2)v\n","\n","## Describe Dataframes ##\n","\n","#frame_picked = dataframe[dataframe[\"SSIM (ML-EM)\"]>dataframe[\"SSIM (FBP)\"]]\n","#frame_picked = dataframe[dataframe[\"SSIM (Network)\"]>dataframe[\"SSIM (ML-EM)\"]]\n","\n","#frame_picked = dataframe[dataframe[\"MSE (Network)\"]<dataframe[\"MSE (ML-EM)\"]]\n","#frame_picked = dataframe[dataframe[\"MSE (ML-EM)\"]<dataframe[\"MSE (FBP)\"]]\n","\n","#frame_picked = dataframe1[dataframe1[\"MSE (FBP)\"]>0.95908]\n","#frame_picked = dataframe1[dataframe1[\"MSE (FBP)\"]<0.330922]\n","frame_picked = dataframe1[dataframe1[\"SSIM (FBP)\"]<0.837850]\n","\n","#dataframe1.describe()\n","dataframe2.describe()\n","#frame_picked.describe()"]},{"cell_type":"markdown","metadata":{"id":"kAmVEnJChNGK"},"source":["### Plot: Test Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJbGBeJbKYt7"},"outputs":[],"source":["# Define Plotting Functions #\n","def plot_hist_1D(ax, dataframe, title, x_label, y_label, column_1, column_2, xlim, ylim, bins=400, alpha=0.5):\n","    '''\n","    Plots a histogram of a columns in a dataframe.\n","    '''\n","    dataframe = dataframe[dataframe[column_1]>xlim[0]] # Only grab those elements of dataframe that are within the correct limits\n","    dataframe = dataframe[dataframe[column_1]<xlim[1]]\n","    dataframe = dataframe[dataframe[column_2]>xlim[0]]\n","    dataframe = dataframe[dataframe[column_2]<xlim[1]]\n","\n","    dataframe[[column_1, column_2]].plot.hist(xlim=xlim, ylim=ylim, bins=bins, alpha=alpha, ax=ax, fontsize=fontsize)\n","    ax.set_title(title, fontsize=titlesize)  # Add a title to the axis.\n","    ax.set_xlabel(x_label, fontsize=fontsize)  # Add an x-label to the axis.\n","    ax.set_ylabel(y_label, fontsize=fontsize)  # Add a y-label to the axis.\n","\n","def plot_hist_2D(ax, dataframe, title, x_label, y_label, x_column, y_column, xlim=(0,1), ylim=(0,1), gridsize=None):\n","    '''\n","    Plots hexagonal bin plot of a two columns in a dataframe.\n","\n","    dataframe   the dataframe from which to grab the data\n","    x_label     label of data to plot on the x-axis. This must match a column label in the dataframe.\n","    y_label     label of data to plot on the y-axis. This must match a column label in the dataframe.\n","    gridsize    how large to make the grid on the gridplot\n","    '''\n","    dataframe = dataframe[dataframe[x_column]>xlim[0]]\n","    dataframe = dataframe[dataframe[x_column]<xlim[1]]\n","    dataframe = dataframe[dataframe[y_column]>ylim[0]]\n","    dataframe = dataframe[dataframe[y_column]<ylim[1]]\n","    dataframe.plot.hexbin(ax=ax, x=x_column, y=y_column, xlim=xlim, ylim=ylim, gridsize=gridsize, fontsize=ticksize)\n","\n","    ax.set_title(title, fontsize=titlesize)  # Add a title to the axis.\n","    ax.set_xlabel(x_label, fontsize=fontsize)  # Add an x-label to the axis.\n","    ax.set_ylabel(y_label, fontsize=fontsize)  # Add a y-label to the axis.\n","    ax.plot(xlim, ylim, linestyle='--') # plot dividing line\n","\n","## Specify Plotting Parameters ##\n","plot_type = 2 # 1 = histograms, 2 = bin plots, 3 = both\n","\n","column_MSE_1 = 'MSE (ML-EM)'\n","#column_MSE_1 = 'MSE (FBP)'\n","column_MSE_2 = 'MSE (Network)'\n","#column_MSE_2 = 'MSE (FBP)'\n","\n","column_SSIM_1 = 'SSIM (ML-EM)'\n","#column_SSIM_1 = 'SSIM (FBP)'\n","column_SSIM_2 = 'SSIM (Network)'\n","#column_SSIM_2 = 'SSIM (FBP)'\n","\n","\n","titlesize=12\n","fontsize=9\n","ticksize=7\n","dpi=800\n","\n","if plot_type == 1 or plot_type == 2:\n","    figsize=(8,6) # 17,5\n","    fig = plt.figure(figsize=figsize, dpi=dpi)\n","    gs = gridspec.GridSpec(ncols=100, nrows=100)\n","\n","    # Top Row Axes #\n","    ax1 = fig.add_subplot(gs[0:42,   0:43])\n","    ax2 = fig.add_subplot(gs[0:42,   57:100])\n","\n","    # Bottom Row Axes #\n","    ax3 = fig.add_subplot(gs[58:100, 0:43])\n","    ax4 = fig.add_subplot(gs[58:100, 57:100])\n","\n","    if plot_type == 1:\n","        plot_hist_1D(ax1, dataframe1, '(1) CNN-A: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2, xlim=(0,4), ylim=(0,5000), bins=40)\n","        plot_hist_1D(ax2, dataframe1, '(2) CNN-A: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n","        plot_hist_1D(ax3, dataframe2, '(3) CNN-B: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2,  xlim=(0,4), ylim=(0,5000),  bins=40)\n","        plot_hist_1D(ax4, dataframe2, '(4) CNN-B: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n","    if plot_type == 2:\n","        plot_hist_2D(ax1, dataframe1, '(1) CNN-A: MSE Bin Plot', column_MSE_1, 'MSE (CNN-A)', column_MSE_1 , column_MSE_2,(0,1.5), (0,1.5), gridsize=60)\n","        plot_hist_2D(ax2, dataframe1, '(2) CNN-A: SSIM Bin Plot',column_SSIM_1, 'SSIM (CNN-A)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n","        plot_hist_2D(ax3, dataframe2, '(3) CNN-B: MSE Bin Plot', column_MSE_1, 'MSE (CNN-B)', column_MSE_1 , column_MSE_2, (0,1.5), (0,1.5), gridsize=60)\n","        plot_hist_2D(ax4, dataframe2, '(4) CNN-B: SSIM Bin Plot', column_SSIM_1, 'SSIM (CNN-B)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n","\n","if plot_type == 3:\n","    figsize=(15,6) # 17,5\n","    fig = plt.figure(figsize=figsize, dpi=dpi)\n","    gs = gridspec.GridSpec(ncols=100, nrows=100)\n","\n","    # Top Row Axes #\n","    ax1 = fig.add_subplot(gs[0:42,   0:18]) # 20\n","    ax2 = fig.add_subplot(gs[0:42,   25:47]) # 22\n","    ax3 = fig.add_subplot(gs[0:42,   53:74]) # 20\n","    ax4 = fig.add_subplot(gs[0:42,   80:100]) # 22\n","\n","    # Bottom Row Axes #\n","    ax5 = fig.add_subplot(gs[58:100, 0:18]) # -5-\n","    ax6 = fig.add_subplot(gs[58:100, 25:47]) # -3 - -3-\n","    ax7 = fig.add_subplot(gs[58:100, 53:74]) # -5-\n","    ax8 = fig.add_subplot(gs[58:100, 80:100])\n","\n","    plot_hist_1D(ax1, dataframe1, '(1) CNN-A: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2, xlim=(0,4), ylim=(0,5000), bins=40)\n","    plot_hist_1D(ax2, dataframe1, '(3) CNN-A: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n","    plot_hist_2D(ax3, dataframe1, '(5) CNN-A: MSE Bin Plot', column_MSE_1, 'MSE (CNN-A)', column_MSE_1 , column_MSE_2,(0,1.5), (0,1.5), gridsize=60)\n","    plot_hist_2D(ax4, dataframe1, '(7) CNN-A: SSIM Bin Plot',column_SSIM_1, 'SSIM (CNN-A)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n","\n","    plot_hist_1D(ax5, dataframe2, '(2) CNN-B: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2,  xlim=(0,4), ylim=(0,5000),  bins=40)\n","    plot_hist_1D(ax6, dataframe2, '(4) CNN-B: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n","    plot_hist_2D(ax7, dataframe2, '(6) CNN-B: MSE Bin Plot', column_MSE_1, 'MSE (CNN-B)', column_MSE_1 , column_MSE_2, (0,1.5), (0,1.5), gridsize=60)\n","    plot_hist_2D(ax8, dataframe2, '(8) CNN-B: SSIM Bin Plot', column_SSIM_1, 'SSIM (CNN-B)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n","\n","save_path = plot_dir+'figure-histograms.png'\n","savefig(save_path, bbox_inches='tight')\n"]},{"cell_type":"markdown","metadata":{"id":"t5qkU0fpHsKa"},"source":["## Plot: Example Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9PCpxBnHo07"},"outputs":[],"source":["### User Parameters ###\n","#######################\n","\n","## Indexes of Example Images ##\n","#-----------------------------#\n","# Panel 1: Network performs much better for all images\n","#indexes = [2280+4, 13*120, 187*120+13, 151 * 120, 240+37, 147*120, 187*120+17, 239*120]\n","\n","# Panel 2: Network performs somewhat better:\n","#indexes = [1073, 840+48, 108 * 120,  147*120+33, 153*120, 1440+71, 13560+17, 153*120, 224*120+161]\n","\n","# Panel 3: ML-EM performs better\n","#indexes = [960+97, 1268*120, 111*120]\n","\n","#Panel 5: Panel 1 + Panel 2 --> image 0-8 (network much better)\n","#indexes = [2280+4, 13*120, 151 * 120, 240+37, 147*120, 187*120+17, 239*120, 1073]\n","#indexes = [840+48+1, 108 * 120,  147*120+33, 153*120+2, 1440+71, 13560+17, 224*120+161+5]\n","\n","# Panel 6: Used in SPIE Paper\n","#indexes = [2280+4, 240+37, 187*120+17, 239*120, 1073, 840+48+1, 153*120+2,  13*120, 224*120+161+5]\n","#indexes = [240+37, 187*120+17, 239*120, 1073, 840+48+1, 224*120+161+5, 153*120+2, 13*120] # Final Cut\n","\n","# Final Panel (nine images max)\n","\n","## Paths ##\n","indexes = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n","\n","image_path = '/content/drive/MyDrive/Repository/PET_Data/test_image-35k.npy'\n","sino_path =   '/content/drive/MyDrive/Repository/PET_Data/test_sino-35k.npy'\n","\n","\n","checkpoint_dir = checkpoint_dir='/content/drive/MyDrive/Colab/Working/Checkpoints-trainOnFull'\n","\n","checkpoint_file_SSIM= 'checkpoint-tunedSSIM-14d-6epochs'\n","checkpoint_file_MSE = 'checkpoint-tunedMSE-fc6-6epochs'\n","checkpoint_file_MAE = 'checkpoint-tunedMAE-b08-6epochs'\n","checkpoint_file_LDM = 'checkpoint-tunedLDM-w10s8-b5c-6epochs'\n","checkpoint_file_LDM_batch='checkpoint-tunedLDM_batch-f9f-6epochs'\n","\n","## Dimensions ##\n","image_size=90\n","sino_size=90\n","image_channels=1\n","sino_channels=1\n","\n","## CNNs ##\n","config_MSE= { # 1x90x90, Tuned for MSE - fc6 #\n","    \"SI_dropout\": False, \"SI_exp_kernel\": 4, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": None, \"SI_gen_hidden_dim\": 14, \"SI_gen_mult\": 2.3737518721494038,\n","    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 300, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100,\n","    \"batch_size\": 266, \"gen_b1\": 0.5194977285709309, \"gen_b2\": 0.4955647195661826, \"gen_lr\": 0.0006569034263698925, \"sup_criterion\": nn.MSELoss() }\n","\n","config_SSIM = { # 1x90x90, Tuned for SSIM - 14d #\n","    \"SI_dropout\": False, \"SI_exp_kernel\": 4, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": nn.Tanh(), \"SI_gen_hidden_dim\": 23, \"SI_gen_mult\": 1.6605902406330195,\n","    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 789, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 71, \"gen_b1\": 0.2082092731474774,\n","    \"gen_b2\": 0.27147903136187507, \"gen_lr\": 0.0005481469822215635, \"sup_criterion\": nn.MSELoss()}\n","\n","config_MAE = { # 1x90x90, Tuned for MAE, - b08 #\n","    \"SI_dropout\": True, \"SI_exp_kernel\": 3, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": nn.Tanh(), \"SI_gen_hidden_dim\": 29, \"SI_gen_mult\": 3.4493572412953926,\n","    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 92, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 184,\n","    \"gen_b1\": 0.41793988944151467, \"gen_b2\": 0.15133808988276928, \"gen_lr\": 0.0012653525173041019, \"sup_criterion\": nn.L1Loss() }\n","\n","config_LDM={ # 1x90x90, Tuned for CUSTOM = LDM (image statistics)\n","    \"SI_dropout\": False, \"SI_exp_kernel\": 4, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": None, \"SI_gen_hidden_dim\": 9, \"SI_gen_mult\": 2.1547197646081444,\n","    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 344, \"SI_layer_norm\": \"batch\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 47,\n","    \"gen_b1\": 0.31108788447029295, \"gen_b2\": 0.3445239707919786, \"gen_lr\": 0.0007561178182660596, \"sup_criterion\": nn.L1Loss()}\n","\n","config_LDM_batch={ # 1x90x90, Tuned for CUSTOM = LDM (batch statistics)\n","    \"SI_dropout\": False, \"SI_exp_kernel\": 3, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": nn.Tanh(), \"SI_gen_hidden_dim\": 19, \"SI_gen_mult\": 2.70340867805694,\n","    \"SI_gen_neck\": 1, \"SI_gen_z_dim\": 1616, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 363,\n","    \"gen_b1\": 0.20393974474424928, \"gen_b2\": 0.6490512100839003, \"gen_lr\": 0.0004491464075393307, \"sup_criterion\": nn.MSELoss()}\n","\n","## Defaults ##\n","train_type='SUP'\n","train_SI=True\n","image_array = np.load(image_path, mmap_mode='r')       # self.image_tensor.shape=(#examples x1x71x71)\n","sino_array = np.load(sino_path, mmap_mode='r')     # self.sinogram_tensor.shape=(#examples x3x101x180)\n","\n","## Build Image & Sino Tensors ##\n","def BuildImageSinoTensors(image_array, sino_array, config, indexes):\n","    '''\n","    Build image and sinogram tensors with images and sinograms determined by the indexes list.\n","    '''\n","    first=True\n","    i=0\n","    for idx in indexes:\n","        sino_ground, sino_ground_scaled, image_ground, image_ground_scaled = NpArrayDataLoader(image_array, sino_array, config,\n","                                                                                image_size = image_size, sino_size=sino_size,\n","                                                                                image_channels=image_channels,\n","                                                                                sino_channels=sino_channels,\n","                                                                                augment=False, index=idx)\n","        # If first time through the loop, create blank tensors (for sino & image) with the correct shape\n","        if first==True:\n","            image_tensor = torch.zeros(len(indexes), image_ground_scaled.shape[0], image_ground_scaled.shape[1], image_ground_scaled.shape[2]).to(device)\n","            sino_tensor  = torch.zeros(len(indexes), sino_scsino_ground_scaled0],  sino_scsino_ground_scaled1],  sino_scsino_ground_scaled2]).to(device)\n","            first=False\n","        # Fill the tensors with images & sinograms\n","        image_tensor[i,:] = image_ground_scaled\n","        sino_tensor[i,:]  = sino_ground_scaled\n","        i+=1\n","    return image_tensor, sino_tensor\n","\n","## CNN Outputs ##\n","def CNN_reconstruct(sino_tensor, config, checkpoint_file_name, input_size=90, input_channels=1, output_channels=1):\n","    '''\n","    Construct CNN reconstructions of images of a sinogram tensor.\n","    '''\n","    gen =  Generator(config=config, gen_SI=True, input_size=input_size, input_channels=input_channels, output_channels=output_channels).to(device)\n","    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file_name)\n","    checkpoint = torch.load(checkpoint_path)\n","    gen.load_state_dict(checkpoint['gen_state_dict'])\n","    gen.eval()\n","    return gen(sino_tensor).detach()\n","\n","## Outputs ###\n","image_tensor, sino_tensor = BuildImageSinoTensors(image_array, sino_array, config_MSE, indexes)\n","\n","MLEM_output = reconstruct(sino_tensor, config_MSE, image_size=image_size, recon_type='MLEM', circle=True)\n","CNN_output_MSE = CNN_reconstruct(sino_tensor, config_MSE, checkpoint_file_MSE)\n","CNN_output_SSIM = CNN_reconstruct(sino_tensor, config_SSIM, checkpoint_file_SSIM)\n","CNN_output_MAE = CNN_reconstruct(sino_tensor, config_MAE, checkpoint_file_MAE)\n","CNN_output_LDM = CNN_reconstruct(sino_tensor, config_LDM, checkpoint_file_LDM)\n","CNN_output_LDM_batch = CNN_reconstruct(sino_tensor, config_LDM_batch, checkpoint_file_LDM_batch)\n","\n","#MLEM_output2 = reconstruct(sino_tensor, config1, image_size=image_size, recon_type='MLEM', circle=False)\n","#FBP_output =  reconstruct(sino_tensor, config1, image_size=image_size, recon_type='FBP')\n","\n","#############\n","## Metrics ##\n","#############\n","\n","'''\n","frame_SSIM_MLEM, placeholder = calculate_metric(MLEM_output, image_tensor, SSIM, dataframe = True, label='MLEM, SSIM')\n","frame_MSE_MLEM, placeholder =  calculate_metric(MLEM_output, image_tensor, MSE, dataframe = True, label='MLEM, MSE')\n","print('################### MLEM ###################')\n","print(frame_SSIM_MLEM.T)\n","print(frame_MSE_MLEM.T)\n","\n","\n","frame_SSIM_tunedMSE, placeholder = calculate_metric(CNN_output_MSE, image_tensor, SSIM, dataframe = True, label='TunedMSE, SSIM')\n","frame_MSE_tunedMSE, placeholder =  calculate_metric(CNN_output_MSE, image_tensor, MSE, dataframe = True, label='TunedMSE, MSE')\n","print('################### CNN (MSE) ##################')\n","print(frame_SSIM_tunedMSE.T)\n","print(frame_MSE_tunedMSE.T)\n","\n","frame_SSIM_tunedSSIM, placeholder = calculate_metric(CNN_output_SSIM, image_tensor, SSIM, dataframe = True, label='TunedSSIM, SSIM')\n","frame_MSE_tunedSSIM, placeholder =  calculate_metric(CNN_output_SSIM, image_tensor, MSE, dataframe = True, label='TunedSSIM, MSE')\n","frame_LDM_tunedSSIM, placeholder =  calculate_metric(CNN_output_SSIM, image_tensor, custom_metric, dataframe = True, label='TunedSSIM, LDM')\n","print('################### CNN (SSIM) ##################')\n","print(frame_SSIM_tunedSSIM.T)\n","print(frame_MSE_tunedSSIM.T)\n","print(frame_LDM_tunedSSIM.T)\n","\n","frame_SSIM_tunedLDM, placeholder = calculate_metric(CNN_output_LDM, image_tensor, SSIM, dataframe = True, label='TunedLDM, SSIM')\n","frame_MSE_tunedLDM, placeholder =  calculate_metric(CNN_output_LDM, image_tensor, MSE, dataframe = True, label='TunedLDM, MSE')\n","frame_LDM_tunedLDM, placeholder =  calculate_metric(CNN_output_LDM, image_tensor, custom_metric, dataframe = True, label='TunedLDM, LDM')\n","print('################### CNN (LDM) ##################')\n","print(frame_SSIM_tunedLDM.T)\n","print(frame_MSE_tunedLDM.T)\n","print(frame_LDM_tunedLDM.T)\n","'''\n","\n","####################\n","## Display Images ##\n","####################\n","\n","\n","#show_multiple_matched_tensors(image_tensor, CNN_output_MSE, CNN_output_LDM, CNN_output_MAE, MLEM_output, CNN_output_SSIM, fig_size=1.0)\n","#show_multiple_matched_tensors(image_tensor, MLEM_output, CNN_output_MSE, CNN_output_MAE, CNN_output_SSIM, CNN_output_LDM_batch, CNN_output_LDM, fig_size=1.0)\n","#show_multiple_matched_tensors(image_tensor, fig_size=1.0)\n","\n","#print('Ground Truth/MLEM/CNN1/CNN2')\n","show_single_unmatched_tensor(sino_tensor)\n","show_multiple_matched_tensors(image_tensor, MLEM_output, CNN_output_MSE, CNN_output_MAE, fig_size=1.0)"]},{"cell_type":"markdown","metadata":{"id":"v-tDENcFyMMC"},"source":["# Sort: Dataset by Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2q2L7hFyQ1d"},"outputs":[],"source":["def sort_DataSet(config, load_image_path, load_sino_path, save_image_path, save_sino_path, max_save_index, metric_function, threshold, threshold_min_max, num_examples=-1, visualize=False):\n","    '''\n","    '''\n","    # Variables #\n","    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n","\n","    # Dataloader #\n","    dataloader = DataLoader(\n","        NpArrayDataSet(image_path=load_image_path, sino_path=load_sino_path, config=config, image_size=image_size, image_channels=image_channels,\n","                       sino_size=sino_size, sino_channels=sino_channels, num_examples=num_examples),\n","        batch_size=1,\n","        shuffle=True\n","    )\n","\n","    ### Loop Over Batches ###\n","    first = True\n","    saved_idx = 0\n","    for sino_ground, sino_ground_scaled, image_ground, image_ground_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n","\n","        # Open memory map if first time through the loop #\n","        if first==True:\n","            save_image_array_shape = (max_save_index, image_ground_scaled.shape[1], image_ground_scaled.shape[2], image_ground_scaled.shape[3])\n","            save_sino_array_shape = (max_save_index, sino_ground_scaled.shape[1], sino_ground_scaled.shape[2], sino_ground_scaled.shape[3])\n","            print('save_image_array_shape: ', save_image_array_shape)\n","            print('save_sino_array_shape: ', save_sino_array_shape)\n","\n","            save_image_array = np.lib.format.open_memmap(save_image_path, mode='w+', shape=save_image_array_shape, dtype=np.float32)\n","            save_sino_array =  np.lib.format.open_memmap(save_sino_path , mode='w+', shape=save_sino_array_shape,  dtype=np.float32)\n","            first=False\n","\n","        # Test the image to see if fits the criteria #\n","        FBP_output =  reconstruct(sino_ground_scaled, config, image_size=image_size, recon_type='FBP')\n","        image_metric = metric_function(image_ground_scaled, FBP_output)\n","\n","        if threshold_min_max == 'min':\n","            keep = True if (image_metric > threshold) else False\n","        else:\n","            keep = True if (image_metric < threshold) else False\n","\n","        if keep==True:\n","            save_sino_array[saved_idx] = sino_ground_scaled.cpu().numpy()\n","            save_image_array[saved_idx] = image_ground_scaled.cpu().numpy()\n","            saved_idx += 1\n","            print('Current index (for next image): ', saved_idx)\n","\n","        if visualize==True:\n","            # Visualize the rejected or accepted sample #\n","            print('==================================')\n","            print('Image Metric: ', image_metric)\n","            print('Threshold: ', threshold)\n","            print('Keep?: ', keep)\n","            print('Current index (for next image): ', saved_idx)\n","            print('Saved Arrays:')\n","            print('image_ground_scaled / FBP_output / sino_ground_scaled')\n","            show_multiple_matched_tensors(image_ground_scaled, FBP_output)\n","            show_multiple_matched_tensors(sino_ground_scaled)\n","            show_multiple_matched_tensors(torch.from_numpy(save_sino_array[0:9]))\n","            show_multiple_matched_tensors(torch.from_numpy(save_image_array[0:9]))\n","\n","    return save_sino_array, save_image_array\n","\n","## Changeable Variables ##\n","\n","load_sino_path = '/content/drive/MyDrive/Repository/PET_Data/train_sino-70k.npy'\n","load_image_path = '/content/drive/MyDrive/Repository/PET_Data/train_image-70k.npy'\n","save_sino_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n","save_image_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_image-lowSSIM-17500.npy'\n","'''\n","metric_function = MSE\n","max_save_index = 17500\n","threshold = 0.330922\n","threshold_min_max = 'min'\n","'''\n","metric_function = SSIM\n","max_save_index = 17500\n","threshold = 0.837850 #0.837850  # MSE (min): 0.330922, SSIM (max): 0.837850\n","threshold_min_max = 'max'\n","\n","## Run & Verify Result ##\n","save_sino_array, save_image_array = sort_DataSet(config, load_image_path, load_sino_path, save_image_path, save_sino_path, max_save_index,\n","                                                   metric_function, threshold, threshold_min_max=threshold_min_max, visualize=False)\n","\n","sino_ground_scaled"]},{"cell_type":"markdown","metadata":{"id":"jwU5ioxR6bHg"},"source":["### Save Datasets & Check"]},{"cell_type":"markdown","metadata":{"id":"DQCMCuDx6bEp"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTrqg3MHNl1x"},"outputs":[],"source":["#save_sino_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n","#save_image_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n","\n","# Print sorted array shape & display a few images #\n","print('save_sino_array.shape: ', save_sino_array.shape)\n","print('save_image_array.shape: ', save_image_array.shape)\n","\n","print('save_sino_array sample images')\n","print('save_image_array sample images')\n","show_multiple_matched_tensors(torch.from_numpy(save_sino_array[500:509]))\n","show_multiple_matched_tensors(torch.from_numpy(save_image_array[500:509]))\n","\n","\n","# Save the sorted array to disk #\n","save_sino_array.flush()\n","save_image_array.flush()\n","#np.save(save_sino_path, save_sino_array)\n","#np.save(save_image_path, save_image_array)\n","\n","# Load the saved array and make sure it's the same size/has the same images #\n","load_sino_array = np.load(save_sino_path, mmap_mode='r')\n","load_image_array = np.load(save_image_path, mmap_mode='r')\n","print('load_sino_array.shape: ', load_sino_array.shape)\n","print('load_image_array.shape: ', load_image_array.shape)\n","\n","print('load_sino_array sample images')\n","print('load_image_array sample images')\n","show_multiple_matched_tensors(torch.from_numpy(load_sino_array[500:509]))\n","show_multiple_matched_tensors(torch.from_numpy(load_image_array[500:509]))\n"]},{"cell_type":"markdown","metadata":{"id":"h0S2M7PagXsv"},"source":["# Experimenting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23TOba33L4qf"},"outputs":[],"source":["## Find what GPU I'm using ##\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIf7aS5gfAGp"},"outputs":[],"source":["'''\n","import sys\n","print('a')\n","sys.exit()\n","print('b')\n","'''\n","'''\n","print(train_image_path)\n","print(train_sino_path)\n","print(test_image_path)\n","print(test_sino_path)\n","'''\n","print(shuffle)"]},{"cell_type":"markdown","metadata":{"id":"U3anZSZqZgJs"},"source":["# Notes:\n","Change next\n","===========\n","tune_even_reporting=False\n","\n","\n","For high/low MSE experiments\n","============================\n","-Tuned networks for 180 minutes each.\n","\n","-Trained for 100 epochs using on-the-fly augmentation\n","\n","-See notes in checkpoint folder\n","\n","\n","For LDM, window = 5, stride = 2\n","===============================\n","tune_max_t = 20            \n","\n","tune_minutes = 180      \n","\n","tune_display_step=12    \n","\n","tune_augment=False\n","\n","\n","GPUs\n","====\n","From best to worst:\n","\n","V100 - 6.92/hr\n","\n","L4 - 2.15/hr\n","\n","T4 - 1.7/hr\n","\n","v6e-1 TPU - 4.21/hr\n","\n","v5e-1 TPU - 4.11/hr\n","\n","v2-8 TPU - 1.82/hr"]},{"cell_type":"markdown","metadata":{"id":"0AHpRBHQKzo9"},"source":["Tensor board works for all experiments except the last one.\n","My plotting function no longer works for any of the experiments."]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1IKZAHG3MkebzxT6rbCXh6UyK1A_-pgjW","timestamp":1662946787019},{"file_id":"1rhTeO9VU1bsXALEA91XBWt5RmysWxoO_","timestamp":1657063751190}],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyP+nWh+m2yWqLxRnJv54sO9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}